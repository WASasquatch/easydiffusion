{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WASasquatch/easydiffusion/blob/main/Stability_AI_Easy_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y6RXjS1tTji"
      },
      "source": [
        "# Stability.AI Easy Diffusion v0.9 ![visitors](https://visitor-badge.glitch.me/badge?page_id=EasyDiffusion&left_color=blue&right_color=orange) [![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/WASasquatch/easydiffusion)\n",
        "\n",
        "Easy Diffusion was originally a fork of NOP's notebook, but has sort of evolved into it's own thing with many features. Such as depth output for 3D Facebook images, or post processing such as Depth of Field.\n",
        "\n",
        "If you'd like to help support the project and my time, feel free to buy me some bandwidth (I live rural and pay for bandwidth): https://paypal.me/ThompsonJordan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G76cGaiuGdjJ"
      },
      "source": [
        "## Information\n",
        "\n",
        "Changelog:\n",
        "- v0.1: Forked [NOP's Stable Diffusion Colab v0.23](https://colab.research.google.com/drive/1jUwJ0owjigpG-9m6AI_wEStwimisUE17?usp=sharing)\n",
        "  - Added File Prompts\n",
        "  - Added Noodle Soup Prompts\n",
        "- 8/25/2022) Added better image output display\n",
        "- 8/26/2022) Added `INIT_IMAGE` support\n",
        "  - Added basic image output option\n",
        "- 8/27/2022) Patched CodeFormer fidelity path bug\n",
        "- 8/27/2022) Various code tweaks (by plambe#5832)\n",
        "  - Download some of the dependencies to google drive if enabled \n",
        "    - For instance the stable diffusion model\n",
        "    - Also multiple of the git repos\n",
        "  - Replaced all `!` and `%` in code to make it more universal\n",
        "- 8/27/2022) Patch NSP Installation, changed paths for Stable Diffusion and output images. (by WAS#0263)\n",
        "- 8/27/2022) Organized and improved installations\n",
        "- 8/28/2022) Real-ESRGAN bug fix (by plambe#5832)\n",
        "- 8/28/2022) GFPGAN bug fix (by plambe#5832)\n",
        "- 8/28/2022) CodeFormer bug fix (by plambe#5832)\n",
        "- 8/28/2022) Added cached diffusion piping: This will speed up run performance (WAS#0263)\n",
        "  - Added `RECACHE_PIPES` option\n",
        "  - Added `INCREMENT_ITERATION_SEED` option\n",
        "  - Patched working directory path for non-gdrive installations\n",
        "  - Patched working directory path for pipe cache not found\n",
        "  - Patched CodeFormer Fidelity paths, again?\n",
        "  - Added pre-ESRGAN down scaling option for GFPGAN + Real-ESRGAN, and CodeFormer + Real-ESRGAN.\n",
        "  - Added post diffusion sharpen option\n",
        "- V0.6 | 8/29/2022) Added optional cached pipes. Using cached pipes is best for a high VRAM environment\n",
        "  - Added Kromo's Chromatic Aberration\n",
        "  - Added Sharpening\n",
        "  - Added optional dependency installs (save some space!)\n",
        "- v0.7 | 8/29/2022) Added MiDaS Depth Approximation\n",
        "  - Depth maps can be used to apply Depth of Field, or other filmic effects in post processing with your favorite tools.\n",
        "- v0.8 | 8/30/2022) Added Sampling Schedulers\n",
        "  -  Track function timing\n",
        "- 8/31/2022) Patch MiDaS Depth Export even when unchecked.\n",
        "- v0.9 | 9/01/2022) Added multi-init functionality to `INIT_IMAGE`.\n",
        "  - `INIT_IMAGE` supports a local/remote image path/url, a txt file containing a path/url per line, or a path to a folder containing images.\n",
        "\n",
        "<br>\n",
        "\n",
        "## Stablity.AI Model Terms of Use\n",
        "\n",
        "**By using this Notebook, you agree to the following Terms of Use, and license**\n",
        "\n",
        "This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\n",
        "\n",
        "The CreativeML OpenRAIL License specifies:\n",
        "1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content\n",
        "2. CompVis claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n",
        "3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n",
        "\n",
        "Please read the full license here: https://huggingface.co/spaces/CompVis/stable-diffusion-license \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NOEF-K5F5db"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_ekR-LW6trWG"
      },
      "outputs": [],
      "source": [
        "#@title Check GPU Status\n",
        "#@markdown Check the status of the allocated GPU\n",
        "import subprocess\n",
        "print(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "nvidiasmi_simple = subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "gpu_name = nvidiasmi_simple.split(':')[1].split('(')[0].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8gV4-qRDn1b",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title <font size=\"5\" color=\"orange\">**Setup Environment**</font>\n",
        "\n",
        "# Import future print\n",
        "from __future__ import print_function\n",
        "try:\n",
        "    import __builtin__\n",
        "except ImportError:\n",
        "    import builtins as __builtin__\n",
        "\n",
        "# Emoticon fun!\n",
        "import subprocess\n",
        "try:\n",
        "    import emoji\n",
        "except ImportError:\n",
        "     multipip_res = subprocess.run(['pip', '-q', 'install', 'emoji'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "finally:\n",
        "    import emoji\n",
        "\n",
        "print(subprocess.run('python -m ensurepip --upgrade'.split(' '), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "\n",
        "# Override Print Function\n",
        "def print(message, *args, **kwargs):\n",
        "    if 'defaultprint' in kwargs:\n",
        "        kwargs.pop('defaultprint')\n",
        "        return __builtin__.print(message, *args, **kwargs)\n",
        "    else:\n",
        "        return __builtin__.print(emoji.emojize(message), *args, **kwargs)\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### **Google Drive Options**\n",
        "USE_DRIVE_FOR_PICS = True #@param {type:\"boolean\"}\n",
        "#@markdown <font size=\"3\">Use Google Drive to store images and prompt information</font>\n",
        "USE_DRIVE_FOR_LOCAL_COPIES = False #@param {type:\"boolean\"}\n",
        "#@markdown <font size=\"3\">Use Google Drive to store local copies of git repos, models and other assets</font><br>\n",
        "#@markdown <font size=\"3\" color=\"orange\">**WARNING:**</font> Requires 14gb+ of space (not including images produced). May not be suitable for Free Google Drive accounts.</font><br>\n",
        "#@markdown <font size=\"3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If you encounter issues loading pipes, or Upscalers, you're likely out of storage space.</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### **Install Optional Features**\n",
        "INSTALL_GFPGAN = True #@param{type:'boolean'}\n",
        "#@markdown <font size=\"3\">Install GFPGAN Face Enhancement</font>\n",
        "INSTALL_CODEFORMER = True #@param{type:'boolean'}\n",
        "#@markdown <font size=\"3\">Install CodeFormer Face Enhancement</font>\n",
        "INSTALL_ESRGAN = True #@param{type:'boolean'}\n",
        "#@markdown <font size=\"3\">Install Real-ESRGAN Super Resolution</font>\n",
        "INSTALL_KROMO = True #@param{type:'boolean'}\n",
        "#@markdown <font size=\"3\">Install Kromo Chromatic Aberration gnerator</font>\n",
        "INSTALL_MIDAS = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Install timm for MiDaS support (this allows you to export Depth Maps)</font>\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "LOW_VRAM_PATCH = True #@param {type:\"boolean\"}\n",
        "#@markdown <font size=\"3\">`LOW_VRAM_PATCH`: Use 16bit float instead of 32bit float. This saves VRAM, at the potential cost of model fidelity.<br>**Note:** You may need this if you're using a GPU with ~16GB VRAM.</font><br>\n",
        "#@markdown ---\n",
        "ENABLE_NSFW_FILTER = False #@param {type:\"boolean\"}\n",
        "#@markdown <font size=\"3\">`ENABLE_NSFW_FILTER`: Enable NSFW censoring</font><br>\n",
        "\n",
        "#@markdown ---\n",
        "CACHE_PIPELINES = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Whether to cache pipes to disk and load on demand (this can speed up diffusion start time)</font>\n",
        "RECACHE_PIPES = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">**NOTE:** If you're having trouble loading pipes to start diffusions, check this and run this cell again.</font><br>\n",
        "\n",
        "#@markdown ---\n",
        "CLEAR_SETUP_LOG = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Clear the setup log after installation completes.</font>\n",
        "SUPPRESS_WARNINGS = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Supress warnings from installation scripts and runtime scripts.</font>\n",
        "\n",
        "# Enable third-party widgets\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "# SETUP BASE DIRECTORIES\n",
        "OUTDIR = '/content/Stable_Diffusion/images_out'\n",
        "\n",
        "import os, sys, time\n",
        "\n",
        "if USE_DRIVE_FOR_PICS and not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "STABLE_DIFFUSION_WORKDIR = '/content/Stable_Diffusion'\n",
        "GDRIVE_WORKDIR = '/content/drive/MyDrive/AI/Stable_Diffusion'\n",
        "\n",
        "if USE_DRIVE_FOR_LOCAL_COPIES:\n",
        "    STABLE_DIFFUSION_WORKDIR = GDRIVE_WORKDIR\n",
        "    if not os.path.exists(STABLE_DIFFUSION_WORKDIR):\n",
        "        os.makedirs(STABLE_DIFFUSION_WORKDIR)\n",
        "if not USE_DRIVE_FOR_LOCAL_COPIES:\n",
        "    if not os.path.exists(STABLE_DIFFUSION_WORKDIR):\n",
        "        os.makedirs(STABLE_DIFFUSION_WORKDIR)\n",
        "\n",
        "os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "sys.path.append(STABLE_DIFFUSION_WORKDIR)\n",
        "\n",
        "# SETUP DEPENDENCIES\n",
        "\n",
        "import torch, gc, requests, io, shutil\n",
        "\n",
        "def wget(url, outputdir):\n",
        "    res = subprocess.run(['wget', '-q', '--show-progress', url, '-P', f'{outputdir}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "def clean_env(v=False):\n",
        "    stt = time.time()\n",
        "    if torch.cuda.is_available(): torch.cuda.synchronize(); torch.cuda.empty_cache(); gc.collect()\n",
        "    if v: print(':recycling_symbol: Cleared memory.  Time taken was %f secs' % (time.time() - stt))\n",
        "    if not torch.cuda.is_available(): print(\":WARNING: There is no CUDA device available! Cannot run diffusion models!\")\n",
        "\n",
        "def fetch_bytes(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        from urllib.request import urlopen \n",
        "        return urlopen(url_or_path) \n",
        "    return open(url_or_path, 'r')\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def clear():\n",
        "    from IPython.display import clear_output; return clear_output()\n",
        "\n",
        "def time_format(seconds: int):\n",
        "    if seconds is not None:\n",
        "        seconds = int(seconds)\n",
        "        d = seconds // (3600 * 24)\n",
        "        h = seconds // 3600 % 24\n",
        "        m = seconds % 3600 // 60\n",
        "        s = seconds % 3600 % 60\n",
        "        if d > 0:\n",
        "            return '{:02d}D {:02d}H {:02d}m {:02d}s'.format(d, h, m, s)\n",
        "        elif h > 0:\n",
        "            return '{:02d}H {:02d}m {:02d}s'.format(h, m, s)\n",
        "        elif m > 0:\n",
        "            return '{:02d}m {:02d}s'.format(m, s)\n",
        "        elif s > 0:\n",
        "            return '{:02d}s'.format(s)\n",
        "    return '-'\n",
        "\n",
        "# Basic image display\n",
        "def displayJsImage(b, i, name, img):\n",
        "    import cv2\n",
        "    from IPython.display import display, Javascript, clear_output\n",
        "    from google.colab.output import eval_js\n",
        "    from base64 import b64encode\n",
        "    from google.colab import files\n",
        "    img = np.asarray(img)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    js = Javascript('''\n",
        "        async function showImage(b, i, name, image, width, height) {\n",
        "        batchBlock = document.getElementById('batch-block-'+b);\n",
        "        block = document.getElementById('block-'+b+'-'+i)\n",
        "        img = document.getElementById(name);\n",
        "        cont = document.getElementById(name+'_container');\n",
        "        if (batchBlock == null) {\n",
        "            batchBlock = document.createElement('div');\n",
        "            batchBlock.id = 'batch-block-'+b;\n",
        "            batchBlock.style = 'background-color:rgba(0,0,0,0.25);width:auto;margin-bottom:25px;padding:5px;text-align:center;';\n",
        "            batchBlock.innerHTML = '<h2 style=\"background-color:rgba(255,255,255,0.1);margin:0;margin-bottom:5px;padding:4px;text-align:center;\">Batch '+b+'</h2>';\n",
        "            document.body.appendChild(batchBlock)\n",
        "        }\n",
        "        if (block == null) {\n",
        "            block = document.createElement('div');\n",
        "            block.id = 'block-'+b+'-'+i;\n",
        "            block.style = 'width: auto;margin-bottom:15px;padding:5px;text-align:center;';\n",
        "            block.innerHTML = '<h3 style=\"margin:3px;text-align:center;\">Iteration '+i+'</h3>';\n",
        "            batchBlock.appendChild(block);\n",
        "        }\n",
        "        if(img == null && cont == null) {\n",
        "            cont = document.createElement('div');\n",
        "            cont.id = name+'_container';\n",
        "            link = document.createElement('a');\n",
        "            link.href = image;\n",
        "            link.target = '_blank';\n",
        "            img = document.createElement('img');\n",
        "            img.id = name;\n",
        "            img.class = \"resultImage\"\n",
        "            cont.style = 'display:inline-block;width:auto;font-size:14px;font-weight:bold;background-color:rgba(0,0,0,0.1);border-radius:5px;padding:2px;margin:2px;'\n",
        "            cont.innerHTML = '<p style=\"margin:3px auto;width:180px;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;\">'+name+'</p>';\n",
        "            block.appendChild(cont);\n",
        "            cont.appendChild(link);\n",
        "            link.appendChild(img);\n",
        "        }\n",
        "        img.src = image;\n",
        "        img.style = \"margin: 5px; vertical-align: text-top; max-width: 256px; max-height: 512px;\";\n",
        "        }\n",
        "    ''')\n",
        "    height, width = img.shape[:2]\n",
        "    ret, data = cv2.imencode('.png', img)\n",
        "    data = b64encode(data)\n",
        "    data = data.decode()\n",
        "    data = 'data:image/png;base64,' + data\n",
        "    display(js)\n",
        "    eval_js(f'showImage({b}, {i}, \"{name}\", \"{data}\", {width}, {height})')\n",
        "\n",
        "def printPrompt(prompt, limit=12):\n",
        "    pw = prompt.split(\" \"); i=0; oi=0; pstr = ''\n",
        "    for w in pw:\n",
        "        oi+=1; pstr += f'{w} '\n",
        "        if i is limit or oi is len(pw): print(pstr.strip()); pstr = ''; i = 0; pass\n",
        "        i+=1\n",
        "\n",
        "def sharpenImage(image, samples=1):\n",
        "    import PIL\n",
        "    from PIL import Image, ImageFilter\n",
        "    im = image\n",
        "    for i in range(samples):\n",
        "        im = im.filter(ImageFilter.SHARPEN)\n",
        "    return im\n",
        "\n",
        "def setup_pipes(pipe_type='default'):\n",
        "    if pipe_type is 'lowvram':\n",
        "        clean_env()\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(model_id, cache_dir=model_cache, torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "        del pipe.vae.encoder\n",
        "    elif pipe_type is 'img2img':\n",
        "        clean_env()\n",
        "        pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, cache_dir=model_cache, revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\")\n",
        "    elif pipe_type is 'default':\n",
        "        clean_env()\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(model_id, cache_dir=model_cache, use_auth_token=True).to(\"cuda\")\n",
        "    return pipe\n",
        "\n",
        "print(\"\\nStarting Installation Processess.\\nThis should take approximately one eternity...\\n\")\n",
        "\n",
        "try:\n",
        "  with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "    key = f.read().decode('utf-8').split(':')\n",
        "except OSError as e:\n",
        "  print(e)\n",
        "\n",
        "huggingface_username = key[0].strip()\n",
        "huggingface_token = key[1].strip()\n",
        "\n",
        "model_cache = f'{STABLE_DIFFUSION_WORKDIR}/model_cache'\n",
        "if not os.path.exists(model_cache):\n",
        "  os.makedirs(model_cache)\n",
        "\n",
        "if not os.path.exists(f'{model_cache}/sd-v1-4.ckpt'):\n",
        "  # Download model file\n",
        "  wget(f'https://{huggingface_username}:{huggingface_token}@huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt', f'{model_cache}')\n",
        "\n",
        "try:\n",
        "\n",
        "    # Install Joblib\n",
        "    try:\n",
        "        import joblib\n",
        "        from joblib import Memory\n",
        "    except ImportError:\n",
        "        res = subprocess.run(['pip', 'install', 'joblib'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        print(res)\n",
        "    finally:\n",
        "        import joblib\n",
        "        from joblib import Memory\n",
        "        cache_dir = f'{STABLE_DIFFUSION_WORKDIR}/cache'\n",
        "\n",
        "    # Install Shutup\n",
        "    try:\n",
        "        import shutup; \n",
        "    except ImportError:\n",
        "        res = subprocess.run(['pip', 'install', 'shutup'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        print(res)\n",
        "    finally:    \n",
        "        import shutup; \n",
        "        if SUPPRESS_WARNINGS: \n",
        "            shutup.please()\n",
        "\n",
        "    import warnings\n",
        "    if SUPPRESS_WARNINGS:\n",
        "        warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
        "    \n",
        "    res = subprocess.run(['git', 'lfs', 'install'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "    os.environ['GIT_LFS_SKIP_SMUDGE'] = \"0\"\n",
        "    # This will take a while\n",
        "\n",
        "    res = subprocess.run(['git', 'lfs', 'clone', 'https://$huggingface_username:$huggingface_token@huggingface.co/CompVis/stable-diffusion-v1-4'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "    res = subprocess.run(['pip', 'install', '-U', 'git+https://github.com/huggingface/diffusers.git'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "    # Disable NSFW check\n",
        "    if not ENABLE_NSFW_FILTER:\n",
        "        with open('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py','w') as file:\n",
        "            file.write('''\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import CLIPConfig, CLIPVisionModel, PreTrainedModel\n",
        "\n",
        "from ...utils import logging\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "\n",
        "def cosine_distance(image_embeds, text_embeds):\n",
        "    normalized_image_embeds = nn.functional.normalize(image_embeds)\n",
        "    normalized_text_embeds = nn.functional.normalize(text_embeds)\n",
        "    return torch.mm(normalized_image_embeds, normalized_text_embeds.T)\n",
        "\n",
        "\n",
        "class StableDiffusionSafetyChecker(PreTrainedModel):\n",
        "    config_class = CLIPConfig\n",
        "\n",
        "    def __init__(self, config: CLIPConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.vision_model = CLIPVisionModel(config.vision_config)\n",
        "        self.visual_projection = nn.Linear(config.vision_config.hidden_size, config.projection_dim, bias=False)\n",
        "\n",
        "        self.concept_embeds = nn.Parameter(torch.ones(17, config.projection_dim), requires_grad=False)\n",
        "        self.special_care_embeds = nn.Parameter(torch.ones(3, config.projection_dim), requires_grad=False)\n",
        "\n",
        "        self.register_buffer(\"concept_embeds_weights\", torch.ones(17))\n",
        "        self.register_buffer(\"special_care_embeds_weights\", torch.ones(3))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, clip_input, images):\n",
        "        pooled_output = self.vision_model(clip_input)[1]  # pooled_output\n",
        "        image_embeds = self.visual_projection(pooled_output)\n",
        "\n",
        "        special_cos_dist = cosine_distance(image_embeds, self.special_care_embeds).cpu().numpy()\n",
        "        cos_dist = cosine_distance(image_embeds, self.concept_embeds).cpu().numpy()\n",
        "\n",
        "        result = []\n",
        "        batch_size = image_embeds.shape[0]\n",
        "        for i in range(batch_size):\n",
        "            result_img = {\"special_scores\": {}, \"special_care\": [], \"concept_scores\": {}, \"bad_concepts\": []}\n",
        "\n",
        "            # increase this value to create a stronger `nfsw` filter\n",
        "            # at the cost of increasing the possibility of filtering benign images\n",
        "            adjustment = 0.0\n",
        "\n",
        "            for concet_idx in range(len(special_cos_dist[0])):\n",
        "                concept_cos = special_cos_dist[i][concet_idx]\n",
        "                concept_threshold = self.special_care_embeds_weights[concet_idx].item()\n",
        "                result_img[\"special_scores\"][concet_idx] = round(concept_cos - concept_threshold + adjustment, 3)\n",
        "                if result_img[\"special_scores\"][concet_idx] > 0:\n",
        "                    result_img[\"special_care\"].append({concet_idx, result_img[\"special_scores\"][concet_idx]})\n",
        "                    adjustment = 0.01\n",
        "\n",
        "            for concet_idx in range(len(cos_dist[0])):\n",
        "                concept_cos = cos_dist[i][concet_idx]\n",
        "                concept_threshold = self.concept_embeds_weights[concet_idx].item()\n",
        "                result_img[\"concept_scores\"][concet_idx] = round(concept_cos - concept_threshold + adjustment, 3)\n",
        "                if result_img[\"concept_scores\"][concet_idx] > 0:\n",
        "                    result_img[\"bad_concepts\"].append(concet_idx)\n",
        "\n",
        "            result.append(result_img)\n",
        "\n",
        "        has_nsfw_concepts = [len(res[\"bad_concepts\"]) > 0 for res in result]\n",
        "\n",
        "        #for idx, has_nsfw_concept in enumerate(has_nsfw_concepts):\n",
        "        #    if has_nsfw_concept:\n",
        "        #        images[idx] = np.zeros(images[idx].shape)  # black image\n",
        "\n",
        "        if any(has_nsfw_concepts):\n",
        "            logger.warning(\n",
        "                \"Potential NSFW content was detected in one or more images, but the NSFW filter is off.\"\n",
        "            )\n",
        "\n",
        "        return images, has_nsfw_concepts''')\n",
        "\n",
        "    res = subprocess.run(['pip', 'install', 'transformers'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "    # make sure you're logged in with `huggingface-cli login`\n",
        "    from diffusers import StableDiffusionPipeline, LMSDiscreteScheduler\n",
        "\n",
        "    # lms = LMSDiscreteScheduler(\n",
        "    #     beta_start=0.00085, \n",
        "    #     beta_end=0.012, \n",
        "    #     beta_schedule=\"scaled_linear\"\n",
        "    # )\n",
        "\n",
        "    model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "\n",
        "    if not os.path.exists('diffusers_output'):\n",
        "        os.makedirs('diffusers_output')\n",
        "\n",
        "    res = subprocess.run(['pip', 'install',\n",
        "                            'pytorch-pretrained-bert',\n",
        "                            'spacy',\n",
        "                            'ftfy',\n",
        "                            ], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "    res = subprocess.run(['python', '-m', 'spacy', 'download', 'en'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "    res = subprocess.run(['pip', 'install', 'scipy'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "    res = subprocess.run(['git', 'clone', '--recursive', 'https://github.com/crowsonkb/k-diffusion.git'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "    left_of_pipe = subprocess.Popen([\"echo\", huggingface_token], stdout=subprocess.PIPE)\n",
        "    right_of_pipe = subprocess.run(['huggingface-cli', 'login'], stdin=left_of_pipe.stdout, stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(right_of_pipe)\n",
        "\n",
        "    if LOW_VRAM_PATCH:\n",
        "        patched_file = open('/usr/local/lib/python3.7/dist-packages/torch/nn/modules/normalization.py').read().replace('input, self.num_groups, self.weight, self.bias, self.eps)','input, self.num_groups, self.weight.type(input.dtype), self.bias.type(input.dtype), self.eps)')\n",
        "        with open('/usr/local/lib/python3.7/dist-packages/torch/nn/modules/normalization.py','w') as file:\n",
        "            file.write(patched_file)\n",
        "        with open('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py','w') as file:\n",
        "            file.write(\n",
        "  '''\n",
        "import inspect\n",
        "import warnings\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "from ...models import AutoencoderKL, UNet2DConditionModel\n",
        "from ...pipeline_utils import DiffusionPipeline\n",
        "from ...schedulers import DDIMScheduler, LMSDiscreteScheduler, PNDMScheduler\n",
        "from .safety_checker import StableDiffusionSafetyChecker\n",
        "\n",
        "\n",
        "class StableDiffusionPipeline(DiffusionPipeline):\n",
        "  def __init__(\n",
        "      self,\n",
        "      vae: AutoencoderKL,\n",
        "      text_encoder: CLIPTextModel,\n",
        "      tokenizer: CLIPTokenizer,\n",
        "      unet: UNet2DConditionModel,\n",
        "      scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n",
        "      safety_checker: StableDiffusionSafetyChecker,\n",
        "      feature_extractor: CLIPFeatureExtractor,\n",
        "  ):\n",
        "      super().__init__()\n",
        "      scheduler = scheduler.set_format(\"pt\")\n",
        "      self.register_modules(\n",
        "          vae=vae,\n",
        "          text_encoder=text_encoder,\n",
        "          tokenizer=tokenizer,\n",
        "          unet=unet,\n",
        "          scheduler=scheduler,\n",
        "          safety_checker=safety_checker,\n",
        "          feature_extractor=feature_extractor,\n",
        "      )\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def __call__(\n",
        "      self,\n",
        "      prompt: Union[str, List[str]],\n",
        "      height: Optional[int] = 512,\n",
        "      width: Optional[int] = 512,\n",
        "      num_inference_steps: Optional[int] = 50,\n",
        "      guidance_scale: Optional[float] = 7.5,\n",
        "      eta: Optional[float] = 0.0,\n",
        "      generator: Optional[torch.Generator] = None,\n",
        "      output_type: Optional[str] = \"pil\",\n",
        "      **kwargs,\n",
        "  ):\n",
        "      if \"torch_device\" in kwargs:\n",
        "          device = kwargs.pop(\"torch_device\")\n",
        "          warnings.warn(\n",
        "              \"`torch_device` is deprecated as an input argument to `__call__` and will be removed in v0.3.0.\"\n",
        "              \" Consider using `pipe.to(torch_device)` instead.\"\n",
        "          )\n",
        "\n",
        "          # Set device as before (to be removed in 0.3.0)\n",
        "          if device is None:\n",
        "              device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "          self.to(device)\n",
        "\n",
        "      if isinstance(prompt, str):\n",
        "          batch_size = 1\n",
        "      elif isinstance(prompt, list):\n",
        "          batch_size = len(prompt)\n",
        "      else:\n",
        "          raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
        "\n",
        "      if height % 8 != 0 or width % 8 != 0:\n",
        "          raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n",
        "\n",
        "      # get prompt text embeddings\n",
        "      text_input = self.tokenizer(\n",
        "          prompt,\n",
        "          padding=\"max_length\",\n",
        "          max_length=self.tokenizer.model_max_length,\n",
        "          truncation=True,\n",
        "          return_tensors=\"pt\",\n",
        "      )\n",
        "      text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
        "\n",
        "      # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "      # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "      # corresponds to doing no classifier free guidance.\n",
        "      do_classifier_free_guidance = guidance_scale > 1.0\n",
        "      # get unconditional embeddings for classifier free guidance\n",
        "      if do_classifier_free_guidance:\n",
        "          max_length = text_input.input_ids.shape[-1]\n",
        "          uncond_input = self.tokenizer(\n",
        "              [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "          )\n",
        "          uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
        "\n",
        "          # For classifier free guidance, we need to do two forward passes.\n",
        "          # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "          # to avoid doing two forward passes\n",
        "          text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "      # get the intial random noise\n",
        "      latents = torch.randn(\n",
        "          (batch_size, self.unet.in_channels, height // 8, width // 8),\n",
        "          generator=generator,\n",
        "          device=self.device,\n",
        "      )\n",
        "      latents = latents.half()\n",
        "\n",
        "      # set timesteps\n",
        "      accepts_offset = \"offset\" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())\n",
        "      extra_set_kwargs = {}\n",
        "      if accepts_offset:\n",
        "          extra_set_kwargs[\"offset\"] = 1\n",
        "\n",
        "      self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
        "\n",
        "      # if we use LMSDiscreteScheduler, let's make sure latents are mulitplied by sigmas\n",
        "      if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "          latents = latents * self.scheduler.sigmas[0]\n",
        "\n",
        "      # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
        "      # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
        "      # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n",
        "      # and should be between [0, 1]\n",
        "      accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
        "      extra_step_kwargs = {}\n",
        "      if accepts_eta:\n",
        "          extra_step_kwargs[\"eta\"] = eta\n",
        "\n",
        "      steps_bar = tqdm(range(num_inference_steps), desc='Steps')\n",
        "      #for i, t in tqdm(enumerate(self.scheduler.timesteps)):\n",
        "      for i, t in enumerate(self.scheduler.timesteps):\n",
        "          steps_bar.n = i\n",
        "          steps_bar.refresh()\n",
        "          # expand the latents if we are doing classifier free guidance\n",
        "          latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "          if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "              sigma = self.scheduler.sigmas[i]\n",
        "              latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "          # predict the noise residual\n",
        "          noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "          # perform guidance\n",
        "          if do_classifier_free_guidance:\n",
        "              noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "              noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "          # compute the previous noisy sample x_t -> x_t-1\n",
        "          if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "              latents = self.scheduler.step(noise_pred, i, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "          else:\n",
        "              latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "\n",
        "      # scale and decode the image latents with vae\n",
        "      latents = 1 / 0.18215 * latents\n",
        "      image = self.vae.decode(latents)\n",
        "\n",
        "      image = (image / 2 + 0.5).clamp(0, 1)\n",
        "      image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "      # run safety checker\n",
        "      safety_cheker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(self.device)\n",
        "      image, has_nsfw_concept = self.safety_checker(images=image, clip_input=safety_cheker_input.pixel_values)\n",
        "\n",
        "      if output_type == \"pil\":\n",
        "          image = self.numpy_to_pil(image)\n",
        "\n",
        "      return {\"sample\": image, \"nsfw_content_detected\": has_nsfw_concept}\n",
        "  ''')\n",
        "\n",
        "    with open('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py', 'w') as file:\n",
        "        file.write(\n",
        "  '''\n",
        "import inspect\n",
        "import warnings\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "from ...models import AutoencoderKL, UNet2DConditionModel\n",
        "from ...pipeline_utils import DiffusionPipeline\n",
        "from ...schedulers import DDIMScheduler, LMSDiscreteScheduler, PNDMScheduler\n",
        "from .safety_checker import StableDiffusionSafetyChecker\n",
        "\n",
        "\n",
        "class StableDiffusionPipeline(DiffusionPipeline):\n",
        "  def __init__(\n",
        "      self,\n",
        "      vae: AutoencoderKL,\n",
        "      text_encoder: CLIPTextModel,\n",
        "      tokenizer: CLIPTokenizer,\n",
        "      unet: UNet2DConditionModel,\n",
        "      scheduler: Union[DDIMScheduler, PNDMScheduler, LMSDiscreteScheduler],\n",
        "      safety_checker: StableDiffusionSafetyChecker,\n",
        "      feature_extractor: CLIPFeatureExtractor,\n",
        "  ):\n",
        "      super().__init__()\n",
        "      scheduler = scheduler.set_format(\"pt\")\n",
        "      self.register_modules(\n",
        "          vae=vae,\n",
        "          text_encoder=text_encoder,\n",
        "          tokenizer=tokenizer,\n",
        "          unet=unet,\n",
        "          scheduler=scheduler,\n",
        "          safety_checker=safety_checker,\n",
        "          feature_extractor=feature_extractor,\n",
        "      )\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def __call__(\n",
        "      self,\n",
        "      prompt: Union[str, List[str]],\n",
        "      height: Optional[int] = 512,\n",
        "      width: Optional[int] = 512,\n",
        "      num_inference_steps: Optional[int] = 50,\n",
        "      guidance_scale: Optional[float] = 7.5,\n",
        "      eta: Optional[float] = 0.0,\n",
        "      generator: Optional[torch.Generator] = None,\n",
        "      output_type: Optional[str] = \"pil\",\n",
        "      **kwargs,\n",
        "  ):\n",
        "      if \"torch_device\" in kwargs:\n",
        "          device = kwargs.pop(\"torch_device\")\n",
        "          warnings.warn(\n",
        "              \"`torch_device` is deprecated as an input argument to `__call__` and will be removed in v0.3.0.\"\n",
        "              \" Consider using `pipe.to(torch_device)` instead.\"\n",
        "          )\n",
        "\n",
        "          # Set device as before (to be removed in 0.3.0)\n",
        "          if device is None:\n",
        "              device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "          self.to(device)\n",
        "\n",
        "      if isinstance(prompt, str):\n",
        "          batch_size = 1\n",
        "      elif isinstance(prompt, list):\n",
        "          batch_size = len(prompt)\n",
        "      else:\n",
        "          raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
        "\n",
        "      if height % 8 != 0 or width % 8 != 0:\n",
        "          raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n",
        "\n",
        "      # get prompt text embeddings\n",
        "      text_input = self.tokenizer(\n",
        "          prompt,\n",
        "          padding=\"max_length\",\n",
        "          max_length=self.tokenizer.model_max_length,\n",
        "          truncation=True,\n",
        "          return_tensors=\"pt\",\n",
        "      )\n",
        "      text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
        "\n",
        "      # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "      # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "      # corresponds to doing no classifier free guidance.\n",
        "      do_classifier_free_guidance = guidance_scale > 1.0\n",
        "      # get unconditional embeddings for classifier free guidance\n",
        "      if do_classifier_free_guidance:\n",
        "          max_length = text_input.input_ids.shape[-1]\n",
        "          uncond_input = self.tokenizer(\n",
        "              [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "          )\n",
        "          uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
        "\n",
        "          # For classifier free guidance, we need to do two forward passes.\n",
        "          # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "          # to avoid doing two forward passes\n",
        "          text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "      # get the intial random noise\n",
        "      latents = torch.randn(\n",
        "          (batch_size, self.unet.in_channels, height // 8, width // 8),\n",
        "          generator=generator,\n",
        "          device=self.device,\n",
        "      )\n",
        "      latents = latents.half()\n",
        "\n",
        "      # set timesteps\n",
        "      accepts_offset = \"offset\" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())\n",
        "      extra_set_kwargs = {}\n",
        "      if accepts_offset:\n",
        "          extra_set_kwargs[\"offset\"] = 1\n",
        "\n",
        "      self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
        "\n",
        "      # if we use LMSDiscreteScheduler, let's make sure latents are mulitplied by sigmas\n",
        "      if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "          latents = latents * self.scheduler.sigmas[0]\n",
        "\n",
        "      # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
        "      # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
        "      # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n",
        "      # and should be between [0, 1]\n",
        "      accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
        "      extra_step_kwargs = {}\n",
        "      if accepts_eta:\n",
        "          extra_step_kwargs[\"eta\"] = eta\n",
        "\n",
        "      steps_bar = tqdm(range(num_inference_steps), desc='Steps')\n",
        "      #for i, t in tqdm(enumerate(self.scheduler.timesteps)):\n",
        "      for i, t in enumerate(self.scheduler.timesteps):\n",
        "          steps_bar.n = i\n",
        "          steps_bar.refresh()\n",
        "          # expand the latents if we are doing classifier free guidance\n",
        "          latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "          if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "              sigma = self.scheduler.sigmas[i]\n",
        "              latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "          # predict the noise residual\n",
        "          noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "          # perform guidance\n",
        "          if do_classifier_free_guidance:\n",
        "              noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "              noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "          # compute the previous noisy sample x_t -> x_t-1\n",
        "          if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "              latents = self.scheduler.step(noise_pred, i, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "          else:\n",
        "              latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "\n",
        "      # scale and decode the image latents with vae\n",
        "      latents = 1 / 0.18215 * latents\n",
        "      image = self.vae.decode(latents)\n",
        "\n",
        "      image = (image / 2 + 0.5).clamp(0, 1)\n",
        "      image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "      # run safety checker\n",
        "      safety_cheker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(self.device)\n",
        "      safety_cheker_input.pixel_values = safety_cheker_input.pixel_values.half()\n",
        "      image, has_nsfw_concept = self.safety_checker(images=image, clip_input=safety_cheker_input.pixel_values)\n",
        "\n",
        "      if output_type == \"pil\":\n",
        "          image = self.numpy_to_pil(image)\n",
        "\n",
        "      return {\"sample\": image, \"nsfw_content_detected\": has_nsfw_concept}\n",
        "  ''')\n",
        "        \n",
        "    # Image-to-Image\n",
        "    if not os.path.exists('image_to_image.py'):\n",
        "        wget('https://raw.githubusercontent.com/huggingface/diffusers/4674fdf807cdefd4db1758067c0207872d805f8c/examples/inference/image_to_image.py', './')\n",
        "    import requests\n",
        "    from io import BytesIO\n",
        "    from image_to_image import StableDiffusionImg2ImgPipeline, preprocess\n",
        "        \n",
        "    # Setup Piping Cache\n",
        "    if CACHE_PIPELINES:\n",
        "        print('\\n:gear: Setting up Stable Diffusion Pipeline...')\n",
        "        model_cache = f'{STABLE_DIFFUSION_WORKDIR}/model_cache'\n",
        "        pipe_cache = f'{STABLE_DIFFUSION_WORKDIR}/cache'\n",
        "        if not os.path.exists(model_cache):\n",
        "            os.makedirs(model_cache)\n",
        "\n",
        "        if not os.path.exists(pipe_cache):\n",
        "            os.makedirs(pipe_cache)\n",
        "\n",
        "        # DUMP PIPING\n",
        "        clean_env()\n",
        "        if LOW_VRAM_PATCH:\n",
        "            if not os.path.exists(f'{pipe_cache}/LOW_VRAM_PIPE.obj') or RECACHE_PIPES:\n",
        "                joblib.dump(StableDiffusionPipeline.from_pretrained(model_id, cache_dir=model_cache, torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\"), f'{pipe_cache}/LOW_VRAM_PIPE.obj')\n",
        "                #del piping['LOW_VRAM'].vae.encoder\n",
        "                clean_env()\n",
        "        if not os.path.exists(f'{pipe_cache}/IMG2IMG_PIPE.obj') or RECACHE_PIPES:\n",
        "            joblib.dump(StableDiffusionImg2ImgPipeline.from_pretrained(model_id, cache_dir=model_cache, revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=True).to(\"cuda\"), f'{pipe_cache}/IMG2IMG_PIPE.obj')\n",
        "            clean_env()\n",
        "        if not os.path.exists(f'{pipe_cache}/DEFAULT.obj') or RECACHE_PIPES:\n",
        "            joblib.dump(StableDiffusionPipeline.from_pretrained(model_id, cache_dir=model_cache, use_auth_token=True).to(\"cuda\"), f'{pipe_cache}/DEFAULT_PIPE.obj')\n",
        "            clean_env()\n",
        "        print(\":check_mark_button: Pipeline setup complete.\\n\")\n",
        "\n",
        "    if INSTALL_GFPGAN:\n",
        "        print(\"\\n:hourglass_not_done: Installing GFPGAN...\")\n",
        "        if not os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/GFPGAN'):\n",
        "            print(subprocess.run(['git', 'clone', 'https://github.com/TencentARC/GFPGAN.git'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/GFPGAN')\n",
        "            # Set up the environment\n",
        "            # used for enhancing the background (non-face) regions\n",
        "            # Download the pre-trained model\n",
        "            # Now we use the V1.3 model for the demo\n",
        "            wget(\"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\", \"experiments/pretrained_models\")\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "            \n",
        "        # Install basicsr - https://github.com/xinntao/BasicSR\n",
        "        # We use BasicSR for both training and inference\n",
        "        # Install facexlib - https://github.com/xinntao/facexlib\n",
        "        # We use face detection and face restoration helper in the facexlib package\n",
        "        # Install other depencencies\n",
        "        print(subprocess.run(['pip', 'install', 'basicsr', 'facexlib'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(subprocess.run(['pip', 'install', '-r', 'requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(subprocess.run(['python', 'setup.py', 'develop'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(subprocess.run(['pip', 'install', 'realesrgan'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(\":check_mark_button: GFPGAN installed!\\n\")\n",
        "        \n",
        "    if INSTALL_ESRGAN:\n",
        "        print(\"\\n:hourglass_not_done: Installing Real-ESRGAN\")\n",
        "        if not os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/Real-ESRGAN'):\n",
        "            print(subprocess.run(['git', 'clone', 'https://github.com/sberbank-ai/Real-ESRGAN'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            print(subprocess.run(['pip', 'install', '-r', 'Real-ESRGAN/requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            wget(\"https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x2.pth\", \"Real-ESRGAN/weights/\")\n",
        "            wget(\"https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x4.pth\", \"Real-ESRGAN/weights/\")\n",
        "            wget(\"https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x8.pth\", \"Real-ESRGAN/weights/\")\n",
        "        print(\":check_mark_button: Real-ESRGAN installed!\")\n",
        "        \n",
        "        def upscale(image, scale, device='cuda'):\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/Real-ESRGAN')\n",
        "            from realesrgan import RealESRGAN\n",
        "            device = torch.device(device)\n",
        "            model = RealESRGAN(device, scale = scale)\n",
        "            model.load_weights(f'weights/RealESRGAN_x{scale}.pth')\n",
        "            sr_image = model.predict(np.array(image))\n",
        "            del model, device\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}')\n",
        "            return sr_image\n",
        "\n",
        "    if INSTALL_CODEFORMER:\n",
        "        print(\":hourglass_not_done: Installing CodeFormer...\\n\")\n",
        "        if not os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer'):\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "            print(subprocess.run(['git', 'clone', 'https://github.com/sczhou/CodeFormer.git'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "\n",
        "        os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer')\n",
        "        print(subprocess.run(['pip', 'install', '-r', 'requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        # Install basicsr\n",
        "        print(subprocess.run(['python', 'basicsr/setup.py', 'develop'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "\n",
        "        # Download the pre-trained model\n",
        "        print(subprocess.run(['python', 'scripts/download_pretrained_models.py', 'facelib'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(subprocess.run(['python', 'scripts/download_pretrained_models.py', 'CodeFormer'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        os.makedirs('temp', exist_ok=True)\n",
        "        os.makedirs('results', exist_ok=True)\n",
        "        os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "        print(\":check_mark_button: CodeFormer installed!\")\n",
        "\n",
        "    if INSTALL_KROMO:\n",
        "        print(\":hourglass_not_done: Installing Kromo...\")\n",
        "        if not os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/kromo'):\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "            print(subprocess.run(['git', 'clone', 'https://github.com/yoonsikp/kromo'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/kromo')\n",
        "        print(subprocess.run(['pip', 'install', '-r', 'requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "\n",
        "    if INSTALL_MIDAS:\n",
        "        print(\":hourglass_not_done: Installing MiDaS compatibility...\")\n",
        "        print(subprocess.run(['pip', 'install', 'timm'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(\":check_mark_button: MiDaS compatibility installed!\\n\")\n",
        "\n",
        "    # Noodle Soup prompts\n",
        "    try:\n",
        "        import nsp_pantry\n",
        "    except ImportError:\n",
        "        if not os.path.exists('nsp_pantry.py'):\n",
        "            print(\":hourglass_not_done: Installing Noodle Soup Prompts...\")\n",
        "            wget('https://raw.githubusercontent.com/WASasquatch/noodle-soup-prompts/main/nsp_pantry.py', './')\n",
        "    finally:\n",
        "        import nsp_pantry\n",
        "        from nsp_pantry import nspterminology, nsp_parse\n",
        "\n",
        "    if nsp_parse and nspterminology:\n",
        "        print(\"\\r\\r:check_mark_button: \\33[32mNSP installed successfuly.\\33[0m \\x1B[3mMmm... Noodle Soup.\\x1B[0m\\n\")\n",
        "\n",
        "except OSError as e:\n",
        "    raise e\n",
        "except BaseException as e:\n",
        "    raise e\n",
        "finally:\n",
        "    if CLEAR_SETUP_LOG: clear()\n",
        "    print(f\"\\n--[ :confetti_ball::party_popper: \\033[1m\\33[32mEasy Diffusion Environtment Setup Complete\\33[0m :party_popper::confetti_ball: ]--\")\n",
        "\n",
        "from PIL import Image\n",
        "import random, pprint\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from torch import autocast\n",
        "from diffusers.schedulers import PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ucr5_i21xSjv"
      },
      "outputs": [],
      "source": [
        "#@title <font size=\"5\" color=\"green\">**Settings & Diffuse**</font>\n",
        " \n",
        "clean_env()\n",
        "init = None # Clear/set init for run\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #### **Prompt Setup**\n",
        "#@markdown <font size=\"3\">Prompts support [Noodle Soup Prompts](https://github.com/WASasquatch/noodle-soup-prompts/wiki/Terminology-Reference) \\([NSP Prompt Generator](https://rebrand.ly/noodle-soup-prompts)\\)</font>\n",
        "PROMPT = \"A stylish beautiful 3d render portrait of _noun-emote_ cat in a _color_ space helmet on the moon\" #@param {type:'string'}\n",
        "PROMPT_FILE = '' #@param {type: 'string'}\n",
        "#@markdown <font size=\"3\">`PROMPT_FILE` is a optional text file that contains a prompt ***per*** line.</font>\n",
        "NEW_NSP_ON_ITERATION = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Whether to generate NSP once, or on each iteration. Check this if you want each iteration to have a freshly cooked noodle prompt.</font>\n",
        "SAVE_PROMPT_DETAILS = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #### **Init Image Setup**\n",
        "INIT_IMAGE = \"\" #@param {type: 'string'}\n",
        "#@markdown <font size=\"3\">`INIT_IMAGE` accepts the following formats</font>\n",
        "#@markdown - <font size=\"3\">A single local, or remote image</font>\n",
        "#@markdown - <font size=\"3\">A `.txt` file containing a single local, or remote image ***per*** line.</font>\n",
        "#@markdown - <font size=\"3\">A path to a local folder containing images.</font>\n",
        "INIT_STRENGTH = 0.5 #@param{type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #### **Diffusion Settings**\n",
        "SAMPLER = 'DEFAULT' #@param [\"DEFAULT\", \"PNDM\", \"LMS\", \"DDIM\"]\n",
        "DDIM_ETA = 0.65 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown <font size=\"3\">`DDIM_ETA` only applies to the DDIM sampler.</font>\n",
        "STEPS = 145 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "#@markdown <font size=\"3\">Diffusion steps determines the quality of the final image</font>\n",
        "SEED = 0 #@param {type:'integer'}\n",
        "#@markdown <font size=\"3\">The seed used for the generation. Leave at `0` for random.</font>\n",
        "INCREMENT_ITERATION_SEED = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Disable this if you want a new random seed each iteration, or the same unique seed each iteration.</font>\n",
        "NUM_ITERS = 2 #@param {type:\"slider\", min:1, max:100, step:1} \n",
        "#@markdown <font size=\"3\">Number of iterations for a given prompt.</font>\n",
        "WIDTH = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "HEIGHT = 512 #@param {type:\"slider\", min:256, max:1920, step:64}\n",
        "SCALE = 13.5 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "#@markdown <font size=\"3\">The CFG `SCALE` determines how closely a generation follows the prompt, or improvisation. Lower values will try to adhear to your prompt.</font>\n",
        "PRECISION = \"autocast\" #@param [\"full\",\"autocast\"]\n",
        "#@markdown <font size=\"3\">If you're using the `LOW_VRAM_PATCH` you <b>must</b> use `autocast`</font><br>\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #### **Upscaling Settings**\n",
        "#@markdown <font size=\"3\">`IMAGE_UPSCALER`: may not work at resolutions above 512x768/768x512 on GPUs with ~16GB VRAM.<br>**Note:** GFPGAN is good for faces only, and can create visual artifacts if the subject doesn't fill the frame</font>\n",
        "IMAGE_UPSCALER = \"None\" #@param [\"None\",\"GFPGAN\",\"Enhanced Real-ESRGAN\", \"GFPGAN + Enhanced ESRGAN\", \"CodeFormer\", \"CodeFormer + Enhanced ESRGAN\"]\n",
        "UPSCALE_AMOUNT = 2 #@param {type:\"slider\", min:2, max:8, step:2}\n",
        "CODEFORMER_FIDELITY = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown <font size=\"3\">`CODEFORMER_FIDELITY`: Only applies if the upscaler includes Codeformer. Balance the quality (lower number) and fidelity (higher number)</font><br>\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #### **Image Adjustments**\n",
        "SCALE_DOWN_ENHANCEMENTS_FOR_ESRGAN = True #@param{type:'boolean'}\n",
        "#@markdown <font size=\"3\">Scale down enhanced images. Useful if you are also using Real-ESRGAN. This will preserve your upscale factor for Real-ESRGAN after GFPGAN or CodeFormer.</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Sharpen Image\n",
        "#@markdown <font size=\"3\">Sharpen the base diffusion image before upscsaling.</font>\n",
        "SHARPEN_AMOUNT = 1 #@param{type:'slider', min:0, max:3, step:1}\n",
        "#@markdown <font size=\"3\">Sharpen iteration amount. `0` for no sharpen.</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Kromo Chromatic Aberration\n",
        "CA_DIFFUSE_IMAGE = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Apply Chromatic Aberration to the base diffusion image (pre sharpen if enabled)</font>\n",
        "CA_STRENGTH = 0.2 #@param {type:\"slider\", min:0, max:5, step:0.1}\n",
        "#@markdown <font size=\"3\">Chromatic Aberration strength</font>\n",
        "CA_JITTER = 1 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "#@markdown <font size=\"3\">Chromatic Aberration set channel offset pixels</font>\n",
        "CA_OVERLAY = 0.1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown <font size=\"3\">Alpha of original image overlay.</font>\n",
        "CA_NO_RADIAL_BLUR = False #@param{type: 'boolean'}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### MiDaS Depth Map\n",
        "EXPORT_MIDAS_DEPTH = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Save a MiDaS depth approximation of the diffusion result</font>\n",
        "MIDAS_TYPE = \"DPT_Large\" #@param [\"DPT_Large\",\"DPT_Hybrid\",\"MiDaS_small\"]\n",
        "#@markdown <font size=\"3\">`MIDAS_TYPE` determines the model to use for depth approximation.</font>\n",
        "MIDAS_MODE = \"CPU\" #@param [\"CPU\",\"CUDA\"]\n",
        "#@markdown <font size=\"3\">**CPU Mode:** If you get: \"`RuntimeError: \"linspace_cpu\" not implemented for 'Half'`\" something has changed with CPU and you need to disconnect/reconnect (Google Colab)</font>\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #### **Other Settings**\n",
        "IMAGES_FOLDER = \"time_to_stabilize\" #@param {type: 'string'}\n",
        "#@markdown <font size=\"3\">Define a custom folder to saves images within your `images_out` folder. Example: `CAR_CONCEPTS`</font><br>\n",
        "#@markdown <font size=\"3\">**Note:** Path: `/content/Stable_Diffusion/images_out` or with Google Drive `/content/drive/MyDrive/AI/Stable_Diffusion/images_out`</font>\n",
        "USE_BASIC_IMAGE_DISPLAY = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Use basic image output instead of organized JS Image Output</font>\n",
        "\n",
        "if LOW_VRAM_PATCH and PRECISION is not 'autocast': \n",
        "    print(f\"PRECISION must be 'autocast' when running in low vram compatibility mode! Defaulting to autocast...\")\n",
        "    PRECISION = 'autocast'\n",
        "precision_scope = autocast if PRECISION==\"autocast\" else nullcontext\n",
        "ORIG_SEED = SEED\n",
        "\n",
        "os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "\n",
        "GDRIVE_OUT_PATH = f'{GDRIVE_WORKDIR}/images_out/{IMAGES_FOLDER}'\n",
        "if USE_DRIVE_FOR_PICS:\n",
        "    if not os.path.exists(GDRIVE_OUT_PATH):\n",
        "        os.makedirs(GDRIVE_OUT_PATH)\n",
        "    OUTDIR = GDRIVE_OUT_PATH        \n",
        "\n",
        "print(f\"Images Output Directory: {OUTDIR}\\n\")\n",
        "\n",
        "# Check Upscaling Mode\n",
        "if IMAGE_UPSCALER == 'GFPGAN' and not INSTALL_GFPGAN:\n",
        "    print(\":WARNING: GFPGAN Face Restoration is not installed. Disabling upscaling...\")\n",
        "    IMAGE_UPSCALER = 'None'\n",
        "if IMAGE_UPSCALER == 'Enhanced Real-ESRGAN' and not INSTALL_ESRGAN:\n",
        "    print(\":WARNING: Real-ESRGAN is not installed. Disabling upscaling...\")\n",
        "    IMAGE_UPSCALER = 'None'\n",
        "if IMAGE_UPSCALER == 'CodeFormer' and not INSTALL_CODEFORMER:\n",
        "    print(\":WARNING: CodeFormer is not installed! Disabling upscaling...\")\n",
        "    IMAGE_UPSCALER = 'None'\n",
        "if IMAGE_UPSCALER == 'GFPGAN + Enhanced ESRGAN':\n",
        "    if not INSTALL_GFPGAN and INSTALL_ESRGAN:\n",
        "        print(\":WARNING: GFPGAN is not installed, defaulting to Real-ESRGAN...\")\n",
        "        IMAGE_UPSCALER = 'Enhanced Real-ESRGAN'\n",
        "    if not INSTALL_ESRGAN and INSTALL_GFPGAN:\n",
        "        print(\":WARNING: Real-ESRGAN is not installed, defaulting to GFPGAN...\")\n",
        "        IMAGE_UPSCALER = 'GFPGAN'\n",
        "if IMAGE_UPSCALER == 'CodeFormer + Enhanced ESRGAN':\n",
        "    if not INSTALL_CODEFORMER and INSTALL_ESRGAN:\n",
        "        print(\":WARNING: CodeFormer is not installed, defaulting to Real-ESRGAN...\")\n",
        "        IMAGE_UPSCALER = 'Enhanced Real-ESRGAN'\n",
        "    if not INSTALL_ESRGAN and INSTALL_CODEFORMER:\n",
        "        print(\":WARNING: Real-ESRGAN is not installed, defaulting to CodeFormer...\")\n",
        "        IMAGE_UPSCALER = 'CodeFormer'\n",
        "\n",
        "def closest_value(input_list, input_value):\n",
        "    difference = lambda input_list : abs(input_list - input_value)\n",
        "    res = min(input_list, key=difference)\n",
        "    return res\n",
        "\n",
        "nearest_value = closest_value([2,4,8],UPSCALE_AMOUNT)\n",
        "\n",
        "# Diffuse Function\n",
        "def diffuse_run():\n",
        "\n",
        "    clean_env()\n",
        "\n",
        "    global SEED, UPSCALE_AMOUNT\n",
        "    if not CACHE_PIPELINES: global pipe\n",
        "\n",
        "    if ORIG_SEED is 0 and SEED is 0:\n",
        "        SEED = random.randint(0,sys.maxsize)\n",
        "    else:\n",
        "        if INCREMENT_ITERATION_SEED and iteration > 0:\n",
        "            SEED += 1\n",
        "\n",
        "    gen_seed = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "    epoch_time = int(time.time())\n",
        "    eta_prev = f' (ETA: {DDIM_ETA})' if SAMPLER is 'DDIM' else ''\n",
        "    print(f\"\\n\\033[1mIteration {iteration}\\033[0m\")\n",
        "    print(f':seedling: Seed: \\033[1m{SEED}\\033[0m, :triangular_ruler: Scale: \\033[1m{SCALE}\\033[0m, :footprints: Steps: \\033[1m{STEPS}\\033[0m, :artist_palette: Sampler: {SAMPLER}{eta_prev} :framed_picture: Resolution: \\033[1m{WIDTH}x{HEIGHT}')\n",
        "    midas_prev = f' (Type: \\033[1m{MIDAS_TYPE}\\033[0m, Mode: \\033[1m{MIDAS_MODE}\\033[0m)' if EXPORT_MIDAS_DEPTH else ''\n",
        "    ca_prev = f' (Strength: \\033[1m{CA_STRENGTH}\\033[0m, Jitter: \\033[1m{CA_JITTER}\\033[0m, Overlay: \\033[1m{CA_OVERLAY}\\033[0m, No Radial Blur: \\033[1m{CA_NO_RADIAL_BLUR}\\033[0m)\\n' if CA_DIFFUSE_IMAGE else ''\n",
        "    print(f'Scale Down: \\033[1m{SCALE_DOWN_ENHANCEMENTS_FOR_ESRGAN}\\033[0m, Sharpen Passes: \\033[1m{SHARPEN_AMOUNT}\\033[0m, Chromatic Aberration: \\033[1m{CA_DIFFUSE_IMAGE}\\033[0m{ca_prev}Depth Export: \\033[1m{EXPORT_MIDAS_DEPTH}\\033[0m{midas_prev}\\n')\n",
        "    print(\"\\033[0m:black_nib: Prompt:\\033[1m\")\n",
        "    printPrompt(PROMPT)\n",
        "    print(\"\\033[0m\\n\")\n",
        "\n",
        "    if init is not None:\n",
        "        if USE_BASIC_IMAGE_DISPLAY:\n",
        "            print(\"Resized Init Image:\")\n",
        "            display(original_init)\n",
        "        else:\n",
        "            displayJsImage(i, iteration, f'Resized Init Image B: {i} I: {iteration}', original_init)\n",
        "\n",
        "    if CACHE_PIPELINES:\n",
        "        clean_env()\n",
        "        stt = int(time.time())\n",
        "        print(':gear: Loading Stable Diffusion Pipeline from cache...')\n",
        "        if init is None and LOW_VRAM_PATCH:\n",
        "            pipe = joblib.load(f'{STABLE_DIFFUSION_WORKDIR}/cache/LOW_VRAM_PIPE.obj')\n",
        "            del pipe.vae.encoder\n",
        "            clean_env()\n",
        "        elif init is not None:\n",
        "            pipe = joblib.load(f'{STABLE_DIFFUSION_WORKDIR}/cache/IMG2IMG_PIPE.obj')\n",
        "            clean_env()\n",
        "        else:\n",
        "            pipe = joblib.load(f'{STABLE_DIFFUSION_WORKDIR}/cache/DEFAULT_PIPE.obj')\n",
        "            clean_env()\n",
        "        fnt = time_format(int(time.time()) - stt)\n",
        "        print(f':check_mark_button: Pipeline loaded in {fnt}')\n",
        "\n",
        "    if SAMPLER is 'DEFAULT':\n",
        "        pipe.scheduler = PNDMScheduler (\n",
        "            beta_end= 0.012,\n",
        "            beta_schedule= \"scaled_linear\",\n",
        "            beta_start= 0.00085,\n",
        "            num_train_timesteps= 1000,\n",
        "            skip_prk_steps= True\n",
        "        )\n",
        "    if SAMPLER == 'PNDM':\n",
        "        pipe.scheduler = PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "    elif SAMPLER == 'LMS':\n",
        "        pipe.scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "    elif SAMPLER == 'DDIM':\n",
        "        pipe.scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "\n",
        "    # Diffusion Piping\n",
        "    try:\n",
        "        stt = int(time.time())\n",
        "        print(f\":alembic: Starting Diffusion run with {model_id}\")\n",
        "        if init is not None:\n",
        "            with autocast(\"cuda\"):\n",
        "                image = pipe(prompt=PROMPT, num_inference_steps=STEPS, init_image=init, strength=INIT_STRENGTH, guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]\n",
        "        else:\n",
        "            if SAMPLER == 'ddim':\n",
        "                image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, eta=DDIM_ETA, generator=gen_seed)[\"sample\"][0]\n",
        "            else:\n",
        "                image = pipe(PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, generator=gen_seed)[\"sample\"][0]\n",
        "    except BaseException as e:\n",
        "        raise e\n",
        "    finally:\n",
        "        fnt = time_format(int(time.time()) - stt)\n",
        "        print(f':check_mark_button: Diffusion completed in {fnt}')\n",
        "        clean_env()\n",
        "\n",
        "    filename = f'{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png'\n",
        "    filedir = f'{OUTDIR}/{filename}'\n",
        "    image.save(filedir)\n",
        "\n",
        "    if INSTALL_MIDAS:\n",
        "        if EXPORT_MIDAS_DEPTH:\n",
        "            stt = int(time.time())\n",
        "            print(\"Approximating diffusion depth...\")\n",
        "            midas = torch.hub.load(\"intel-isl/MiDaS\", MIDAS_TYPE)\n",
        "            if MIDAS_MODE is 'CUDA':\n",
        "                device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "            else:\n",
        "                device = torch.device(\"cpu\")\n",
        "            midas.to(device)\n",
        "            midas.eval()\n",
        "            midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "\n",
        "            if MIDAS_TYPE == \"DPT_Large\" or MIDAS_TYPE == \"DPT_Hybrid\":\n",
        "                transform = midas_transforms.dpt_transform\n",
        "            else:\n",
        "                transform = midas_transforms.small_transform\n",
        "\n",
        "            import cv2\n",
        "            img = cv2.imread(filedir)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            input_batch = transform(img).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                prediction = midas(input_batch)\n",
        "\n",
        "                prediction = torch.nn.functional.interpolate(\n",
        "                    prediction.unsqueeze(1),\n",
        "                    size=img.shape[:2],\n",
        "                    mode=\"bicubic\",\n",
        "                    align_corners=False,\n",
        "                ).squeeze()\n",
        "\n",
        "            depth = prediction.cpu().numpy()\n",
        "            depth = (depth * 255 / (np.max(depth)+1)).astype('uint8')\n",
        "            depth_image = Image.fromarray(depth)\n",
        "            depth_image.save(filedir.replace('.png', '_depth.png'))\n",
        "            del midas, device, midas_transforms, transform, img, input_batch, prediction, depth\n",
        "            fnt = time_format(int(time.time()) - stt)\n",
        "            print(f'Depth approximation completed in {fnt}')\n",
        "            clean_env()\n",
        "\n",
        "    if INSTALL_KROMO:\n",
        "        if CA_DIFFUSE_IMAGE:\n",
        "            stt = int(time.time())\n",
        "            print(f\"Applying chromatic aberration to result image.\\n\")\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/kromo')\n",
        "            ca_no_blur = '-n ' if CA_NO_RADIAL_BLUR else ''\n",
        "            print(subprocess.run(f'python kromo.py -s {CA_STRENGTH} -j {CA_JITTER} -y {CA_OVERLAY} {ca_no_blur}-o {filedir} {filedir}'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            image = Image.open(filedir).resize((WIDTH,HEIGHT))\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "            fnt = time_format(int(time.time() - stt))\n",
        "            print(f'Chromatic aberration applied in {fnt}')\n",
        "        clean_env()\n",
        "\n",
        "    if SHARPEN_AMOUNT > 0:\n",
        "        stt = int(time.time())\n",
        "        print(f\"Sharpening diffusion result with {SHARPEN_AMOUNT} passes.\\n\")\n",
        "        image = sharpenImage(image, SHARPEN_AMOUNT)\n",
        "        fnt = time_format(int(time.time()) - stt)\n",
        "        print(f'Sharpening completed in {fnt}')\n",
        "        clean_env()\n",
        "\n",
        "    if USE_BASIC_IMAGE_DISPLAY:\n",
        "        display(image)\n",
        "    else:\n",
        "        displayJsImage(i, iteration, f'Stability Diffusion B: {i} I: {iteration}', image)\n",
        "\n",
        "    try:\n",
        "        if depth_image:\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                display(depth_image)\n",
        "            else:\n",
        "                displayJsImage(i, iteration, f'Depth Map B: {i} I: {iteration}', depth_image)\n",
        "            depth_image.close()\n",
        "    except NameError:\n",
        "      pass\n",
        "\n",
        "    if 'ESRGAN' in IMAGE_UPSCALER:\n",
        "        os.chdir(f\"{STABLE_DIFFUSION_WORKDIR}/Real-ESRGAN\")\n",
        "        if not os.path.exists(f'weights/RealESRGAN_x{UPSCALE_AMOUNT}.pth'):\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "\n",
        "    if INSTALL_GFPGAN:\n",
        "        if IMAGE_UPSCALER == \"GFPGAN\":\n",
        "            stt = int(time.time())\n",
        "            clean_env()\n",
        "            print(':sparkle: GFPGAN Face Restoration... ')\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/GFPGAN')\n",
        "            print(subprocess.run(f'python inference_gfpgan.py -i {filedir} -o {OUTDIR} -v 1.3 -s {UPSCALE_AMOUNT} --bg_upsampler realesrgan'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                display(Image.open(f'{OUTDIR}/restored_imgs/{filename}'))\n",
        "            else:\n",
        "                displayJsImage(i, iteration, f'GFPGAN B: {i} I: {iteration}', Image.open(f'{OUTDIR}/restored_imgs/{filename}'))\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "            print(f'Moving enhanced image to {OUTDIR}')\n",
        "            shutil.move(f'{OUTDIR}/restored_imgs/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            fnt = time_format(int(time.time()) - stt)\n",
        "            print(f'GFPGAN Face Restoration completed in {fnt}')\n",
        "            clean_env()\n",
        "\n",
        "    if INSTALL_ESRGAN:\n",
        "        if IMAGE_UPSCALER == \"Enhanced Real-ESRGAN\":\n",
        "            stt = int(time.time())\n",
        "            clean_env()\n",
        "            print(':multiply: Real-ESRGAN Upscaling... ')\n",
        "            print(f'For Real-ESRGAN upscaling only 2, 4, and 8 are supported. Choosing the nearest Value: {nearest_value}')\n",
        "            UPSCALE_AMOUNT = nearest_value\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "            sr_image = upscale(image, UPSCALE_AMOUNT)\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                display(sr_image)\n",
        "            else:\n",
        "                displayJsImage(i, iteration, f'Real-ESRGAN B: {i} I: {iteration}', sr_image)\n",
        "            try:\n",
        "                sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            except NameError:\n",
        "                sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            sr_image.close()\n",
        "            fnt = time_format(int(time.time()) - stt)\n",
        "            print(f'Enhanced Real-ESRGAN completed in {fnt}')\n",
        "            clean_env()\n",
        "\n",
        "    if INSTALL_GFPGAN and INSTALL_ESRGAN:\n",
        "        if IMAGE_UPSCALER == \"GFPGAN + Enhanced ESRGAN\":\n",
        "            stt = int(time.time())\n",
        "            clean_env()\n",
        "            # GFPGAN\n",
        "            print(':sparkle: GFPGAN Face Restoration... ')\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/GFPGAN')\n",
        "            print(subprocess.run(f'python inference_gfpgan.py -i {filedir} -o {OUTDIR} -v 1.3 -s 1 --bg_upsampler realesrgan'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "            shutil.move(f'{OUTDIR}/restored_imgs/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            \n",
        "            # Real-ESRGAN\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/Real-ESRGAN')\n",
        "            enhanced_image = Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            if SCALE_DOWN_ENHANCEMENTS_FOR_ESRGAN:\n",
        "                enhanced_image = enhanced_image.resize((WIDTH,HEIGHT))\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                display(enhanced_image)\n",
        "            else:\n",
        "                displayJsImage(i, iteration, f'GFPGAN B: {i} I: {iteration}', enhanced_image)\n",
        "            print(\":multiply: Real-ESRGAN Upscaling... \")\n",
        "            if UPSCALE_AMOUNT not in [2,4,8]:\n",
        "              UPSCALE_AMOUNT = nearest_value\n",
        "              print(f'For Real-ESRGAN upscaling only 2, 4, and 8 are supported. Choosing the nearest Value: {nearest_value}')\n",
        "            sr_image = upscale(enhanced_image, UPSCALE_AMOUNT)\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                display(sr_image)\n",
        "            else:\n",
        "                displayJsImage(i, iteration, f'Real-ESRGAN B: {i} I: {iteration}', sr_image)\n",
        "            sr_image.save(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            sr_image.close()\n",
        "            enhanced_image.close()\n",
        "            fnt = time_format(int(time.time()) - stt)\n",
        "            print(f'GFPGAN + Real-ESRGAN completed in {fnt}')\n",
        "            clean_env()\n",
        "\n",
        "    if INSTALL_CODEFORMER:\n",
        "        if IMAGE_UPSCALER == \"CodeFormer\":\n",
        "            stt = int(time.time())\n",
        "            clean_env()\n",
        "            print(\":sparkle: CodeFormer Face Restoration... \")\n",
        "            # It was behaving weird, hence why I am doing this the weird way\n",
        "            print(subprocess.run(f'cp {filedir} {STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp/'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer')\n",
        "            print(subprocess.run(f'python inference_codeformer.py --w {CODEFORMER_FIDELITY} --test_path {STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp --upscale {UPSCALE_AMOUNT} --bg_upsampler realesrgan'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            os.remove(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp/{filename}')\n",
        "            shutil.copyfile(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer/results/temp_{float(CODEFORMER_FIDELITY)}/final_results/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}')\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                display(Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'))\n",
        "            else:\n",
        "                displayJsImage(i, iteration, f'CodeFormer B: {i} I: {iteration}', Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png'))\n",
        "            fnt = time_format(int(time.time()) - stt)\n",
        "            print(f'CodeFormer Face Restoration completed in {fnt}')\n",
        "            clean_env()\n",
        "    else:\n",
        "        print(\"CodeFormer is not installed! Please check CodeFormer and run the environment setup again.\")\n",
        "\n",
        "\n",
        "    if INSTALL_CODEFORMER and INSTALL_ESRGAN:\n",
        "        if IMAGE_UPSCALER == \"CodeFormer + Enhanced ESRGAN\":\n",
        "            stt = int(time.time())\n",
        "            clean_env()\n",
        "            # CodeFormer\n",
        "            print(\":sparkle: CodeFormer Face Restoration... \")\n",
        "            print(subprocess.run(f'cp {filedir} {STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp/'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer')\n",
        "            print(subprocess.run(f'python inference_codeformer.py --w {CODEFORMER_FIDELITY} --test_path {STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp --bg_upsampler realesrgan'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            os.remove(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp/{filename}')\n",
        "            shutil.copyfile(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer/results/temp_{float(CODEFORMER_FIDELITY)}/final_results/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            \n",
        "            # Real-ESRGAN\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/Real-ESRGAN')\n",
        "            enhanced_image = Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            if SCALE_DOWN_ENHANCEMENTS_FOR_ESRGAN:\n",
        "                enhanced_image = enhanced_image.resize((WIDTH,HEIGHT))\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                display(enhanced_image)\n",
        "            else:\n",
        "                displayJsImage(i, iteration, f'CodeFormer B: {i} I: {iteration}', enhanced_image)\n",
        "            print(\":multiply: Real-ESRGAN Upscaling... \")\n",
        "            if UPSCALE_AMOUNT not in [2,4,8]:\n",
        "              UPSCALE_AMOUNT = nearest_value\n",
        "              print(f'For Real-ESRGAN upscaling only 2, 4, and 8 are supported. Choosing the nearest Value: {nearest_value}')\n",
        "            sr_image = upscale(enhanced_image, UPSCALE_AMOUNT)\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                display(sr_image)\n",
        "            else:\n",
        "                displayJsImage(i, iteration, f'Real-ESRGAN B: {i} I: {iteration}', sr_image)\n",
        "            sr_image.save(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            sr_image.close()\n",
        "            enhanced_image.close()\n",
        "            fnt = time_format(int(time.time()) - stt)\n",
        "            print(f'CodeFormer + Real-ESRGAN completed in {fnt}')\n",
        "            clean_env()\n",
        "   \n",
        "    image.close()\n",
        "# End Diffuse Function\n",
        "\n",
        "# Setup Prompts\n",
        "if PROMPT.lower() in [None, '', 'none'] and PROMPT_FILE in [None, '', 'none']:\n",
        "    raise Exception(\"PROMPT and PROMPT_FILE are empty! You need to provide a PROMPT or PROMPT_FILE!\")\n",
        "\n",
        "PROMPTS = []\n",
        "if PROMPT_FILE not in ['','none']:\n",
        "    try:\n",
        "        with open(PROMPT_FILE, \"r\") as f:\n",
        "            PROMPTS = f.read().splitlines()\n",
        "    except OSError as e:\n",
        "        raise e\n",
        "\n",
        "def getInitImages(path, verbose=False):\n",
        "    valid = ['.jpeg','.jpg','.gif','.png']\n",
        "    if path.startswith('http://') or path.startswith('https://'):\n",
        "        if verbose: print(f'Found 1 remote image: {path}\\n')\n",
        "        return path\n",
        "    if os.path.isdir(path):\n",
        "        try:\n",
        "            images = next(os.walk(path), (None, None, []))[2]\n",
        "            ret_images = []\n",
        "            if images:\n",
        "                if verbose: print(f\"Found {len(images)} image(s) in {path}\\n\")\n",
        "                for img in images:\n",
        "                    ext = os.path.splitext(img)[1]\n",
        "                    if ext in valid:\n",
        "                        img = f'{path}/{img}'\n",
        "                        if verbose: print(f' -> {img}', defaultprint=True)\n",
        "                        ret_images.append(img)\n",
        "                print('')\n",
        "            if len(ret_images) == 0:\n",
        "                if verbose: print(f'Found no valid image(s)\\n')\n",
        "                return None\n",
        "        except OSError as e:\n",
        "            raise e\n",
        "    elif os.path.isfile(path):\n",
        "        try:\n",
        "            if path.lower().endswith('.txt'):\n",
        "                with open(path, \"r\") as f:\n",
        "                    images = f.read().splitlines()\n",
        "                    if images:\n",
        "                        ret_images = []\n",
        "                        if verbose: print(f\"Found {len(images)} image(s) in {path}\\n\")\n",
        "                        for img in images:\n",
        "                            ext = os.path.splitext(img)[1]\n",
        "                            if ext in valid:\n",
        "                                if verbose: print(f' -> {img}', defaultprint=True)\n",
        "                                ret_images.append(img)\n",
        "                        print('')\n",
        "            else:\n",
        "                ext = os.path.splitext(path)[1]\n",
        "                if ext.lower() in valid:\n",
        "                    if verbose: print(f'Found 1 image: {path}\\n')\n",
        "                    return path\n",
        "                else:\n",
        "                    if verbose: print(f'Found no valid image(s)\\n')\n",
        "                    return None\n",
        "        except OSError as e:\n",
        "            raise e\n",
        "    return ret_images\n",
        "\n",
        "\n",
        "\n",
        "if PROMPT not in ['', 'none']:\n",
        "    PROMPTS.insert(0, PROMPT)\n",
        "\n",
        "#Get corrected sizes\n",
        "WX = (WIDTH//64)*64;\n",
        "HY = (HEIGHT//64)*64;\n",
        "if int(WX) != int(WIDTH) or int(HY) != int(HEIGHT):\n",
        "    print(f'Changing output size to {WX}x{HY}. Dimensions must by multiples of 64.')\n",
        "    WIDTH = WX\n",
        "    HEIGHT = HY\n",
        "\n",
        "# Setup init_iamge\n",
        "inits = None\n",
        "last_pipe_type = None\n",
        "if INIT_IMAGE.lower() not in [None, '', 'none']:\n",
        "    if INIT_IMAGE.lower().startswith('http://') or INIT_IMAGE.lower().startswith('https://'):\n",
        "        inits = INIT_IMAGE\n",
        "    else:\n",
        "        inits = getInitImages(INIT_IMAGE, True)\n",
        "    if inits is not None:\n",
        "        pipe_type = 'img2img'\n",
        "    else:\n",
        "        print(f\":WARNING: No valid image(s) found in {INIT_IMAGE}. Switching to default Text-to-Image run...\")\n",
        "        pipe_type = 'lowvram' if LOW_VRAM_PATCH else 'default'\n",
        "else:\n",
        "    pipe_type = 'lowvram' if LOW_VRAM_PATCH else 'default'\n",
        "\n",
        "# Initiate non-cached pipelines\n",
        "if not CACHE_PIPELINES:\n",
        "    print(\"Setting up diffusion model pipeline...\")\n",
        "    try:\n",
        "        if pipe:\n",
        "            print(\"Pipeline already in memory. Starting diffusion environment...\\n\")\n",
        "    except:\n",
        "        pipe = setup_pipes(pipe_type)\n",
        "        pass\n",
        "    \n",
        "    if pipe_type is not last_pipe_type:\n",
        "        pipe = setup_pipes(pipe_type)\n",
        "\n",
        "last_pipe_type = pipe_type\n",
        "\n",
        "with torch.no_grad():\n",
        "    with precision_scope(\"cuda\"):\n",
        "\n",
        "        # Hack in Image List Support\n",
        "        DO = None\n",
        "        if type(inits) is list:\n",
        "            ITERATE_THIS = inits\n",
        "            DO = 'inits'\n",
        "        else:\n",
        "            ITERATE_THIS = PROMPTS\n",
        "            DO = 'prompts'\n",
        "\n",
        "        i = 0\n",
        "        for pi in ITERATE_THIS: # Replace PROMPTS with ITERATE_THIS switch\n",
        "\n",
        "            if DO is 'inits':\n",
        "                init = pi\n",
        "                if i > len(PROMPTS)-1:\n",
        "                    pi = PROMPTS[-1]\n",
        "                else:\n",
        "                    pi = PROMPTS[i]\n",
        "            elif DO is 'prompts':\n",
        "                if inits is not None:\n",
        "                    init = inits\n",
        "            if init:    \n",
        "                from PIL import ImageOps\n",
        "                init = Image.open(fetch(init)).convert(\"RGB\")\n",
        "                init = ImageOps.exif_transpose(init)\n",
        "                init = init.resize((WIDTH,HEIGHT))\n",
        "                original_init = init\n",
        "                init = preprocess(init)\n",
        "\n",
        "            # Define Run Prompt\n",
        "            if NEW_NSP_ON_ITERATION is not True:\n",
        "                PROMPT = nsp_parse(pi)\n",
        "                epoch_time = int(time.time())\n",
        "                if SAVE_PROMPT_DETAILS:\n",
        "                    with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:\n",
        "                            file.write(f'{PROMPT}\\n\\nHeight: {HEIGHT}\\nWidth: {WIDTH}\\nSeed: {SEED}\\nScale: {SCALE}\\nPrecision: {PRECISION}\\n')\n",
        "\n",
        "            for iteration in range(NUM_ITERS):\n",
        "\n",
        "                # Define Iteration Prompt\n",
        "                if NEW_NSP_ON_ITERATION:\n",
        "                    PROMPT = nsp_parse(pi)\n",
        "                    epoch_time = int(time.time())\n",
        "                    if SAVE_PROMPT_DETAILS:\n",
        "                        with open(f'{OUTDIR}/{epoch_time}_prompt.txt', 'w') as file:\n",
        "                                file.write(f'{PROMPT}\\n\\nHeight: {HEIGHT}\\nWidth: {WIDTH}\\nSeed: {SEED}\\nScale: {SCALE}\\nPrecision: {PRECISION}\\n')\n",
        "\n",
        "                try:\n",
        "\n",
        "                    diffuse_run()\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    if 'out of memory' in str(e):\n",
        "                        print(f\"\\u001b[31m\\u001b[1m\\u001b[4mCRITICAL ERROR\\u001b[0m: {gpu_name} ran out of memory! If this error persists, the GPU may have crashed, and requires a disconnect/re-run.\")\n",
        "                        pass\n",
        "                    else:\n",
        "                        raise e\n",
        "                except KeyboardInterrupt as e:\n",
        "                    raise SystemExit('\\33[33mExecution interrupted by user.\\33[0m')\n",
        "                except Exception as e:\n",
        "                    raise e\n",
        "                finally:\n",
        "                    print(\"Cleaning up...\\n\")\n",
        "                    clean_env(True)\n",
        "                    \n",
        "            \n",
        "            i+=1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <font color=\"orange\">**Clear Memory**</font>\n",
        "#@markdown Attempt to manually clean up the environment if you're having strange memory troubles.<br>\n",
        "#@markdown **Note:** Sometimes the GPU just crashes and you should disconnect and restart. Additionally, your GPU could have been unallocated due to runtime constraints.\n",
        "clean_env(True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RjZhn2vwEydT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}