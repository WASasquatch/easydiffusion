{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WASasquatch/easydiffusion/blob/main/Stability_AI_Easy_Diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y6RXjS1tTji"
      },
      "source": [
        "# Stability.AI Easy Diffusion v1.3 ![visitors](https://visitor-badge.glitch.me/badge?page_id=EasyDiffusion&left_color=blue&right_color=orange) [![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/WASasquatch/easydiffusion)\n",
        "\n",
        "\n",
        "Easy Diffusion was originally a fork of NOP's notebook, but has sort of evolved into it's own thing with many features. Such as depth output for 3D Facebook images, or post processing such as Depth of Field.\n",
        "\n",
        "If you'd like to help support the project and my time, *as well as the compute points required :(*, feel free to buy me some bandwidth (I live rural and pay for bandwidth): https://paypal.me/ThompsonJordan\n",
        "\n",
        "### ðŸ‘· [Development branch is located here](https://colab.research.google.com/github/WASasquatch/easydiffusion/blob/dev/Stability_AI_Easy_Diffusion.ipynb) <-- Use this version if encountering urllib3 error | or run `!pip install --upgrade urllib3` and restart colab\n",
        "\n",
        "<br>\n",
        "<hr>\n",
        "\n",
        "## <font color=\"#e8cf53\">**Menu**</font>\n",
        "- <a href=\"#changelog\">**Change Log**</a>\n",
        "- <a href=\"#gpustatus\">**Check GPU Status**</a>\n",
        "- #### <a href=\"#setupenv\">**Setup Environment**</a>\n",
        " - <a href=\"#googledrive\">**Google Drive Options**</a>\n",
        " - <a href=\"#optionalfeats\">**Install Optional Features**</a>\n",
        " - <a href=\"#otherinstall\">**Other Install Options**</a>\n",
        "- #### <a href=\"#settingsdiffuse\">**Settings & Diffuse**</a>\n",
        " - <a href=\"exportimport\">**Export / Import Settings**</a>\n",
        " - <a href=\"#promptsetup\">**Prompt Setup**</a>\n",
        " - <a href=\"#initsetup\">**Init Image Setup**</a>\n",
        "   - <a href=\"#recursiveevo\">**Recursive Evolution**</a>\n",
        " - <a href=\"#diffusionsettings\">**Diffusion Settings**</a>\n",
        "   - <a href=\"#diffusionmodel\">**Diffusion Model**</a>\n",
        "   - <a href=\"#conceptembed\">**Concept Embedding**</a>\n",
        " - <a href=\"#upscalers\">**Upscaling Setup**</a>\n",
        "   - <a href=\"#upscalers-codeformer\">**CodeFormer Upscale Settings**</a>\n",
        "   - <a href=\"#upscalers-gobig\">**GOBIG Upscale Settings**</a>\n",
        "   - <a href=\"#upscalers-hybrid\">**Hybrid Upscale Settings**</a>\n",
        " - <a href=\"#imageprocessors\">**Image Processing Setup**</a>\n",
        "   - <a href=\"#sharpen\">**Sharpen Image**</a>\n",
        "   - <a href=\"#kromo\">**Kromo Chromatic Aberration**</a>\n",
        "   - <a href=\"#median\">**Median Filter Image**</a>\n",
        "   - <a href=\"#midas\">**MiDaS Depth Export**</a>\n",
        "   - <a href=\"#fdof\">**Fake Depth of Field Filter**</a>\n",
        "   - <a href=\"#tileable\">**Tileable Seamless Image**</a>\n",
        " - <a href=\"#clipinterrogator\">**CLIP Interrogator**</a>\n",
        " - <a href=\"#cachepipes\">**Pipeline Flags**</a>\n",
        " - <a href=\"#othersettings\">**Console Output settings**</a>\n",
        "- <a href=\"#cleanenv\">**Clean Environment Up**</a>\n",
        "\n",
        "<br>\n",
        "<hr>\n",
        "\n",
        "## Stablity.AI Model Terms of Use\n",
        "\n",
        "**By using this Notebook, you agree to the following Terms of Use, and license**\n",
        "\n",
        "This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\n",
        "\n",
        "The CreativeML OpenRAIL License specifies:\n",
        "1. You can't use the model to deliberately produce nor share illegal or harmful outputs or content\n",
        "2. CompVis claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n",
        "3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n",
        "\n",
        "Please read the full license here: https://huggingface.co/spaces/CompVis/stable-diffusion-license \n",
        "\n",
        "## Expand for Changelog"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G76cGaiuGdjJ"
      },
      "source": [
        "\n",
        "\n",
        "## <a name=\"changelog\"><font color=\"#e8cf53\">Change Log</font></a>:\n",
        "- v0.1: Forked [NOP's Stable Diffusion Colab v0.23](https://colab.research.google.com/drive/1jUwJ0owjigpG-9m6AI_wEStwimisUE17?usp=sharing)\n",
        "  - Added File Prompts\n",
        "  - Added Noodle Soup Prompts\n",
        "- 8/25/2022) Added better image output display\n",
        "- 8/26/2022) Added `INIT_IMAGE` support\n",
        "  - Added basic image output option\n",
        "- 8/27/2022) Patched CodeFormer fidelity path bug\n",
        "- 8/27/2022) Various code tweaks (by plambe#5832)\n",
        "  - Download some of the dependencies to google drive if enabled \n",
        "    - For instance the stable diffusion model\n",
        "    - Also multiple of the git repos\n",
        "  - Replaced all `!` and `%` in code to make it more universal\n",
        "- 8/27/2022) Patch NSP Installation, changed paths for Stable Diffusion and output images. (by WAS#0263)\n",
        "- 8/27/2022) Organized and improved installations\n",
        "- 8/28/2022) Real-ESRGAN bug fix (by plambe#5832)\n",
        "- 8/28/2022) GFPGAN bug fix (by plambe#5832)\n",
        "- 8/28/2022) CodeFormer bug fix (by plambe#5832)\n",
        "- 8/28/2022) Added cached diffusion piping: This will speed up run performance (WAS#0263)\n",
        "  - Added `RECACHE_PIPES` option\n",
        "  - Added `INCREMENT_ITERATION_SEED` option\n",
        "  - Patched working directory path for non-gdrive installations\n",
        "  - Patched working directory path for pipe cache not found\n",
        "  - Patched CodeFormer Fidelity paths, again?\n",
        "  - Added pre-ESRGAN down scaling option for GFPGAN + Real-ESRGAN, and CodeFormer + Real-ESRGAN.\n",
        "  - Added post diffusion sharpen option\n",
        "- **V0.6** | 8/29/2022) Added optional cached pipes. Using cached pipes is best for a high VRAM environment\n",
        "  - Added Kromo's Chromatic Aberration\n",
        "  - Added Sharpening\n",
        "  - Added optional dependency installs (save some space!)\n",
        "- **v0.7** | 8/29/2022) Added MiDaS Depth Approximation\n",
        "  - Depth maps can be used to apply Depth of Field, or other filmic effects in post processing with your favorite tools.\n",
        "- **v0.8** | 8/30/2022) Added Sampling Schedulers\n",
        "  -  Track function timing\n",
        "  - 8/31/2022) Patch MiDaS Depth Export even when unchecked.\n",
        "- **v0.9** | 9/1/2022) Added multi-init functionality to `INIT_IMAGE`.\n",
        "  - `INIT_IMAGE` supports a local/remote image path/url, a txt file containing a path/url per line, or a path to a folder containing images.\n",
        "  - Add `ESRGAN_MODE` which allows you to run ESRGAN on CPU if you want to conserve more VRAM.\n",
        "  - Add `MIDAS_PERSISTENT` mode. Keep MiDaS models in memory between iterations.\n",
        "  - 9/2/2022) Patch Pillow<9.0.0 versions for Resampling calls.\n",
        "  - Prepare for model selection\n",
        "- **v0.10** | 9/3/2022) Added collapsible batches and iterations in organized JS image output mode (default behavior)\n",
        "  - Improved environment cleanup (again)\n",
        "  - Added ability to clear log between iterations. *This would clear the diffusion result before viewing it in standard mode, so is disabled for non JS image output*\n",
        "- 9/4/2022) Add ability to skip diffusion run. Useful for processing prior diffusions or inits with image filters and upscalers. \n",
        "- **v0.11** | 9/5/2022) Patched img2img pipeline\n",
        " - Added Median Filter\n",
        " - Added Fake Depth of Field\n",
        " - Added Tileable Seamless Texture outout (also supports seamless depth map if Export Depth Map is enabled)\n",
        " - Various code improvements.\n",
        "- **v0.12** | 9/6/2022) Overhaul of Easy Diffusion Setup Process\n",
        " -  Easily enable/disable NSFW checker per run, and now returns a blurred image instead of black image. \n",
        " - CodeFormer now has a seprate upscale param, which unfortuantely couldn't be a slider. But CodeFormer now supports 1x (no upscaling).\n",
        " - Patch image display for FDOF active mode. \n",
        "- **v0.13** | 9/7/2022) Added Attention Slices optimization. This feature splits the job into slices for better use of available VRAM. Use `ENABLE_ATTENTION_SLICES` in diffusion settings to try it out.\n",
        " - `LOW_VRAM_PATCH` can be toggled from diffusion settings for **non-cached** pipes. \n",
        " - Add ability to set `MAX_SEED` size. Either a custom integether or `'system_max'` (with semi-quotes).\n",
        " - Add text2seed ability. Seed is now a raw input, and you can input text like `'rockycanyon'` (with semi-quotes) which will be converted to textual equivalent. \n",
        "- **v0.14** | 9/8/2022) Added ability to save model to Google Drive if not using drive for local copies.\n",
        " - Add `CUSTOM_MODEL` path field. This will override the selected `MODEL_ID` if exists.\n",
        " - Pipelines have been overhauled and should be much lighter on memory (should be the same as a regular loaded pipe), and offer much faster load times.\n",
        "- **v0.15** | 9/10/2022) Added Menu's for quicker initial navigation. \n",
        " - Added Google Colab paramScraper for settings export. \n",
        "- **v0.16** | 9/11/2022) Added pharmapsychotic's CLIP Interrogator\n",
        " - Interrogate init images or diffusion result\n",
        " - Model options are now available under `MODEL_ID`\n",
        "- **v0.17** | 9/14/2022) Added `INIT_IMAGE` support\n",
        " - Added `RECURSIVE_EVOLUTION` support. Allows you to take a txt2img result straight to img2img for further evolution. Doesn't work well with low `INIT_SCALE` values. \n",
        "<br>\n",
        "- **v0.18** | 9/14/2022) Rebuilt `cache_pipes()` function. It now will only cache the type of pipe active, and return said pipe. This will save storage space by only caching the pipes you actually use. \n",
        " - Minor patching to non-cached pipes for recursive evolution. \n",
        "- 9/15/2022 ) Added `KEEP_ONLY_FINAL_IMAGE` flag under <a href=\"#imageprocessors\">Image Processing</a> to keep only the final upscaled or processed image.\n",
        " - Various small code patches.\n",
        " - Patch `displayJsImage()` function to support Chrome/Edge\n",
        "- **v0.19** | 9/20/2022) Added Negative Prompts and preset Styles.\n",
        " - Negative prompts can be used denoting a string at the *end* of your prompt with `--`. Example: `Positive prompt here --Negative prompt here`\n",
        " - Add `INIT_FILTER` to filter init images from init files or folders of images for specific keywords.\n",
        " - 9/21/2022) More styles added\n",
        "- **v0.20** | 9/24/2022) Added Hybrid Upscaling. Hybrid Upscaling overlays the original diffusion result over the AI Upscale result. Useful for mitigating the flat surfaces and smoothnes of Real-ESRGAN and other AI upscalers.\n",
        " - Added ability to load Easy Diffusion Settings files.  \n",
        "- **v0.21** | 9/25/2022) Added IMG2IMG Upscaling. Inherently since it's using diffusion to upscale, it requires a lot of VRAM. A T4 can do a 512x512 upscale 2x.\n",
        " - Rewrote upscaler setups. They will now only install upon use. \n",
        "- **v1.00** | 9/28/2022) Added **GOBIG** IMG2IMG Upscaling originally by lowfuel. Slice a image up and diffuse each slice, merging them back to form a final upscaled image. This can can create very large scale stylish diffusions. I will try to get a GOBIG guide together. \n",
        " - Added `USE_INIT_IMAGE_SIZE` to use init image size in place of `WIDTH` and `HEIGHT` params.\n",
        " - Optional prompt words. You can specify a word option like follows: `<dog|cat|bird>`, or for `<^^#` to specify a number of random words. Ex: `<^^4|dog|cat|bird>`\n",
        " - Parsing of `INIT_IMAGE` files and paths is now sorted.\n",
        " - Check if CLIP Interrogator is installed before attmepting use. \n",
        " - Various code improvments and patches.\n",
        " - Roll-back Diffusers to stable version.\n",
        "- **v1.1** | 10/4/3033) Disabled pipeline patching for negative prompts as it has been merged with diffusers.\n",
        "- **v1.2** | 10/5/2022) Changed optional prompt words to use `[` and `]` instead of `<` and `>`\n",
        " - Added [Stable Diffusion Concept](https://huggingface.co/sd-concepts-library) support\n",
        " - Added support for prompt line repeating. Specify how many times a prompt should be used in successive batches with `^#|The prompt` for example `^5|A cat sitting on a bench` would repeat `A cat sitting on a bench` for 5 batches.\n",
        " - `RECURSIVE_EVOLUTION` supports `INIT_IMAGE` and init or prompt batches.\n",
        " - `RECURSIVE_EVOLUTION` supports hybrid frames. Example; you're on frame 4, frame 2 will be overlayed on frame 3, which is fed as the `INIT_IMAGE` for frame 4.\n",
        "- 10/14/2022) Pipes have been updated for new image retrieval method.\n",
        " - Added adjustable thumbnail sizes for JS image mode.\n",
        "- 10/21/2022) Add Stable Diffusion v1.5 and 1.5 Inpainting\n",
        "- **v1.3** | 11/05/2022) Added LPW Pipeline as main ED pipeline. Pipe supports long prompts, and prompt weights. \n",
        " - Added **Euler_A** and **Euler** Samplers\n",
        "- 11/11/2022) Added DPM Solver++ Sampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NOEF-K5F5db"
      },
      "source": [
        "# Easy Diffusion Setup (Run to Install - Expand to Customize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ekR-LW6trWG",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title <a name=\"gpustatus\"><font color=\"#e8cf53\">Check GPU Status</font></a>\n",
        "#@markdown Check the status of the allocated GPU\n",
        "import subprocess\n",
        "print(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "nvidiasmi_simple = subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "gpu_name = nvidiasmi_simple.split(':')[1].split('(')[0].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8gV4-qRDn1b",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title <a name=\"setupenv\"><font size=\"5\" color=\"#e8cf53\">**Setup Environment**</font></a>\n",
        "\n",
        "# Import future print\n",
        "from __future__ import print_function\n",
        "try:\n",
        "    import __builtin__\n",
        "except ImportError:\n",
        "    import builtins as __builtin__\n",
        "\n",
        "# Emoticon fun!\n",
        "import subprocess\n",
        "try:\n",
        "    import emoji\n",
        "except ImportError:\n",
        "     multipip_res = subprocess.run(['pip', '-q', 'install', 'emoji'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "finally:\n",
        "    import emoji\n",
        "\n",
        "print(subprocess.run('python -m ensurepip --upgrade'.split(' '), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "\n",
        "# Override Print Function\n",
        "def print(message, *args, **kwargs):\n",
        "    if 'defaultprint' in kwargs:\n",
        "        kwargs.pop('defaultprint')\n",
        "        return __builtin__.print(message, *args, **kwargs)\n",
        "    else:\n",
        "        return __builtin__.print(emoji.emojize(message), *args, **kwargs)\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### <a name=\"googledrive\"><font color=\"#e8cf53\">**Google Drive Options**</font></a>\n",
        "USE_DRIVE_FOR_PICS = True #@param {type:\"boolean\"}\n",
        "#@markdown <font size=\"3\">Use Google Drive to store images and prompt information</font>\n",
        "USE_DRIVE_FOR_LOCAL_COPIES = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown <font size=\"3\">Use Google Drive to store local copies of git repos, models and other assets</font><br>\n",
        "#@markdown <font size=\"3\" color=\"orange\">**WARNING:**</font> Requires 14gb+ of space (not including images produced). May not be suitable for Free Google Drive accounts.</font><br>\n",
        "#@markdown <font size=\"3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If you encounter issues loading pipes, or Upscalers, you're likely out of storage space.</font>\n",
        "USE_DRIVE_FOR_MODELS = False #@param{type:'boolean'}\n",
        "#@markdown Use Google Drive for storing Stable Diffusion models. Only applicable if `USE_DRIVE_FOR_LOCAL_COPIES` is `False`.\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### <a name=\"optionalfeats\"><font color=\"#e8cf53\">**Install Optional Features**</font></a>\n",
        "INSTALL_GOBIG_V2_SUPPORT = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Install GOBIG v2 diffusion based upscaler by lowfuel</font><br>\n",
        "#@markdown <font size=\"3\"><font size=\"3\" color=\"orange\">**WARNING:**</font> Install Pillow 9.0 which is required to use GOBIG v2. This requires a Runtime Restart. Then you must run this cell again. You can use Ctrl + F9 to run all.\n",
        "INSTALL_CLIP_INTERROGATOR = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Install CLIP Interrogator by pharmapsychotic</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### <a name=\"otherinstall\"><font color=\"#e8cf53\">**Other Install Options**</font></a>\n",
        "CLEAR_SETUP_LOG = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Clear the setup log after installation completes.</font>\n",
        "SUPPRESS_WARNINGS = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Supress warnings from installation scripts and runtime scripts.</font>\n",
        "RESTART_COLAB_AFTER_GOBIG_INSTALL = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Automatically restart the colab after installing Pillow 9.x.</font>\n",
        "\n",
        "import os, sys, time, torch, gc, requests, io, shutil, json\n",
        "\n",
        "TOKEN = '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'\n",
        "settings_template = {\n",
        "    'setup': {\n",
        "        'USE_DRIVE_FOR_PICS': False,\n",
        "        'USE_DRIVE_FOR_LOCAL_COPIES': False,\n",
        "        'USE_DRIVE_FOR_MODELS': False,\n",
        "        'INSTALL_CLIP_INTERROGATOR': False,\n",
        "        'CLEAR_SETUP_LOG': False,\n",
        "        'SUPPRESS_WARNINGS': True,\n",
        "    },\n",
        "    'prompts': {\n",
        "        'PROMPT': None,\n",
        "        'PROMPT_FILE': None,\n",
        "        'PROMPT_STYLE': None,\n",
        "        'NEW_NSP_ON_ITERATION': True,\n",
        "    },\n",
        "    'inits': {\n",
        "        'INIT_IMAGE': None,\n",
        "        'INIT_MASK': None,\n",
        "        'INIT_SCALE': None,\n",
        "        'RECURSIVE_EVOLUTION': False,\n",
        "    },\n",
        "    'diffusion_settings': {\n",
        "        'MODEL_ID': None,\n",
        "        'SAMPLER': None,\n",
        "        'DDIM_ETA': None,\n",
        "        'STEPS': None,\n",
        "        'SEED': None,\n",
        "        'MAX_SEED': None,\n",
        "        'INCREMENT_ITERATION_SEED': None,\n",
        "        'NUM_ITERS': None,\n",
        "        'WIDTH': None,\n",
        "        'HEIGHT': None,\n",
        "        'SCALE': None,\n",
        "        'PRECISION': None,\n",
        "        'IMAGES_FOLDER': None,\n",
        "        'CACHE_PIPELINES': False,\n",
        "        'RECACHE_PIPES': False,\n",
        "        'SKIP_DIFFUSION_RUN': False,\n",
        "        'ENABLE_NSFW_FILTER': False,\n",
        "        'ENABLE_ATTENTION_SLICES': True,\n",
        "        'LOW_VRAM_PATCH': True,\n",
        "    },\n",
        "    'upscalers': {\n",
        "        'IMAGE_UPSCALER': None,\n",
        "        'UPSCALE_AMOUNT': None,\n",
        "        'ESRGAN_MODE': None,\n",
        "        'CODEFORMER_UPSCALE_AMOUNT': None,\n",
        "        'CODEFORMER_FIDELITY': None,\n",
        "    },\n",
        "    'image_processing': {\n",
        "        'KEEP_ONLY_FINAL_IMAGE': False,\n",
        "        'SCALE_DOWN_ENHANCEMENTS_FOR_ESRGAN': True,\n",
        "        'SHARPEN_AMOUNT': None,\n",
        "        'CA_DIFFUSE_IMAGE': False,\n",
        "        'CA_STRENGTH': None,\n",
        "        'CA_JITTER': None,\n",
        "        'CA_OVERLAY': None,\n",
        "        'CA_NO_RADIAL_BLUR': None,\n",
        "        'MEDIAN_FILTER_IMAGE': False,\n",
        "        'MEDIAN_DIAMETER': None,\n",
        "        'MEDIAN_SIGMA_COLOR': None,\n",
        "        'MEDIAN_SIGMA_SPACE': None,\n",
        "        'GENERATE_MIDAS_DEPTH': False,\n",
        "        'SAVE_MIDAS_DEPTH': False,\n",
        "        'MIDAS_TYPE': None,\n",
        "        'MIDAS_MODE': None,\n",
        "        'FDOF_IMAGE': False,\n",
        "        'FDOF_REPLACE_IMAGE': False,\n",
        "        'FDOF_RADIUS': None,\n",
        "        'FDOF_SAMPLES': None,\n",
        "        'TILEABLE_IMAGE': False,\n",
        "        'TILED': True,\n",
        "        'TILE_OVERLAP': None,\n",
        "    },\n",
        "    'clip_interrogator': {\n",
        "        'INTERROGATE_INIT_IAMGE': False,\n",
        "        'INTERROGATE_DIFFUSION_IMAGE': False,\n",
        "        'ViTB32': False,\n",
        "        'ViTB16;': False,\n",
        "        'ViTL14': True,\n",
        "        'ViTL14_336px': True,\n",
        "        'RN101': False,\n",
        "        'RN50': False,\n",
        "        'RN50x4': False,\n",
        "        'RN50x16': False,\n",
        "        'RN50x64': False,\n",
        "        'INTERROGATOR_PROMPT': None,\n",
        "    },\n",
        "    'other_settings': {\n",
        "        'IMAGES_DISPLAY_ABOVE_LOG': False,\n",
        "        'USE_BASIC_IMAGE_DISPLAY': True,\n",
        "        'CLEAR_LOG_BETWEEN_ITERATIONS': True,\n",
        "    },\n",
        "}\n",
        "\n",
        "# Enable third-party widgets\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "# SETUP BASE DIRECTORIES\n",
        "OUTDIR = '/content/Stable_Diffusion/images_out'\n",
        "\n",
        "if USE_DRIVE_FOR_MODELS and USE_DRIVE_FOR_LOCAL_COPIES:\n",
        "    USE_DRIVE_FOR_MODELS = False\n",
        "\n",
        "if USE_DRIVE_FOR_PICS and not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "STABLE_DIFFUSION_WORKDIR = '/content/Stable_Diffusion'\n",
        "GDRIVE_WORKDIR = '/content/drive/MyDrive/AI/Stable_Diffusion'\n",
        "\n",
        "if USE_DRIVE_FOR_LOCAL_COPIES:\n",
        "    STABLE_DIFFUSION_WORKDIR = GDRIVE_WORKDIR\n",
        "    if not os.path.exists(STABLE_DIFFUSION_WORKDIR):\n",
        "        os.makedirs(STABLE_DIFFUSION_WORKDIR)\n",
        "if not USE_DRIVE_FOR_LOCAL_COPIES:\n",
        "    if not os.path.exists(STABLE_DIFFUSION_WORKDIR):\n",
        "        os.makedirs(STABLE_DIFFUSION_WORKDIR)\n",
        "\n",
        "os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "sys.path.append(STABLE_DIFFUSION_WORKDIR)\n",
        "\n",
        "EMBEDDINGS_PATH = f\"{STABLE_DIFFUSION_WORKDIR}/downloaded_embedding\"\n",
        "DRIVE_MODEL_CACHE = f'{GDRIVE_WORKDIR}/model_cache'\n",
        "model_cache = f'{STABLE_DIFFUSION_WORKDIR}/model_cache'\n",
        "PIPE_CACHE = f'{STABLE_DIFFUSION_WORKDIR}/cache'\n",
        "\n",
        "MOVED_FROM_CACHE = False\n",
        "last_model = None\n",
        "\n",
        "if not os.path.exists(model_cache):\n",
        "    os.makedirs(model_cache)\n",
        "\n",
        "if not os.path.exists(PIPE_CACHE):\n",
        "    os.makedirs(PIPE_CACHE)\n",
        "\n",
        "if not os.path.exists(EMBEDDINGS_PATH):\n",
        "    os.makedirs(EMBEDDINGS_PATH)\n",
        "\n",
        "# DEFINE NECESSARY FUNCTIONS\n",
        "\n",
        "def packages():\n",
        "    import sys, subprocess\n",
        "    return [r.decode().split('==')[0] for r in subprocess.check_output([sys.executable, '-m', 'pip', 'freeze']).split()]\n",
        "\n",
        "def wget(url, outputdir):\n",
        "    res = None\n",
        "    try:\n",
        "        res = subprocess.run(['wget', '-q', '--show-progress', url, '-P', f'{outputdir}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    except OSError as e:\n",
        "        raise e\n",
        "    finally:\n",
        "        if res and res.strip() is not '':\n",
        "            print(res)\n",
        "\n",
        "def wgeto(url, outputdir):\n",
        "    res = None\n",
        "    try:\n",
        "        res = subprocess.run(['wget', '-q', '--show-progress', url, '-O', f'{outputdir}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    except OSError as e:\n",
        "        raise e\n",
        "    finally:\n",
        "        if res and res.strip() is not '':\n",
        "            print(res)\n",
        "\n",
        "def plotSettings(settingsType=None, locals=None):\n",
        "    global settings\n",
        "    if settingsType and settings.__contains__(settingsType) and type(locals) is dict:\n",
        "        for k in settings[settingsType].keys():\n",
        "            if locals.keys().__contains__(k):\n",
        "                settings[settingsType][k] = locals[k]\n",
        "\n",
        "def fetch_bytes(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        from urllib.request import urlopen \n",
        "        return urlopen(url_or_path) \n",
        "    return open(url_or_path, 'r')\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "def clear():\n",
        "    from IPython.display import clear_output; return clear_output()\n",
        "\n",
        "def time_format(seconds: int):\n",
        "    if seconds is not None:\n",
        "        seconds = int(seconds)\n",
        "        d = seconds // (3600 * 24)\n",
        "        h = seconds // 3600 % 24\n",
        "        m = seconds % 3600 // 60\n",
        "        s = seconds % 3600 % 60\n",
        "        ms = round(seconds * 1000)\n",
        "        if d > 0:\n",
        "            return '{:02d}D {:02d}H {:02d}m {:02d}s'.format(d, h, m, s)\n",
        "        elif h > 0:\n",
        "            return '{:02d}H {:02d}m {:02d}s'.format(h, m, s)\n",
        "        elif m > 0:\n",
        "            return '{:02d}m {:02d}s'.format(m, s)\n",
        "        elif s > 0:\n",
        "            return '{:02d}s'.format(s)\n",
        "        elif ms > 0:\n",
        "            return '{:02d}ms'.format(ms)\n",
        "    return '0s'\n",
        "\n",
        "def text2seed(string, max):\n",
        "    seed = None\n",
        "    def digits(n, max):\n",
        "        import math\n",
        "        ndigits = int(math.log10(n))+1\n",
        "        try:\n",
        "            return n//int(10**(ndigits-max))\n",
        "        except ZeroDivisionError:\n",
        "            return n\n",
        "    if string:\n",
        "        for chr in [*string]:\n",
        "            if seed is None:\n",
        "                seed = str(ord(chr))\n",
        "            else:\n",
        "                seed += str(ord(chr))\n",
        "        seed = digits(int(seed), max)\n",
        "    return seed\n",
        "\n",
        "def gpu_memory_usage(gpu_id):\n",
        "    command = f\"nvidia-smi --id={gpu_id} --query-gpu=memory.used --format=csv\"\n",
        "    output_cmd = subprocess.check_output(command.split())\n",
        "    memory_used = output_cmd.decode(\"ascii\").split(\"\\n\")[1]\n",
        "    memory_used = int(memory_used.split()[0])\n",
        "    return memory_used\n",
        "\n",
        "def gpu_memory_total(gpu_id):\n",
        "    command = f\"nvidia-smi --id={gpu_id} --query-gpu=memory.total --format=csv\"\n",
        "    output_cmd = subprocess.check_output(command.split())\n",
        "    memory_used = output_cmd.decode(\"ascii\").split(\"\\n\")[1]\n",
        "    memory_used = int(memory_used.split()[0])\n",
        "    return memory_used\n",
        "\n",
        "def clean_env(v=False, device=0):\n",
        "    import time\n",
        "    cuda_availabe = torch.cuda.is_available()\n",
        "    mem_used = gpu_memory_usage(device)\n",
        "    mem_total = gpu_memory_total(device)\n",
        "    if v: print(f'VRAM Total: {mem_total}mb, VRAM Allocatd: {mem_used}mb')\n",
        "    stt = int(time.time())\n",
        "    if cuda_availabe:\n",
        "        a = None\n",
        "        try:\n",
        "            a = torch.zeros(sys.maxsize, dtype=torch.int8).cuda()\n",
        "        except Exception:\n",
        "            pass\n",
        "        finally:\n",
        "            del a\n",
        "            torch.cuda.synchronize(); \n",
        "            torch.cuda.empty_cache(); \n",
        "    time.sleep(0.25)\n",
        "    gc.collect()\n",
        "    try:\n",
        "        global midas, transform, prediction, input_batch, depth, depth_image, image, sr_image, enhanced_image, img, init,  original_init\n",
        "        del midas, transform, prediction, input_batch, depth, depth_image, image, sr_image, enhanced_image, img, init,  original_init\n",
        "    except NameError:\n",
        "        pass\n",
        "    time.sleep(1)\n",
        "    if v: print(f':recycling_symbol: Cleared memory.  Time taken was {time_format(int(int(time.time()) - stt))}')\n",
        "    new_mem_used = gpu_memory_usage(device)\n",
        "    if v: print(f'VRAM Allocatd: {new_mem_used}mb, VRAM Released: {mem_used - new_mem_used}mb')\n",
        "    if not cuda_availabe:\n",
        "        print(\":WARNING: There is no CUDA device available! Cannot run diffusion models!\")\n",
        "\n",
        "# Basic image display -- God, what is this monster I've spawned? \n",
        "def displayJsImage(b, i, max_width, prepend, name, img):\n",
        "    import cv2\n",
        "    from IPython.display import display, Javascript\n",
        "    from google.colab.output import eval_js\n",
        "    from base64 import b64encode\n",
        "    from google.colab import files\n",
        "    import numpy as np\n",
        "    img = np.asarray(img, dtype=np.uint8)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    js = Javascript('''\n",
        "        async function showImage(b, i, max_width, prepend, name, image, width, height) {\n",
        "            batchBlock = document.getElementById('batch-block-'+b);\n",
        "            block = document.getElementById('block-'+b+'-'+i)\n",
        "            img = document.getElementById(name);\n",
        "            cont = document.getElementById(name+'_container');\n",
        "\n",
        "            if (batchBlock == null) {\n",
        "                batchBlock = document.createElement('div');\n",
        "                batchBlock.id = 'batch-block-'+b;\n",
        "                batchBlock.style = 'background-color:rgba(0,0,0,0.25);width:auto;margin-bottom:25px;padding:5px;text-align:center;box-shadow: 0px 0px 5px rgba(0,0,0,0.5);';\n",
        "                //batchBlock.innerHTML = '<h2 style=\"background-color:rgba(255,255,255,0.1);margin:0;margin-bottom:5px;padding:4px;text-align:center;text-shadow: 1px 1px rgba(0,0,0,0.35);\">Batch '+b+'</h2>';\n",
        "                if (prepend == 1) {\n",
        "                    document.body.prepend(batchBlock)\n",
        "                } else {\n",
        "                    document.body.appendChild(batchBlock)\n",
        "                }\n",
        "                buttonBt = document.createElement('button');\n",
        "                buttonBt.className = 'collapsible';\n",
        "                buttonBt.style = 'cursor:pointer;width:100%;margin-bottom:5px;border:none;border-bottom:3px solid #999999;padding:5px;text-align:center;font-size:16px;font-weight:bold;color:white;background-color:rgba(155,155,155,0.15);text-shadow: 1px 1px rgba(0,0,0,0.35);transition: all 0.5s;'\n",
        "                buttonBt.innerHTML = 'Batch '+b;\n",
        "                buttonBt.value = 'Batch '+b;\n",
        "                batchBlock.before(buttonBt)\n",
        "            }\n",
        "            if (block == null) {\n",
        "                block = document.createElement('div');\n",
        "                block.id = 'block-'+b+'-'+i;\n",
        "                block.style = 'width: auto;margin-bottom:15px;padding:5px;text-align:center;';\n",
        "                //block.innerHTML = '<h3 style=\"margin:3px;text-align:center;text-shadow: 1px 1px rgba(0,0,0,0.35);\">Iteration '+i+'</h3>';\n",
        "                batchBlock.appendChild(block);\n",
        "                buttonIt = document.createElement('button');\n",
        "                buttonIt.className = 'collapsible';\n",
        "                buttonIt.style = 'cursor:pointer;width:100%;margin-bottom:5px;border:none;border-bottom:3px solid #999999;padding:5px;text-align:center;font-size:16px;font-weight:bold;color:white;background-color:rgba(155,155,155,0.15);text-shadow: 1px 1px rgba(0,0,0,0.35);transition: all 0.5s;'\n",
        "                buttonIt.innerHTML = 'Iteration '+i;\n",
        "                buttonIt.value = 'Iteration '+i;\n",
        "                block.before(buttonIt)\n",
        "            }\n",
        "            if(img == null && cont == null) {\n",
        "                cont = document.createElement('div');\n",
        "                cont.id = name+'_container';\n",
        "                link = document.createElement('a');\n",
        "                link.href = image;\n",
        "                link.target = '_blank';\n",
        "                img = document.createElement('img');\n",
        "                img.id = name;\n",
        "                img.class = \"resultImage\"\n",
        "                cont.style = 'display:inline-block;width:auto;font-size:14px;font-weight:bold;background-color:rgba(0,0,0,0.5);border-radius:5px;padding:2px;margin:2px;box-shadow: 0px 0px 5px rgba(0,0,0,0.5);'\n",
        "                cont.innerHTML = '<p style=\"margin:3px auto;width:180px;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-shadow: 1px 1px rgba(0,0,0,0.35);\">'+name+'</p>';\n",
        "                block.appendChild(cont);\n",
        "                cont.appendChild(link);\n",
        "                link.appendChild(img);\n",
        "            }\n",
        "            img.src = image;\n",
        "            img.style = \"margin: 5px; vertical-align: text-top; max-width: \"+max_width+\"px; max-height: 768px;\";\n",
        "        }\n",
        "\n",
        "        function debugBase64(base64URL){\n",
        "            var win = window.open();\n",
        "            win.document.write('<iframe src=\"' + base64URL  + '\" frameborder=\"0\" style=\"border:0; top:0px; left:0px; bottom:0px; right:0px; width:100%; height:100%;\" allowfullscreen></iframe>');\n",
        "        }\n",
        "\n",
        "        var coll = document.getElementsByClassName(\"collapsible\");\n",
        "        var i;\n",
        "        for (i = 0; i < coll.length; i++) {\n",
        "\n",
        "            coll[i].addEventListener('mouseover',function(){\n",
        "                this.style.color = 'orange';\n",
        "                this.style.borderBottom = \"3px solid orange\";\n",
        "            })\n",
        "\n",
        "            coll[i].addEventListener('mouseleave',function(){\n",
        "                this.style.color = 'white';\n",
        "                this.style.borderBottom = \"3px solid #999\";\n",
        "            })\n",
        "\n",
        "            coll[i].addEventListener(\"click\", function() {\n",
        "                this.classList.toggle(\"active\");\n",
        "                var content = this.nextElementSibling;\n",
        "                if (content.style.display === \"block\") {\n",
        "                content.style.display = \"none\";\n",
        "                } else {\n",
        "                content.style.display = \"block\";\n",
        "                }\n",
        "            });\n",
        "\n",
        "        }\n",
        "    ''')\n",
        "    height, width = img.shape[:2]\n",
        "    ret, data = cv2.imencode('.png', img)\n",
        "    data = b64encode(data)\n",
        "    data = data.decode()\n",
        "    data = 'data:image/png;base64,' + data\n",
        "    display(js)\n",
        "    eval_js(f'showImage({b}, {i}, {max_width}, {int(prepend)}, \"{name}\", \"{data}\", {width}, {height})')\n",
        "\n",
        "def clearOutputArea(b, i):\n",
        "    from IPython.display import display, Javascript\n",
        "    from google.colab.output import eval_js\n",
        "    js = Javascript('''\n",
        "        function onReady(fn) {\n",
        "            if (document.readyState==='complete' || document.readyState==='interactive') {\n",
        "                setTimeout(fn, 1);\n",
        "            } else {\n",
        "                document.addEventListener(\"DOMContentLoaded\", fn);\n",
        "            }\n",
        "        }\n",
        "        function clearColabOutput(b, i) {\n",
        "            var streams = document.getElementsByClassName('stream');\n",
        "            var dataOutputs = document.getElementsByClassName('display_data');\n",
        "            for(var i = 0; i < streams.length; i++) {\n",
        "                streams[i].innerHTML = '';\n",
        "            }\n",
        "            for(var i = 0; i < dataOutputs.length; i++) {\n",
        "                dataOutputs[i].innerHTML = ''\n",
        "            }\n",
        "        }\n",
        "    ''')\n",
        "    display(js)\n",
        "    eval_js(f'onReady(clearColabOutput({b}, {i}));')\n",
        "\n",
        "def closest_value(input_list, input_value):\n",
        "    difference = lambda input_list : abs(input_list - input_value)\n",
        "    res = min(input_list, key=difference)\n",
        "    return res\n",
        "\n",
        "def printPrompt(prompt, limit=12):\n",
        "    pw = prompt.split(\" \"); i=0; oi=0; pstr = ''\n",
        "    for w in pw:\n",
        "        oi+=1; pstr += f'{w} '\n",
        "        if i is limit or oi is len(pw): print(pstr.strip()); pstr = ''; i = 0; pass\n",
        "        i+=1\n",
        "\n",
        "def sharpenImage(image, samples=1):\n",
        "    import PIL\n",
        "    from PIL import Image, ImageFilter\n",
        "    im = image\n",
        "    for i in range(samples):\n",
        "        im = im.filter(ImageFilter.SHARPEN)\n",
        "    return im\n",
        "\n",
        "def medianFilter(img, diameter, sigmaColor, sigmaSpace):\n",
        "    from PIL import Image\n",
        "    import cv2 as cv\n",
        "    import numpy as np\n",
        "    diameter = int(diameter); sigmaColor = int(sigmaColor); sigmaSpace = int(sigmaSpace)\n",
        "    img = img.convert('RGB')\n",
        "    img = cv.cvtColor(np.array(img), cv.COLOR_RGB2BGR)\n",
        "    img = cv.bilateralFilter(img, diameter, sigmaColor, sigmaSpace)\n",
        "    img = cv.cvtColor(np.array(img), cv.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img).convert('RGB')\n",
        "\n",
        "def portraitBlur(img, mask, radius=5, samples=1):\n",
        "    from PIL import Image, ImageFilter\n",
        "    mask = mask.resize(img.size).convert('L')\n",
        "    #bimg = img.filter(ImageFilter.BoxBlur(int(boxBlur)))\n",
        "    bimg = medianFilter(img, radius, (radius * 500), 75)\n",
        "    bimg.convert(img.mode)\n",
        "    rimg = None\n",
        "    if samples > 1:\n",
        "        for i in range(samples):\n",
        "            if i is 0:\n",
        "                rimg = Image.composite(img, bimg, mask)\n",
        "            else:\n",
        "                rimg = Image.composite(rimg, bimg, mask)\n",
        "    else:\n",
        "        rimg = Image.composite(img, bimg, mask).convert('RGB')\n",
        "    \n",
        "    return rimg\n",
        "\n",
        "def PILSampler(sampling='LANCZOS'):\n",
        "    import PIL\n",
        "    from PIL import Image\n",
        "    if not hasattr(PIL.Image, 'Resampling'): PIL.Image.Resampling = PIL.Image\n",
        "    samplers = {'none': Image.Resampling.NONE,'lanczos': Image.Resampling.LANCZOS,'bilinear': Image.Resampling.LINEAR,'bicubic': Image.Resampling.CUBIC,'box': Image.Resampling.BOX,'hamming': Image.Resampling.HAMMING}\n",
        "    return Image.Resampling(int(sampling)) if sampling.isdigit() and int(sampling)<=5 else ( samplers[sampling.lower()] if samplers.__contains__(sampling.lower()) else Image.Resampling(1) )\n",
        "        \n",
        "\n",
        "def overlayImage(upscaled, source, percent=0.5, sampling='LANCZOS', superres=False):\n",
        "    import PIL\n",
        "    from PIL import Image, ImageFilter\n",
        "    if percent > 1: percent = 1\n",
        "    sampler = PILSampler(sampling)\n",
        "    source_upscaled = source.copy()\n",
        "    if superres:\n",
        "        source_upscaled = source_upscaled.filter(ImageFilter.SMOOTH_MORE)\n",
        "        source_upscaled_edge = source_upscaled.filter(ImageFilter.EDGE_ENHANCE)\n",
        "        source_upscaled = Image.blend(source_upscaled, source, 0.5)\n",
        "        source_upscaled = Image.blend(source_upscaled, source_upscaled_edge, 0.25)\n",
        "        source_upscaled = source_upscaled.resize((upscaled.size[0]*8, upscaled.size[0]*8), Image.Resampling.LINEAR)\n",
        "        source_upscaled = source_upscaled.filter(ImageFilter.SHARPEN)\n",
        "    source_upscaled = source_upscaled.resize(upscaled.size, sampler)\n",
        "    return Image.blend(upscaled.convert('RGB'), source_upscaled.convert('RGB'), percent)\n",
        "\n",
        "def getInitImages(path, filters='', verbose=False):\n",
        "    ret_images = []\n",
        "    valid = ['.jpeg','.jpg','.gif','.png']\n",
        "    if filters is not '':\n",
        "        filters = [f.strip() for f in filters.split(',')] if ',' in filters else [filters]\n",
        "    if path.startswith('http://') or path.startswith('https://'):\n",
        "        add = True\n",
        "        if filters is not '':\n",
        "            add = False\n",
        "            for f in filters:\n",
        "                if f in os.path.basename(path):\n",
        "                    add = True\n",
        "        if add:\n",
        "            if verbose: print(f'Found 1 remote image: {path}\\n')\n",
        "            return path\n",
        "        else: \n",
        "            if verbose: print(f'Found no valid image(s)\\n')\n",
        "            return None\n",
        "    if os.path.isdir(path):\n",
        "        try:\n",
        "            images = next(os.walk(path), (None, None, []))[2]\n",
        "            ret_images = []\n",
        "            if images:\n",
        "                if verbose: print(f\"Found {len(images)} image(s) in {path}\\n\")\n",
        "                for img in sorted(images):\n",
        "                    ext = os.path.splitext(img)[1]\n",
        "                    if ext in valid:\n",
        "                        add = True\n",
        "                        if filters is not '':\n",
        "                            add = False\n",
        "                            if verbose: print(\"Filtering with:\", filters)\n",
        "                            for f in filters:\n",
        "                                if f in img:\n",
        "                                    add = True\n",
        "                        if add:\n",
        "                            img = f'{path}/{img}'\n",
        "                            if verbose: print(f' -> {img}', defaultprint=True)\n",
        "                            ret_images.append(img)\n",
        "                print('')\n",
        "            if len(ret_images) == 0:\n",
        "                if verbose: print(f'Found no valid image(s)\\n')\n",
        "                return None\n",
        "        except OSError as e:\n",
        "            raise e\n",
        "    elif os.path.isfile(path):\n",
        "        try:\n",
        "            if path.lower().endswith('.txt'):\n",
        "                with open(path, \"r\") as f:\n",
        "                    images = f.read().splitlines()\n",
        "                    if images:\n",
        "                        ret_images = []\n",
        "                        if verbose: print(f\"Found {len(images)} image(s) in {path}\\n\")\n",
        "                        for img in sorted(images):\n",
        "                            ext = os.path.splitext(img)[1]\n",
        "                            if ext in valid:\n",
        "                                add = True\n",
        "                                if verbose: print(\"Filtering with:\", filters)\n",
        "                                if filters is not '':\n",
        "                                    add = False\n",
        "                                    for f in filters:\n",
        "                                        if f in img:\n",
        "                                            add = True\n",
        "                                if add:\n",
        "                                    if verbose: print(f' -> {img}', defaultprint=True)\n",
        "                                    ret_images.append(img)\n",
        "                        #ret_images.sort(0)\n",
        "                        print('')\n",
        "            else:\n",
        "                ext = os.path.splitext(path)[1]\n",
        "                if ext.lower() in valid:\n",
        "                    add = True\n",
        "                    if filters is not '':\n",
        "                        add = False\n",
        "                        for f in filters:\n",
        "                            if f in os.path.basename(path):\n",
        "                                add = True\n",
        "                    if add:\n",
        "                        if verbose: print(f'Found 1 image: {path}\\n')\n",
        "                        return path\n",
        "                    else:\n",
        "                        if verbose: print(f'Found no valid image(s)\\n')\n",
        "                        return None\n",
        "                else:\n",
        "                    if verbose: print(f'Found no valid image(s)\\n')\n",
        "                    return None\n",
        "        except OSError as e:\n",
        "            raise e\n",
        "    return ret_images\n",
        "\n",
        "def setup_pipe(pipe_type='default', model_id=None, model_cache=None, device='cuda'):\n",
        "    from diffusers import DiffusionPipeline\n",
        "    global TOKEN\n",
        "    clean_env()\n",
        "    print(\":gear: Loading pipeline into memory...\")\n",
        "    if pipe_type == 'default':\n",
        "        return DiffusionPipeline.from_pretrained(model_id, custom_pipeline=\"lpw_stable_diffusion\", cache_dir=model_cache, use_auth_token=TOKEN).to(device)\n",
        "    else:\n",
        "        return DiffusionPipeline.from_pretrained(model_id, custom_pipeline=\"lpw_stable_diffusion\", cache_dir=model_cache, torch_dtype=torch.float16, use_auth_token=TOKEN).to(device)\n",
        "\n",
        "def cache_pipe(pipe_type, model_id, model_cache, PIPE_CACHE, device='cuda'):\n",
        "    global RECACHE_PIPES, TOKEN\n",
        "    import joblib, os, gc\n",
        "    from diffusers import DiffusionPipeline\n",
        "    if not os.path.exists(f'{PIPE_CACHE}/lpw.pipe') or RECACHE_PIPES:\n",
        "        if pipe_type == 'default':\n",
        "            joblib.dump(DiffusionPipeline.from_pretrained(model_id, custom_pipeline=\"lpw_stable_diffusion\", cache_dir=model_cache, use_auth_token=TOKEN).to(device), f'{PIPE_CACHE}/lpw.pipe')\n",
        "        else:\n",
        "            joblib.dump(DiffusionPipeline.from_pretrained(model_id, custom_pipeline=\"lpw_stable_diffusion\", cache_dir=model_cache, torch_dtype=torch.float16, use_auth_token=TOKEN).to(device), f'{PIPE_CACHE}/lpw.pipe')\n",
        "        if os.path.exists(f'{PIPE_CACHE}/lpw.pipe'):\n",
        "            print('Cached pipe:', f'{PIPE_CACHE}/lpw.pipe')\n",
        "        gc.collect()\n",
        "    pipe = joblib.load(f'{PIPE_CACHE}/lpw.pipe')\n",
        "    #if pipe.model_id != model_id:\n",
        "    #    pipe.model_id = model_id\n",
        "    return pipe\n",
        "\n",
        "    return None\n",
        "\n",
        "def safetyCheckerDummy(images, **kwargs):\n",
        "    return images, False\n",
        "\n",
        "def preprocess(image):\n",
        "    import PIL\n",
        "    import numpy as np\n",
        "    w, h = image.size\n",
        "    w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.0 * image - 1.0\n",
        "\n",
        "# Optimization Functions\n",
        "\n",
        "def forward(self, x, context=None, mask=None):\n",
        "\n",
        "    import math\n",
        "    from torch import einsum\n",
        "    import subprocess\n",
        "    try:\n",
        "      from einops import rearrange\n",
        "    except ModuleNotFoundError:\n",
        "      subprocess.run(['pip', 'install', 'einops'], stdout=subprocess.DEVNULL)\n",
        "      from einops import rearrange\n",
        "    import types\n",
        "    from diffusers.models.attention import CrossAttention\n",
        "    import torch\n",
        "    batch_size, sequence_length, dim = x.shape\n",
        "\n",
        "    h = self.heads\n",
        "\n",
        "    q = self.to_q(x)\n",
        "    context = context if context is not None else x\n",
        "    k = self.to_k(context)\n",
        "    v = self.to_v(context)\n",
        "    del context, x\n",
        "\n",
        "    q = self.reshape_heads_to_batch_dim(q)\n",
        "    k = self.reshape_heads_to_batch_dim(k)\n",
        "    v = self.reshape_heads_to_batch_dim(v)\n",
        "\n",
        "    r1 = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device)\n",
        "\n",
        "    stats = torch.cuda.memory_stats(q.device)\n",
        "    mem_total = torch.cuda.get_device_properties(0).total_memory\n",
        "    mem_active = stats['active_bytes.all.current']\n",
        "    mem_free = mem_total - mem_active\n",
        "\n",
        "    mem_required = q.shape[0] * q.shape[1] * k.shape[1] * 4 * 2.5\n",
        "    steps = 1\n",
        "\n",
        "    if mem_required > mem_free:\n",
        "        steps = 2**(math.ceil(math.log(mem_required / mem_free, 2)))\n",
        "\n",
        "    slice_size = q.shape[1] // steps if (q.shape[1] % steps) == 0 else q.shape[1]\n",
        "    for i in range(0, q.shape[1], slice_size):\n",
        "        end = i + slice_size\n",
        "        s1 = einsum('b i d, b j d -> b i j', q[:, i:end], k)\n",
        "        s1 *= self.scale\n",
        "\n",
        "        s2 = s1.softmax(dim=-1)\n",
        "        del s1\n",
        "\n",
        "        r1[:, i:end] = einsum('b i j, b j d -> b i d', s2, v)\n",
        "        del s2\n",
        "\n",
        "    del q, k, v\n",
        "\n",
        "    r2 = rearrange(r1, '(b h) n d -> b n (h d)', h=h)\n",
        "    del r1\n",
        "\n",
        "    return self.to_out(r2)\n",
        "\n",
        "def optimize_attention(model):\n",
        "    import types\n",
        "    from diffusers.models.attention import CrossAttention\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, CrossAttention):\n",
        "            module.forward = types.MethodType(forward, module)\n",
        "\n",
        "# End Optimization Functions\n",
        "\n",
        "def move_files(source, destination):\n",
        "    for src_dir, dirs, files in os.walk(source):\n",
        "        dst_dir = src_dir.replace(source, destination)\n",
        "        if not os.path.exists(dst_dir):\n",
        "            os.mkdir(dst_dir)\n",
        "        for file_ in files:\n",
        "            src_file = os.path.join(src_dir, file_)\n",
        "            dst_file = os.path.join(dst_dir, file_)\n",
        "            if os.path.exists(dst_file):\n",
        "                os.remove(dst_file)\n",
        "            shutil.copy(src_file, dst_dir)\n",
        "\n",
        "def download_model(model_id, redownload=False):\n",
        "    import os, shutil\n",
        "    global MOVED_FROM_CACHE, last_model\n",
        "    rep = ['CompVis/','hakurei/']\n",
        "    localf = model_id\n",
        "    for r in rep:\n",
        "        localf = model_id.replace(r, '')\n",
        "    model = f'{model_cache}/{localf}'\n",
        "    if redownload and MOVED_FROM_CACHE:\n",
        "        MOVED_FROM_CACHE = False\n",
        "    drive_model = f'{DRIVE_MODEL_CACHE}/{localf}'\n",
        "    if USE_DRIVE_FOR_MODELS and not USE_DRIVE_FOR_LOCAL_COPIES and not MOVED_FROM_CACHE and not redownload:\n",
        "        print(\":open_file_folder: Moving model files to model cache from drive cache ...\")\n",
        "        if not os.path.exists(model_cache):\n",
        "            os.makedirs(model_cache)\n",
        "        if os.path.exists(drive_model) or len(os.listdir(DRIVE_MODEL_CACHE)) > 0:\n",
        "            try:\n",
        "                move_files(DRIVE_MODEL_CACHE, model_cache)\n",
        "                print(\":check_mark_button: Move complete.\")\n",
        "                redownload = False\n",
        "                MOVED_FROM_CACHE = True\n",
        "            except OSError as e:\n",
        "                redownload = True\n",
        "                print(\"Uneable to move model cache from:\", DRIVE_MODEL_CACHE)\n",
        "                pass\n",
        "        else:\n",
        "            print(f':WARNING: \\'{model_id}\\' doesn\\'t exist in \\'{DRIVE_MODEL_CACHE}\\', or any other model weights or models!')\n",
        "            redownload = True\n",
        "    if redownload:\n",
        "        if os.path.exists(model):\n",
        "            shutil.rmtree(model)\n",
        "        os.chdir(model_cache)\n",
        "        print(\":hourglass_not_done: Downloading model weights for:\", model_id)\n",
        "        print(subprocess.run(['git', 'clone', f'https://{hu}:{ht}@huggingface.co/{model_id}'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(\":check_mark_button: Downloaded complete.\")\n",
        "        os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "    if USE_DRIVE_FOR_MODELS and not USE_DRIVE_FOR_LOCAL_COPIES and not MOVED_FROM_CACHE:\n",
        "        print(\":open_file_folder: Moving model files to drive cache ...\")\n",
        "        if not os.path.exists(DRIVE_MODEL_CACHE):\n",
        "            os.makedirs(DRIVE_MODEL_CACHE)\n",
        "        if os.path.exists(model) or len(os.listdir(model_cache)) > 0:\n",
        "            move_files(model_cache, DRIVE_MODEL_CACHE)\n",
        "            print(\":check_mark_button: Move complete.\")\n",
        "        else:\n",
        "            print(f':WARNING: \\'{model_id}\\' doesn\\'t exist in \\'{model_cache}\\', or any other model weights or models!')\n",
        "        MOVED_FROM_CACHE = True\n",
        "\n",
        "# End Optimization Functions\n",
        "\n",
        "# GO BIG Functions\n",
        "\n",
        "def addalpha(im, mask):\n",
        "    imr, img, imb, ima = im.split()\n",
        "    mmr, mmg, mmb, mma = mask.split()\n",
        "    im = Image.merge('RGBA', [imr, img, imb, mma])  # we want the RGB from the original, but the transparency from the mask\n",
        "    return(im)\n",
        "\n",
        "# Alternative method composites a grid of images at the positions provided\n",
        "def grid_merge(source, slices):\n",
        "    source.convert(\"RGBA\")\n",
        "    for slice, posx, posy in slices: # go in reverse to get proper stacking\n",
        "        source.alpha_composite(slice, (posx, posy))\n",
        "    return source\n",
        "\n",
        "def grid_coords(target, original, overlap, maxed):\n",
        "    #generate a list of coordinate tuples for our sections, in order of how they'll be rendered\n",
        "    #target should be the size for the gobig result, original is the size of each chunk being rendered\n",
        "    target_x, target_y = target\n",
        "    original_x, original_y = original\n",
        "    do_calc = True\n",
        "    while do_calc:\n",
        "        center = []\n",
        "        center_x = int(target_x / 2)\n",
        "        center_y = int(target_y / 2)\n",
        "        x = center_x - int(original_x / 2)\n",
        "        y = center_y - int(original_y / 2)\n",
        "        center.append((x,y)) #center chunk\n",
        "        uy = y #up\n",
        "        uy_list = []\n",
        "        dy = y #down\n",
        "        dy_list = []\n",
        "        lx = x #left\n",
        "        lx_list = []\n",
        "        rx = x #right\n",
        "        rx_list = []\n",
        "        while uy > 0: #center row vertical up\n",
        "            uy = uy - original_y + overlap\n",
        "            uy_list.append((lx, uy))\n",
        "        while (dy + original_y) <= target_y: #center row vertical down\n",
        "            dy = dy + original_y - overlap\n",
        "            dy_list.append((rx, dy))\n",
        "        while lx > 0:\n",
        "            lx = lx - original_x + overlap\n",
        "            lx_list.append((lx, y))\n",
        "            uy = y\n",
        "            while uy > 0:\n",
        "                uy = uy - original_y + overlap\n",
        "                uy_list.append((lx, uy))\n",
        "            dy = y\n",
        "            while (dy + original_y) <= target_y:\n",
        "                dy = dy + original_y - overlap\n",
        "                dy_list.append((lx, dy))\n",
        "        while (rx + original_x) <= target_x:\n",
        "            rx = rx + original_x - overlap\n",
        "            rx_list.append((rx, y))\n",
        "            uy = y\n",
        "            while uy > 0:\n",
        "                uy = uy - original_y + overlap\n",
        "                uy_list.append((rx, uy))\n",
        "            dy = y\n",
        "            while (dy + original_y) <= target_y:\n",
        "                dy = dy + original_y - overlap\n",
        "                dy_list.append((rx, dy))\n",
        "        if maxed:\n",
        "            # calculate a new size that will fill the canvas, which will be optionally used in grid_slice and go_big\n",
        "            last_coordx, last_coordy = dy_list[-1:][0]\n",
        "            render_edgey = last_coordy + original_y # outer bottom edge of the render canvas\n",
        "            render_edgex = last_coordx + original_x # outer side edge of the render canvas\n",
        "            render_edgex += (render_edgex - target_x) # we have to extend the \"negative\" side as well, so we do it twice\n",
        "            render_edgey += (render_edgey - target_y)\n",
        "            scalarx = render_edgex / target_x\n",
        "            scalary = render_edgey / target_y\n",
        "            if scalarx <= scalary:\n",
        "                target_x = int(target_x * scalarx)\n",
        "                target_y = int(target_y * scalarx)\n",
        "            else:\n",
        "                target_x = int(target_x * scalary)\n",
        "                target_y = int(target_y * scalary)\n",
        "            maxed = False\n",
        "        else:\n",
        "            do_calc = False\n",
        "    # now put all the chunks into one master list of coordinates (essentially reverse of how we calculated them so that the central slices will be on top)\n",
        "    result = []\n",
        "    for coords in dy_list[::-1]:\n",
        "        result.append(coords)\n",
        "    for coords in uy_list[::-1]:\n",
        "        result.append(coords)\n",
        "    for coords in rx_list[::-1]:\n",
        "        result.append(coords)\n",
        "    for coords in lx_list[::-1]:\n",
        "        result.append(coords)\n",
        "    result.append(center[0])\n",
        "    return result, (target_x, target_y)\n",
        "\n",
        "# Chop our source into a grid of images that each equal the size of the original render\n",
        "def grid_slice(source, overlap, og_size, maxed=False):\n",
        "    global SCALING_SAMPLER\n",
        "    width, height = og_size # size of the slices to be rendered\n",
        "    coordinates, new_size = grid_coords(source.size, og_size, overlap, maxed)\n",
        "    if source.size != new_size:\n",
        "        source = source.resize(new_size, PILSampler(SCALING_SAMPLER))\n",
        "    slices = []\n",
        "    for coordinate in coordinates:\n",
        "        x, y = coordinate\n",
        "        slices.append(((source.crop((x, y, x+width, y+height))), x, y))\n",
        "    global slices_todo\n",
        "    slices_todo = len(slices) - 1\n",
        "    return slices, source\n",
        "\n",
        "def do_gobig(gobig_init, opt):\n",
        "    from PIL import ImageDraw\n",
        "    import time\n",
        "    \n",
        "    global ESRGAN_MODE, pipe_type, PROMPT, original_prompt, init, original_init, mask, original_mask, WIDTH, HEIGHT, SEED, NUM_ITERS, i, iteration, KEEP_ONLY_FINAL_IMAGE, SCALING_SAMPLER, GOBIG_SLICE_RUN, inst, SLICES_NUM\n",
        "    overlap = opt.gobig_overlap\n",
        "    outpath = opt.outdir\n",
        "    \n",
        "    # get our render size for each slice, and our target size\n",
        "    input_image = Image.open(fetch(gobig_init)).convert('RGB')\n",
        "    if opt.gobig_mask_image:\n",
        "        input_mask_image = Image.open(fetch(opt.gobig_mask_image)).convert('RGB')\n",
        "    if opt.gobig_prescaled == False:\n",
        "        opt.W, opt.H = input_image.size\n",
        "        target_W = opt.W * opt.gobig_scale\n",
        "        target_H = opt.H * opt.gobig_scale\n",
        "        if opt.gobig_realesrgan:\n",
        "            if not inst['ESRGAN_INSTALLED']:\n",
        "                install_realesrgan()\n",
        "            input_image = upscale(input_image, 2, ESRGAN_MODE)\n",
        "            if opt.gobig_mask_image:\n",
        "                input_mask_image = upscale(input_mask_image, 2, ESRGAN_MODE)\n",
        "        input_image = input_image.convert('RGBA')\n",
        "        target_image = input_image.resize((target_W, target_H), PILSampler(SCALING_SAMPLER)) #esrgan resizes 4x by default, so this brings us in line with our actual scale target\n",
        "        if opt.gobig_mask_image:\n",
        "            input_mask_image.convert('RGBA')\n",
        "            target_mask_image = input_mask_image.resize((target_W, target_H), PILSampler(SCALING_SAMPLER)) \n",
        "    else:\n",
        "        target_W, target_H = input_image.size\n",
        "        target_image = input_image.convert('RGBA')\n",
        "        if opt.gobig_mask_image:\n",
        "            target_mask_image = input_mask_image\n",
        "            target_mask_image.convert('L')\n",
        "    \n",
        "    slices, target_image = grid_slice(target_image, overlap, (opt.W, opt.H), opt.gobig_maximize)\n",
        "    if opt.gobig_mask_image:\n",
        "        mask_slices, target_mask_image = grid_slice(target_mask_image, overlap, (opt.W, opt.H), opt.gobig_maximize)\n",
        "    SLICES_NUM = len(slices)\n",
        "\n",
        "    if USE_BASIC_IMAGE_DISPLAY:\n",
        "        print('GOBIG Input:', display(target_image))\n",
        "    else:\n",
        "        displayJsImage((i+1), (iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'GOBIG Input B: {(i+1)} I: {(iteration+1)}', target_image)\n",
        "\n",
        "    if opt.gobig_mask_image:\n",
        "        if USE_BASIC_IMAGE_DISPLAY:\n",
        "            print('GOBIG Mask Input:', display(target_mask_image))\n",
        "        else:\n",
        "            displayJsImage((i+1), (iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'GOBIG Mask Input B: {(i+1)} I: {(iteration+1)}', target_mask_image)\n",
        "\n",
        "\n",
        "    print(f':information: GOBIG target resolution: {target_W}x{target_H}')\n",
        "\n",
        "    betterslices = []\n",
        "    slice_image = 'gobig_slice.png'\n",
        "    mask_slice_image = 'gobig_mask_slice.png'\n",
        "    original_pipe_type = pipe_type\n",
        "    pipe_type = 'img2img' if opt.gobig_mask_image is None else 'inpaint'\n",
        "    original_iteration = iteration\n",
        "    original_batch = i\n",
        "    iteration += 1\n",
        "\n",
        "    start_prompt = opt.gobig_prompt\n",
        "    PROMPT = start_prompt\n",
        "    INIT_SCALE = opt.gobig_scale\n",
        "    WIDTH = opt.W\n",
        "    HEIGHT = opt.H\n",
        "\n",
        "    GOBIG_SLICE_RUN = True\n",
        "\n",
        "    print(f\":information: Created {SLICES_NUM} slices. Starting IMG2IMG batch.\")\n",
        "\n",
        "    # now we trigger a do_run for each slice\n",
        "    sk = 0\n",
        "    for count, chunk_w_coords in enumerate(slices):\n",
        "        chunk, coord_x, coord_y = chunk_w_coords\n",
        "        chunk.save(slice_image)\n",
        "        if opt.gobig_mask_image:\n",
        "            mask_chunk, m_coord_x, m_coord_y = mask_slices[count]\n",
        "            mask_chunk.save(mask_slice_image)\n",
        "            mask = Image.open(fetch(mask_slice_image))\n",
        "            original_mask = mask.copy()\n",
        "\n",
        "        # Diffusion Options\n",
        "        init = Image.open(fetch(slice_image)).convert('RGB')\n",
        "        original_init = init.copy()\n",
        "\n",
        "        image, result = diffuse_run()\n",
        "        PROMPT = start_prompt #Fix for Style Prompts and NSP\n",
        "\n",
        "        if opt.gobig_hybrid_slice:\n",
        "            image = overlayImage(image, chunk, opt.gobig_hybrid_overlay, opt.gobig_hybrid_sampler, opt.gobig_hybrid_superres)\n",
        "\n",
        "        resultslice = image.convert('RGBA')\n",
        "        betterslices.append((resultslice.copy(), coord_x, coord_y))\n",
        "        resultslice.close()\n",
        "        image.close()\n",
        "        chunk.close()\n",
        "        if opt.gobig_keep_slices == False:\n",
        "            os.remove(result)\n",
        "\n",
        "        iteration += 1\n",
        "        sk += 1\n",
        "\n",
        "    GOBIG_SLICE_RUN = False\n",
        "\n",
        "    # create an alpha channel for compositing the slices\n",
        "    alpha = Image.new('L', (opt.W, opt.H), color=0xFF)\n",
        "    alpha_gradient = ImageDraw.Draw(alpha)\n",
        "    a = 0\n",
        "    oi = 0\n",
        "    a_overlap = int(overlap / 2) # we want the alpha gradient to be half the size of the overlap, otherwise we always see some of the original background underneath\n",
        "    shape = ((opt.W, opt.H), (0,0))\n",
        "    while oi < overlap:\n",
        "        alpha_gradient.rectangle(shape, fill = a)\n",
        "        a += int(255 / a_overlap)\n",
        "        a = 255 if a > 255 else a\n",
        "        oi += 1\n",
        "        shape = ((opt.W - oi, opt.H - oi), (oi,oi))\n",
        "    if a != 255:\n",
        "      # in case we didn't get to full opaque, this ensures we do\n",
        "      alpha_gradient.rectangle(shape, fill = 255)\n",
        "    mask = Image.new('RGBA', (opt.W, opt.H), color=0)\n",
        "    mask.putalpha(alpha)\n",
        "    # now composite the slices together\n",
        "    finished_slices = []\n",
        "    for betterslice, x, y in betterslices:\n",
        "        finished_slice = addalpha(betterslice, mask)\n",
        "        finished_slices.append((finished_slice, x, y))\n",
        "    final_output = grid_merge(target_image, finished_slices)\n",
        "    # name the file in a way that hopefully doesn't break things\n",
        "    \n",
        "    original_result = result\n",
        "    result = result.replace('.png','_GOBIG')\n",
        "\n",
        "    print(f'\\n:information: GOBIG output saved as {result}{opt.filetype}\\n')\n",
        "    if KEEP_ONLY_FINAL_IMAGE:\n",
        "        final_output.save(result.replace('.png', ''), quality = opt.quality, subsampling = opt.subsampling)\n",
        "    else:\n",
        "        final_output.save(f'{result}{opt.filetype}', quality = opt.quality, subsampling = opt.subsampling)\n",
        "    if USE_BASIC_IMAGE_DISPLAY:\n",
        "        print('\\nFinal GOBIG  Output:', display(final_output))\n",
        "    else:\n",
        "        displayJsImage((original_batch+1), (original_iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'GOBIG OUTPUT B: {(original_batch+1)} I: {(original_iteration+1)}', final_output)\n",
        " \n",
        "    pipe_type = original_pipe_type\n",
        "    iteration = original_iteration\n",
        "    final_output.close()\n",
        "    input_image.close()\n",
        "\n",
        "# End Go Big Functions\n",
        "\n",
        "# Conceptualizer Functions\n",
        "\n",
        "# Download a concept from HF or custom URL\n",
        "def download_concept(model):\n",
        "    os.chdir(EMBEDDINGS_PATH)\n",
        "    token_string = None\n",
        "    if model.startswith('http'):\n",
        "        try:\n",
        "            #if 'git-lfs' not in packages():\n",
        "            #    print(subprocess.run(['pip', 'install', 'git-lfs'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            #print(subprocess.run(['git', 'clone', model], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "            wgeto(model, f'{EMBEDDINGS_PATH}/learned_embeds.bin')\n",
        "        except OSError as e:\n",
        "            raise e\n",
        "    else:\n",
        "        embeds_path = hf_hub_download(repo_id=model, filename=\"learned_embeds.bin\")\n",
        "        token_path = hf_hub_download(repo_id=model, filename=\"token_identifier.txt\")\n",
        "        shutil.copy(embeds_path, EMBEDDINGS_PATH)\n",
        "        shutil.copy(token_path, EMBEDDINGS_PATH)\n",
        "        with open(f'{EMBEDDINGS_PATH}/token_identifier.txt', 'r') as file:\n",
        "            token_string = file.read()\n",
        "    os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "    return (f'{EMBEDDINGS_PATH}/learned_embeds.bin', token_string)\n",
        "\n",
        "# Load and return tokenizer and text encoder for concepts\n",
        "def CTTE(model_id):\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n",
        "    text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\")\n",
        "    return (tokenizer, text_encoder)\n",
        "\n",
        "def load_concept(learned_embeds_path, text_encoder, tokenizer, token=None):\n",
        "    model_path, token_text = learned_embeds_path\n",
        "    from io import BytesIO\n",
        "    with open(model_path, \"rb\") as fh:\n",
        "        buf = BytesIO(fh.read())\n",
        "        loaded_learned_embeds = torch.load(buf, map_location=\"cpu\")\n",
        "    # separate token and the embeds\n",
        "    embeds = loaded_learned_embeds[token_text]\n",
        "    # cast to dtype of text_encoder\n",
        "    dtype = text_encoder.get_input_embeddings().weight.dtype\n",
        "    embeds.to('cuda')\n",
        "    # add the token in tokenizer\n",
        "    token = token if token is not None else token_text\n",
        "    num_added_tokens = tokenizer.add_tokens(token)\n",
        "    if num_added_tokens == 0:\n",
        "        print(f\":information: The tokenizer already contains the token {token}. It is either already loaded, or in use.\")\n",
        "        token = f'{token}-2'\n",
        "        tokenizer.add_tokens(token)\n",
        "        print(f\":information: Attempted to add {token} as fail safe alternative...\")\n",
        "    # resize the token embeddings\n",
        "    text_encoder.resize_token_embeddings(len(tokenizer))\n",
        "    # get the id for the token and assign the embeds\n",
        "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
        "    text_encoder.get_input_embeddings().weight.data[token_id] = embeds\n",
        "\n",
        "# Dynamic value - takes ready-made possible options within a string and returns the string with an option randomly selected\n",
        "# Format is \"I will return [Value1|Value2|Value3] in this string\"\n",
        "# Which would come back as \"I will return Value2 in this string\" (for example)\n",
        "# Optionally if a value of ^^# is first, it means to return that many dynamic values,\n",
        "# so [^^2|Value1|Value2|Value3] in the above example would become:\n",
        "# \"I will return Value3 Value2 in this string\"\n",
        "# note: for now assumes a string for return. TODO return a desired type\n",
        "def dynamic_value(incoming):\n",
        "    if type(incoming) == str:  # we only need to do something if it's a string...\n",
        "        if \"[\" in incoming:   # ...and if < is in the string...\n",
        "            text = incoming\n",
        "            while \"[\" in text:\n",
        "                start = text.find('[')\n",
        "                end = text.find(']')\n",
        "                swap = text[(start + 1):end]\n",
        "                value = \"\"\n",
        "                count = 1\n",
        "                values = swap.split('|')\n",
        "                if \"^^\" in values[0]:\n",
        "                    count = values[0]\n",
        "                    values.pop(0)\n",
        "                    count = int(count[2:])\n",
        "                random.shuffle(values)\n",
        "                for i in range(count):\n",
        "                    value = value + values[i] + \" \"\n",
        "                value = value[:-1]  # remove final space\n",
        "                text = text.replace(f'[{swap}]', value, 1)\n",
        "            return text\n",
        "        else:\n",
        "            return incoming\n",
        "    else:\n",
        "        return incoming\n",
        "\n",
        "# SETUP DEPENDENCIES\n",
        "print(\"\\nStarting Installation Processess.\\nThis should take approximately one eternity...\\n\")\n",
        "\n",
        "try:\n",
        "  with fetch_bytes('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/key.txt') as f:\n",
        "    k = f.read().decode('utf-8').split(':'); hu = k[0].strip(); ht = k[1].strip()\n",
        "except OSError as e:\n",
        "  raise e\n",
        "\n",
        "try:\n",
        "\n",
        "    # Install psutil\n",
        "    if 'psutil' in packages():\n",
        "        print(':check_mark_button: \\'psutil\\' already installed.\\n')\n",
        "    else:\n",
        "        print(':hourglass_not_done: Installing \\'psutil\\' ...')\n",
        "        print(subprocess.run(['pip', '-q', 'install', 'psutil'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(':check_mark_button: \\'psutil\\' installed.\\n')\n",
        "    import psutil\n",
        "\n",
        "    if 'joblib' in packages():\n",
        "        print(':check_mark_button: \\'joblib\\' alrleady installed.\\n')\n",
        "    else:\n",
        "        print(':hourglass_not_done: Installing \\'joblib\\' ...')\n",
        "        print(subprocess.run(['pip', '-q', 'install', 'joblib'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(':check_mark_button: \\'joblib\\' installed.\\n')\n",
        "    import joblib\n",
        "    from joblib import Memory\n",
        "    cache_dir = f'{STABLE_DIFFUSION_WORKDIR}/cache'\n",
        "\n",
        "    # Install Shutup\n",
        "    if 'shutup' not in packages():\n",
        "        subprocess.run(['pip', '-q', 'install', 'shutup'], stdout=subprocess.DEVNULL)\n",
        "    import shutup; \n",
        "    if SUPPRESS_WARNINGS: \n",
        "        shutup.please()\n",
        "\n",
        "    import warnings\n",
        "    if SUPPRESS_WARNINGS:\n",
        "        warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
        "    \n",
        "    #rint(subprocess.run(['git', 'lfs', 'install'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "    #os.environ['GIT_LFS_SKIP_SMUDGE'] = \"0\"\n",
        "\n",
        "    # Install Pillow v9.x\n",
        "    pil_warning = None\n",
        "    if INSTALL_GOBIG_V2_SUPPORT:\n",
        "        print(':hourglass_not_done: Installing \\'GOBIG v2\\' support.')\n",
        "        upgrade_pillow = subprocess.run(['pip', '-q', 'install', 'Pillow==9.0.0'],stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        if upgrade_pillow.strip() is not '':\n",
        "            print(upgrade_pillow)\n",
        "        import PIL as PIL\n",
        "        from PIL import Image, ImageFilter, ImageOps, ImageStat, ImageEnhance, ImageDraw\n",
        "        from PIL.PngImagePlugin import PngInfo\n",
        "        if not Image.__version__.startswith('9'):\n",
        "            pil_warning = f\":warning: \\33[33m\\u001b[4mWARNING:\\33[0m \\033[1mYou are using Pillow verison \\33[33m{Image.__version__}\\33[0m\\033[1m, but version \\33[32m9.0.0\\33[0m \\033[1mwas just installed. \\nÂ Â Â Â Â Â Â Â Â Please go to Runtime -> Restart and Run all\\33[0m\"\n",
        "            print(\"\\n\"+pil_warning+\"\\n\")\n",
        "            if RESTART_COLAB_AFTER_GOBIG_INSTALL:\n",
        "                print(f\":information: \\33[33mRestarting colab in 5 seconds...\\33[0m\\n\")\n",
        "                time.sleep(5)\n",
        "                os.kill(os.getpid(), 9)\n",
        "        else: \n",
        "                print(f\":check_mark_button: \\'GOBIG v2\\' support installed successfuly!\\n\")\n",
        "\n",
        "    else:\n",
        "        import PIL as PIL\n",
        "        from PIL import Image, ImageFilter, ImageOps\n",
        "        from PIL.PngImagePlugin import PngInfo\n",
        "        if Image:\n",
        "            pil_warning = f\":warning: \\33[33m\\u001b[4mWARNING:\\33[0m \\033[1mYou are using Pillow verison \\33[33m{Image.__version__}\\33[0m\\033[1m, but version \\33[32m9.0.0\\33[0m \\033[1mis required for Go BIG v2. \\nÂ Â Â Â Â Â Â Â Â If you want to use Go BIG v2, please enable Go BIG v2 Support, Run Setup Environment again, and go to Runtime -> Restart and Run all\\33[0m\"\n",
        "            print(pil_warning+\"\\n\")\n",
        "\n",
        "    # Install Diffusers\n",
        "    if 'diffusers' not in packages():\n",
        "        print(':hourglass_not_done: Installing \\'diffusers\\' ...')\n",
        "        try:\n",
        "            subprocess.run(['pip', '-q', 'install', 'diffusers==0.7.2'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "            #subprocess.run(['pip', '-q', 'install', '-U', 'git+https://github.com/huggingface/diffusers.git@235770dd841080d60f734db09459d0f855ccda46'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        except OSError as e:\n",
        "            raise e\n",
        "        finally:\n",
        "            if 'diffusers' in packages():\n",
        "                print(':check_mark_button: \\'diffusers\\' installed.\\n')\n",
        "            else:\n",
        "                raise OSError(':warning: \\'diffusers\\' could not be installed.\\n')\n",
        "    else:\n",
        "        print(':check_mark_button: \\'diffusers\\' already installed.\\n')\n",
        "        \n",
        "    # Patch Safety Checker\n",
        "    if os.path.exists('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/safety_checker.py'):\n",
        "        os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "        wgeto('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/replacements/diffusers/pipelines/stable_diffusion/safety_checker.py', 'safety_checker.py')\n",
        "        shutil.copy('safety_checker.py', '/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/')\n",
        "\n",
        "    # Patch Stable Diffusion Pipelines\n",
        "    #if os.path.exists('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py'):\n",
        "    #    os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "    #    wgeto('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/replacements/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py', 'pipeline_stable_diffusion.py')\n",
        "    #    shutil.copy('pipeline_stable_diffusion.py', '/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/')\n",
        "\n",
        "    #if os.path.exists('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py'):\n",
        "    #    os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "    #    wgeto('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/replacements/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py', 'pipeline_stable_diffusion_img2img.py')\n",
        "    #    shutil.copy('pipeline_stable_diffusion_img2img.py', '/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/')\n",
        "\n",
        "    #if os.path.exists('/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py'):\n",
        "    #    os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "    #    wgeto('https://raw.githubusercontent.com/WASasquatch/easydiffusion/main/replacements/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py', 'pipeline_stable_diffusion_inpaint.py')\n",
        "    #    shutil.copy('pipeline_stable_diffusion_inpaint.py', '/usr/local/lib/python3.7/dist-packages/diffusers/pipelines/stable_diffusion/')\n",
        "\n",
        "    if 'transformers' not in packages():\n",
        "        print(':hourglass_not_done: Installing \\'transformers\\' ...')\n",
        "        res = None\n",
        "        try:\n",
        "            #!pip install -qq transformers\n",
        "            print(subprocess.run(['pip', '-q', 'install', 'transformers', 'ftfy'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        except OSError as e:\n",
        "            try:\n",
        "                import transformers\n",
        "            except ImportError as e:\n",
        "                print(':warning: \\'transformers\\' could not be installed.\\n')\n",
        "                raise e\n",
        "        finally:\n",
        "            if 'transformers' in packages():\n",
        "                print(':check_mark_button: \\'transformers\\' installed.\\n')\n",
        "            else:\n",
        "                print(':warning: \\'transformers\\' could not be installed.\\n')\n",
        "    else:\n",
        "        print(':check_mark_button: \\'transformers\\' already installed.\\n')\n",
        "\n",
        "    print(':hourglass_not_done: Installing pytorch dependencies...\\n')\n",
        "    res = ''\n",
        "    if 'pytorch-pretrained-bert' not in packages():\n",
        "        res += subprocess.run(['pip', '-q', 'install', 'pytorch-pretrained-bert'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    if 'spacy' not in packages():\n",
        "        res += subprocess.run(['pip', '-q', 'install', 'spacy'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    if 'ftfy' not in packages():\n",
        "        res += subprocess.run(['pip', '-q', 'install', 'ftfy'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    if 'accelerate' not in packages():\n",
        "        res += subprocess.run(['pip', '-q', 'install', 'accelerate'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    if res and res.strip() is not '':\n",
        "        print(res)\n",
        "    if ['pytorch-pretrained-bert', 'spacy', 'ftfy'] in packages():\n",
        "        print(':check_mark_button: pytorch dependencies installed.\\n')\n",
        "\n",
        "    if 'spacy' not in packages():\n",
        "        print(':hourglass_not_done: Setting up \\'spacy\\' ...\\n')\n",
        "        if SUPPRESS_WARNINGS:\n",
        "            subprocess.run(['python', '-m', 'spacy', 'download', 'en'], stdout=subprocess.DEVNULL)\n",
        "        else:\n",
        "            print(subprocess.run(['python', '-m', 'spacy', 'download', 'en'], stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        print(':check_mark_button: \\'spacy\\' setup complete.\\n')\n",
        "    else:\n",
        "        print(':check_mark_button: \\'spacy\\' already installed.\\n')\n",
        "\n",
        "    if 'scipy' not in packages():\n",
        "        print(':hourglass_not_done: Installing \\'scipy\\' ...')\n",
        "        res = None\n",
        "        try:\n",
        "            res = subprocess.run(['pip', '-q', 'install', 'scipy'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        except OSError as e:\n",
        "            raise e\n",
        "        finally:\n",
        "            if 'scipy' in packages():\n",
        "                print(':check_mark_button: \\'scipy\\' installed.\\n')\n",
        "            else:\n",
        "                print(':warning: \\'scipy\\' could not be installed.\\n')\n",
        "    else:\n",
        "        print(':check_mark_button: \\'scipy\\' already installed.\\n')\n",
        "\n",
        "    print(':globe_with_meridians: Logging into HuggingFace API...')\n",
        "    subprocess.run(['git', 'config', '--global', 'credential.helper', 'store'], stdout=subprocess.DEVNULL)\n",
        "    left_of_pipe = subprocess.Popen([\"echo\", ht], stdout=subprocess.PIPE)\n",
        "    right_of_pipe = subprocess.run(['huggingface-cli', 'login'], stdin=left_of_pipe.stdout, stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(right_of_pipe)\n",
        "\n",
        "    # Optional Installers\n",
        "\n",
        "    inst = {\n",
        "        'GFPGAN_INSTALLED': False,\n",
        "        'ESRGAN_INSTALLED': False,\n",
        "        'CODEFORMER_INSTALLED': False,\n",
        "        'KROMO_INSTALLED': False,\n",
        "        'MIDAS_INSTALLED': False,\n",
        "        'IMG2TEXTURE_INSTALLED': False\n",
        "    }\n",
        "\n",
        "    def install_gfpgan():\n",
        "        global opts\n",
        "        print(\"\\n:hourglass_not_done: Installing GFPGAN...\")\n",
        "        res = ''\n",
        "        if not os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/GFPGAN'):\n",
        "            res += subprocess.run(['git', 'clone', '--quiet', 'https://github.com/TencentARC/GFPGAN.git'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/GFPGAN')\n",
        "            wget(\"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth\", \"experiments/pretrained_models\")\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)    \n",
        "        if ['basicsr', 'facexlib'] not in packages():\n",
        "            res += subprocess.run(['pip', '-q', 'install', 'basicsr', 'facexlib'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "            res += subprocess.run(['pip', '-q', 'install', '-r', 'requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "            if SUPPRESS_WARNINGS:\n",
        "                subprocess.run(['python', 'setup.py', 'develop'], stdout=subprocess.DEVNULL)\n",
        "            else:\n",
        "                res += subprocess.run(['python', 'setup.py', 'develop'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        if 'realesrgan' not in packages():\n",
        "            res += subprocess.run(['pip', '-q', 'install', 'realesrgan'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        if res.strip() is not '':\n",
        "             print(res)\n",
        "        inst['GFPGAN_INSTALLED'] = True\n",
        "        print(\":check_mark_button: GFPGAN installed!\\n\")\n",
        "        \n",
        "    def install_realesrgan():\n",
        "        global opts\n",
        "        print(\"\\n:hourglass_not_done: Installing Real-ESRGAN\")\n",
        "        res = ''\n",
        "        if not os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/Real-ESRGAN'):\n",
        "            res += subprocess.run(['git', 'clone', '--quiet', 'https://github.com/sberbank-ai/Real-ESRGAN'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "            res += subprocess.run(['pip', '-q', 'install', '-r', 'Real-ESRGAN/requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "            wget(\"https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x2.pth\", \"Real-ESRGAN/weights/\")\n",
        "            wget(\"https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x4.pth\", \"Real-ESRGAN/weights/\")\n",
        "            wget(\"https://huggingface.co/datasets/db88/Enhanced_ESRGAN/resolve/main/RealESRGAN_x8.pth\", \"Real-ESRGAN/weights/\")\n",
        "        if res.strip() is not '':\n",
        "            print(res)\n",
        "        inst['ESRGAN_INSTALLED'] = True\n",
        "        print(\":check_mark_button: Real-ESRGAN installed!\\n\")\n",
        "        \n",
        "    def upscale(image, scale, device='cuda'):\n",
        "        os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/Real-ESRGAN')\n",
        "        from RealESRGAN import RealESRGAN\n",
        "        device = torch.device(device)\n",
        "        model = RealESRGAN(device, scale = scale)\n",
        "        model.load_weights(f'weights/RealESRGAN_x{scale}.pth')\n",
        "        sr_image = model.predict(np.array(image))\n",
        "        del model, device\n",
        "        os.chdir(f'{STABLE_DIFFUSION_WORKDIR}')\n",
        "        return sr_image\n",
        "\n",
        "    def install_codeformer():\n",
        "        global opts\n",
        "        print(\":hourglass_not_done: Downloading CodeFormer...\\n\")\n",
        "        res = ''\n",
        "        if not os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer'):\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "            res += subprocess.run(['git', 'clone', '--quiet', 'https://github.com/sczhou/CodeFormer.git'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "\n",
        "        os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer')\n",
        "        res += subprocess.run(['pip', '-q', 'install', '-r', 'requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        # Install basicsr\n",
        "        if SUPPRESS_WARNINGS:\n",
        "            subprocess.run(['python', 'basicsr/setup.py', 'develop'], stdout=subprocess.DEVNULL)\n",
        "        else:\n",
        "            res += subprocess.run(['python', 'basicsr/setup.py', 'develop'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "\n",
        "        # Download the pre-trained model\n",
        "        if SUPPRESS_WARNINGS:\n",
        "            subprocess.run(['python', 'scripts/download_pretrained_models.py', 'facelib'], stdout=subprocess.DEVNULL)\n",
        "            subprocess.run(['python', 'scripts/download_pretrained_models.py', 'CodeFormer'], stdout=subprocess.DEVNULL)\n",
        "        else: \n",
        "            res += subprocess.run(['python', 'scripts/download_pretrained_models.py', 'facelib'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "            res += subprocess.run(['python', 'scripts/download_pretrained_models.py', 'CodeFormer'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        if res.strip() is not '':\n",
        "            print(res)\n",
        "        os.makedirs('temp', exist_ok=True)\n",
        "        os.makedirs('results', exist_ok=True)\n",
        "        os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "        inst['CODEFORMER_INSTALLED'] = True\n",
        "        print(\":check_mark_button: CodeFormer downloaded!\\n\")\n",
        "\n",
        "    def install_kromo():\n",
        "        global opts\n",
        "        if 'kromo' not in packages():\n",
        "            print(\":hourglass_not_done: Installing \\'kromo\\' ...\")\n",
        "            res = ''\n",
        "            if not os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/kromo'):\n",
        "                os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "                res += subprocess.run(['git', 'clone', '--quiet', 'https://github.com/yoonsikp/kromo'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "            if 'kromo' not in packages():\n",
        "                os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/kromo')\n",
        "                res += subprocess.run(['pip', '-q', 'install', '-r', 'requirements.txt'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "                os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "        if res.strip() is not '':\n",
        "            print(res)\n",
        "        inst['KROMO_INSTALLED'] = True\n",
        "        print(':check_mark_button: \\'kromo\\' installed.\\n')\n",
        "\n",
        "    def install_midas():\n",
        "        global opts\n",
        "        if 'timm' not in packages():\n",
        "            print(\":hourglass_not_done: Installing MiDaS compatibility...\")\n",
        "            res =''\n",
        "            res += subprocess.run(['pip', '-q', 'install', 'timm'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "            if res.strip() is not '':\n",
        "                print(res)\n",
        "        inst['MIDAS_INSTALLED'] = True\n",
        "        print(\":check_mark_button: MiDaS compatibility installed!\\n\")\n",
        "\n",
        "    def install_img2texture():\n",
        "        global opts\n",
        "        if 'img2texture' not in packages():\n",
        "            print(\":hourglass_not_done: Installing \\'img2texture\\' ...\")\n",
        "            res = ''\n",
        "            res += subprocess.run(['pip3', '-q', 'install', 'img2texture'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "            if res.strip() is not '':\n",
        "                print(res)\n",
        "        inst['IMG2TEXTURE_INSTALLED'] = True\n",
        "        print(\":check_mark_button: img2texture installed.\\n\")\n",
        "\n",
        "    # Colab Param Scraper\n",
        "    try:\n",
        "        from colabparamscraper.paramscraper import paramScraper\n",
        "    except ImportError:\n",
        "        print(\":hourglass_not_done: Installing Colab paramScraper ...\")\n",
        "        res = ''\n",
        "        res += subprocess.run(['git', 'clone', '--quiet', 'https://github.com/WASasquatch/colabparamscraper'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        if res.strip() is not '':\n",
        "            print(res)\n",
        "    finally:\n",
        "        from colabparamscraper.paramscraper import paramScraper\n",
        "        print(\":check_mark_button: Colab paramScraper installed!\\n\")\n",
        "    \n",
        "    # Noodle Soup prompts\n",
        "    try:\n",
        "        import nsp_pantry\n",
        "    except ImportError:\n",
        "        if not os.path.exists('nsp_pantry.py'):\n",
        "            print(\":hourglass_not_done: Installing Noodle Soup Prompts...\")\n",
        "            wget('https://raw.githubusercontent.com/WASasquatch/noodle-soup-prompts/main/nsp_pantry.py', './')\n",
        "    finally:\n",
        "        import nsp_pantry\n",
        "        from nsp_pantry import nsp_parse\n",
        "\n",
        "    if nsp_parse:\n",
        "        print(\"\\r\\r:check_mark_button: \\33[32mNSP installed successfuly.\\33[0m \\x1B[3mMmm... Noodle Soup.\\x1B[0m\\n\")\n",
        "\n",
        "    # CLIP Interrogator\n",
        "    if INSTALL_CLIP_INTERROGATOR:\n",
        "\n",
        "        res = ''\n",
        "        os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "        print(\":hourglass_not_done: Installing CLIP Interrogator, and dependencies ...\")\n",
        "        if ['regex', 'tqdm', 'transformers', 'time', 'fairscale'] not in packages():\n",
        "            #!pip -q install regex\n",
        "            #!pip -q install tqdm\n",
        "            #!pip -q install transformers\n",
        "            #!pip -q install timm\n",
        "            #!pip -q install fairscale==0.4.4\n",
        "            res += subprocess.run(['pip', '-q', 'install', 'regex', 'tqdm', 'transformers', 'timm', 'fairscale==0.4.4'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        if 'clip' not in packages():\n",
        "            res += subprocess.run(['pip', '-q', 'install', 'git+https://github.com/openai/CLIP.git'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        if not os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/clip-interrogator'):\n",
        "            res += subprocess.run(['git', 'clone', '--quiet', 'https://github.com/pharmapsychotic/clip-interrogator.git'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        if not os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/BLIP'):\n",
        "            res += subprocess.run(['git', 'clone', '--quiet', 'https://github.com/salesforce/BLIP'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        if res.strip() is not '':\n",
        "            print(res)\n",
        "\n",
        "        sys.path.append(f'{STABLE_DIFFUSION_WORKDIR}/BLIP')\n",
        "        interrogator_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        interrogator_device = 'cpu'\n",
        "\n",
        "        import clip\n",
        "        import pandas as pd\n",
        "        import requests\n",
        "        import torch\n",
        "        import torchvision.transforms as T\n",
        "        import torchvision.transforms.functional as TF\n",
        "\n",
        "        from IPython.display import display\n",
        "        from PIL import Image\n",
        "        from torch import nn\n",
        "        from torch.nn import functional as F\n",
        "        from torchvision import transforms\n",
        "        from torchvision.transforms.functional import InterpolationMode\n",
        "            \n",
        "        from BLIP.models.blip import blip_decoder\n",
        "        \n",
        "        #%cd /content/BLIP\n",
        "\n",
        "        os.environ['TF_FP16_MATMUL_USE_FP32_COMPUTE']='1'\n",
        "        #os.environ['TF_FP16_MATMUL_USE_FP32_COMPUTE=1']='1'\n",
        "\n",
        "        def generate_caption(pil_image):\n",
        "            gpu_image = transforms.Compose([\n",
        "                transforms.Resize((blip_image_eval_size, blip_image_eval_size), interpolation=InterpolationMode.BICUBIC),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "            ])(pil_image).unsqueeze(0).to(interrogator_device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                caption = blip_model.generate(gpu_image, sample=False, num_beams=3, max_length=20, min_length=5)\n",
        "            del gpu_image\n",
        "            return caption[0]\n",
        "\n",
        "        def load_list(filename):\n",
        "            with open(filename, 'r', encoding='utf-8', errors='replace') as f:\n",
        "                items = [line.strip() for line in f.readlines()]\n",
        "            return items\n",
        "\n",
        "        def rank(model, image_features, text_array, top_count=1):\n",
        "            top_count = min(top_count, len(text_array))\n",
        "            text_tokens = clip.tokenize([text for text in text_array]).to(interrogator_device)\n",
        "            with torch.no_grad():\n",
        "                text_features = model.encode_text(text_tokens).float()\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            similarity = torch.zeros((1, len(text_array))).to(interrogator_device)\n",
        "            for i in range(image_features.shape[0]):\n",
        "                similarity += (100.0 * image_features[i].unsqueeze(0) @ text_features.T).softmax(dim=-1)\n",
        "            similarity /= image_features.shape[0]\n",
        "\n",
        "            top_probs, top_labels = similarity.cpu().topk(top_count, dim=-1)  \n",
        "            \n",
        "            del similarity\n",
        "\n",
        "            return [(text_array[top_labels[0][i].numpy()], (top_probs[0][i].numpy()*100)) for i in range(top_count)]\n",
        "\n",
        "        def interrogate(image, models):\n",
        "\n",
        "            from  torch.cuda.amp import autocast\n",
        "\n",
        "            caption = generate_caption(image)\n",
        "            if len(models) == 0:\n",
        "                print(f\"\\n\\n{caption}\")\n",
        "                return\n",
        "\n",
        "            table = []\n",
        "            bests = [[('',0)]]*5\n",
        "\n",
        "            print('\\n\\033[1mCLIP Interrogator:\\033[0m\\n')\n",
        "\n",
        "            with autocast():\n",
        "\n",
        "                for model_name in models:\n",
        "                    print(f\":magnifying_glass_tilted_right: Interrogating with {model_name}...\")\n",
        "                    model, preprocess = clip.load(model_name)\n",
        "                    model.to(interrogator_device).eval()\n",
        "\n",
        "                    images = preprocess(image).unsqueeze(0).to(interrogator_device)\n",
        "                    with torch.no_grad():\n",
        "                        image_features = model.encode_image(images).float32()\n",
        "                    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "                    ranks = [\n",
        "                        rank(model, image_features, mediums),\n",
        "                        rank(model, image_features, [\"by \"+artist for artist in artists]),\n",
        "                        rank(model, image_features, trending_list),\n",
        "                        rank(model, image_features, movements),\n",
        "                        rank(model, image_features, flavors, top_count=3)\n",
        "                    ]\n",
        "\n",
        "                    for i in range(len(ranks)):\n",
        "                        confidence_sum = 0\n",
        "                        for ci in range(len(ranks[i])):\n",
        "                            confidence_sum += ranks[i][ci][1]\n",
        "                        if confidence_sum > sum(bests[i][t][1] for t in range(len(bests[i]))):\n",
        "                            bests[i] = ranks[i]\n",
        "\n",
        "                    row = [model_name]\n",
        "                    for r in ranks:\n",
        "                        row.append(', '.join([f\"{x[0]} ({x[1]:0.1f}%)\" for x in r]))\n",
        "\n",
        "                    table.append(row)\n",
        "\n",
        "                    clean_env()\n",
        "\n",
        "            display(pd.DataFrame(table, columns=[\"Model\", \"Medium\", \"Artist\", \"Trending\", \"Movement\", \"Flavors\"]))\n",
        "\n",
        "            flaves = ', '.join([f\"{x[0]}\" for x in bests[4]])\n",
        "            medium = bests[0][0][0]\n",
        "            interrogator_prompt = ''\n",
        "            if caption.startswith(medium):\n",
        "                interrogator_prompt = f\"{caption} {bests[1][0][0]}, {bests[2][0][0]}, {bests[3][0][0]}, {flaves}\"\n",
        "            else:\n",
        "                interrogator_prompt = f\"{caption}, {medium} {bests[1][0][0]}, {bests[2][0][0]}, {bests[3][0][0]}, {flaves}\"\n",
        "\n",
        "            globals().update({'INTERROGATOR_PROMPT': interrogator_prompt})\n",
        "\n",
        "            print('\\033[0m:black_nib: Prompt:')\n",
        "            print(f'{interrogator_prompt}\\033[1m\\n\\n')\n",
        "\n",
        "            del table, image_features, model, preprocess\n",
        "            clean_env()\n",
        "\n",
        "        def do_interrogate(image, show_thumb=False):\n",
        "\n",
        "            os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/BLIP')\n",
        "\n",
        "            blip_image_eval_size = 384\n",
        "            blip_model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model*_base_caption.pth'        \n",
        "            blip_model = blip_decoder(pretrained=blip_model_url, image_size=blip_image_eval_size, vit='base')\n",
        "            blip_model.eval()\n",
        "            blip_model = blip_model.to(interrogator_device)\n",
        "\n",
        "            globals().update({'blip_model': blip_model, 'blip_image_eval_size': blip_image_eval_size})\n",
        "\n",
        "            models = []\n",
        "            if ViTB32: models.append('ViT-B/32')\n",
        "            if ViTB16: models.append('ViT-B/16')\n",
        "            if ViTL14: models.append('ViT-L/14')\n",
        "            if ViTL14_336px: models.append('ViT-L/14@336px')\n",
        "            if RN101: models.append('RN101')\n",
        "            if RN50: models.append('RN50')\n",
        "            if RN50x4: models.append('RN50x4')\n",
        "            if RN50x16: models.append('RN50x16')\n",
        "            if RN50x64: models.append('RN50x64')\n",
        "\n",
        "            if show_thumb:\n",
        "                thumb = image.copy()\n",
        "                thumb.thumbnail([blip_image_eval_size, blip_image_eval_size])\n",
        "                display(thumb)\n",
        "                thumb.close()\n",
        "                del thumb\n",
        "\n",
        "            interrogate(image, models=models)\n",
        "\n",
        "            del blip_model\n",
        "            clean_env()\n",
        "\n",
        "            os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "\n",
        "        data_path = f\"{STABLE_DIFFUSION_WORKDIR}/clip-interrogator/data/\"\n",
        "\n",
        "        artists = load_list(os.path.join(data_path, 'artists.txt'))\n",
        "        flavors = load_list(os.path.join(data_path, 'flavors.txt'))\n",
        "        mediums = load_list(os.path.join(data_path, 'mediums.txt'))\n",
        "        movements = load_list(os.path.join(data_path, 'movements.txt'))\n",
        "\n",
        "        sites = ['Artstation', 'behance', 'cg society', 'cgsociety', 'deviantart', 'dribble', 'flickr', 'instagram', 'pexels', 'pinterest', 'pixabay', 'pixiv', 'polycount', 'reddit', 'shutterstock', 'tumblr', 'unsplash', 'zbrush central']\n",
        "        trending_list = [site for site in sites]\n",
        "        trending_list.extend([\"trending on \"+site for site in sites])\n",
        "        trending_list.extend([\"featured on \"+site for site in sites])\n",
        "        trending_list.extend([site+\" contest winner\" for site in sites])\n",
        "\n",
        "        print(\":check_mark_button: CLIP Interrogator Installed!\")\n",
        "\n",
        "    # Final Imports and VARs\n",
        "\n",
        "    from huggingface_hub import hf_hub_download\n",
        "\n",
        "    from PIL import Image, ImageFilter\n",
        "    from io import BytesIO\n",
        "    import random, pprint, requests\n",
        "    from contextlib import contextmanager, nullcontext\n",
        "    from torch import autocast\n",
        "    try:\n",
        "        from diffusers import (\n",
        "            PNDMScheduler, \n",
        "            LMSDiscreteScheduler, \n",
        "            DDIMScheduler, \n",
        "            DDPMScheduler, \n",
        "            EulerAncestralDiscreteScheduler, \n",
        "            EulerDiscreteScheduler, \n",
        "            #DPMSolverMultistepScheduler\n",
        "        )\n",
        "        from diffusers import DiffusionPipeline\n",
        "    except ImportError:\n",
        "        pass\n",
        "    from IPython.display import clear_output, display, HTML, Markdown\n",
        "    import numpy as np\n",
        "\n",
        "    # Conceptualizer Imports (Clean up later)\n",
        "    #import argparse\n",
        "    #import itertools\n",
        "    import math\n",
        "    #import torch.nn.functional as F\n",
        "    #import torch.utils.checkpoint\n",
        "    #from torch.utils.data import Dataset\n",
        "\n",
        "    import PIL\n",
        "    from diffusers import AutoencoderKL, UNet2DConditionModel\n",
        "    from diffusers.hub_utils import init_git_repo, push_to_hub\n",
        "    from diffusers.optimization import get_scheduler\n",
        "\n",
        "    from torchvision import transforms\n",
        "    from tqdm.auto import tqdm\n",
        "    from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "    # Styles\n",
        "    style = {\n",
        "        \"Anime (Japanese Animation Inspired)\": \"Kodomo Style, MOE Anime Style, Ecchi, highly detailed, shadows and highlights, Vibrant Color Scheme, trending on ArtStation, Pixiv --Watermark\",\n",
        "        \"Cartoon (Matt Groening)\": \"Cartoon, American Animation, The Simpsons Art Style, in the Style of Matt Groening and Chris (Simpsons artist), Flat Colors, Lined Cartoon Art, High Resolution, High Quality, Gracie Films --Watermark, border, frame, image compression\",\n",
        "        \"Cartoon (Seth McFarlane)\": \"Cartoon, American Animation, Family Guy ARt Style, in the Style of Seth MacFarlane and Butch Hartman, Flat Colors, Lined Cartoon Art, High Resolution, High Quality, Underdog Productions Animation, Fuzzy Door Productions Animation --Watermark, border, frame, image compression\",\n",
        "        \"Cosmic (Space Art Style)\": \"Science Fiction, Scifi Theme, Cyberpunk, Outer Space, Deep Space, Cosmic Style, Starfield, Nebulas and distant galeies, Astro, Digital Illustration, In the Style of Gabriel BjÃ¶rk StiernstrÃ¶m and John Berkey --Watermark, image compression, film grain, noise\",\n",
        "        \"Cyberpunk\": \"Cyberpunk, Outer Space, Hyper Realistic, ArtStation, CGSociety, Neon lights, Cinematic Lighting, Volumetric Lighting, Realistic Surrealism, Style of Thomas Kinkade and Soufiane Idrassi --Watermark\",\n",
        "        \"Dragan (Andrzej Dragan Inspired)\": \"photorealistic color scheme, digital photography, dragan effect, dragan style, high contrast detail, dirty, gritty, urban, color photo --Watermark\",\n",
        "        \"Dystopian (Bleak)\": \"Photorealistic, Highly Detailed Illustration, Beautiful Aesthetic, Digitial Painting, Dystopian, Moody Atmosphere, Bleak Looking, Eartly, Terrestrial Society, Natural Color Scheme, Hopeless World, Earthborn, Salvaged Materials, Recycled World, Volumetric Lighting, Cinematic, by Ilya Kuvshinov and Aaron Jasinski --Watermark, Border, Frame, Noise, Bloody Skin\",\n",
        "        \"Exopunk (Extreterrestrial Cyberpunk Inspired)\": \"Extraterrestrial Exopunk, Psychedelic Zaha Hadid, Outer Space, Exoplanet, Alien Technology, Colored Lights, Volumetric Lighting, Cinematic Lighting, Atmospheric, Surrealism, Style of Dangiuz and Soufiane Idrassi --Watermark, Noise, Compression\",\n",
        "        \"Futuristic Scifi (Science Fiction Inspired)\": \"Photorealistic, Highly Detailed Illustration, Digitial Painting, Science Fiction, Scifi, Advanced Technology, Intricately Designed, Machinery, AI Artificial Intelligence, Androids, Deep Space, Energy Shields, Z-Space, Post-Human, Quantum, Utopian, Volumetric Lighting, Cinematic, by Ilya Kuvshinov and Aaron Jasinski --Watermark, Border, Frame, Noise\",\n",
        "        \"Gallic (Inspired by Celtic, Gallic, and Gaulish Cultures)\": \"Photorealistic, Digital Art, High Qualtiy, Dark Color Schemes, Gaulish and Celtic Theme, Murky and Atmospheric, Cinematic Lighting, Volumetrics, God Rays, Light Shafts, Particles and Dust in the Air, Elaborate Gallic Designs --Watermark, Border, Frame\",\n",
        "        \"Gigercraft (H.R. Giger and H.P. Lovecraft Inspired)\": \"Sience Fiction, Scifi Theme, Neutral Color Scheme, Organic Growth, Segmentation, Glistening, Wet Surfaces, Dark Atmosphere, Cinematic Lighting, Volumetric Lighting, Extraterrestrial, in the Style of H.R. Giger and H. P. Lovecraft and Dariusz Zawadzki -- Watermark, Image Compression, Film Grain, Noise\",\n",
        "        \"Ink & Watercolor (Zhang QuanZong Inspired)\": \"Photorealistic, Hyperrealism, Highly Detailed, Shaded Colors, Poetic Painting, Ink and Watercolor Influence, Ink and Watercolour, by Zhang QuanZong and Jing Hao (Hongguzi), High Quality, HD, Ornate and Elaborate Inkwork, Vibrant Colors --Watermark, Image Compression, Noise, Western art\",\n",
        "        \"Macabre (Midjourney Inspired)\": \"dark color scheme, grunge macabre aesthetic smudging style dark atmosphere bokeh evil painting --Watermark\",\n",
        "        \"Medieval\": \"Photorealistic, Medieval Theme, Dark nature aesthetic, Atmospheric Lighting, Ambient Lighting, Volumetric Lighting, lightly smokey air, Archaic, Gothic Architecture, Feudal Theme, Anglo-Saxon Theme --Watermark, Image Compression, Noise, Film Graine\",\n",
        "        \"Modern Religious (Christian Art Inspired)\": \"modern, highly detailed, elaborate, prestine clarity holy aesthetic digital painting heavenly atmosphere bokeh ethereal painting divine hazey --Watermark\",\n",
        "        \"Oil (Impressionist)\": \"Oil Painting, Brush Strokes, Canvas Texture, Textured Paint, Range of Color, Oil Canvas Style, Grainy Brush Strokes, Large Brush Strokes, Impressionist --Watermark\",\n",
        "        \"Oil (Naturalist)\": \"Naturalistic Style, Realistic Oil Painting, Highly Detailed, Realism,  Fine Art, Chiaroscuro Style, Campitura --Watermark\",\n",
        "        \"Organic Ornate (Elaborate Decorative Style)\": \"Photorealistic, 3D Matte, by ellen jewett, tomasz alen kopera and Justin Gerard, symmetrical features, ominous, solemn, magical realism, texture, intricate, ornate, royally decorated, Halo, Gilding, Gilded, whirling smoke, particles, gold adornements, white splendid fabric, radiant colors, artstation, volumetric lighting, micro details, 3d sculpture, ray tracing --Watermark, Picture Frame\",\n",
        "        \"Pen & Pencil\": \"hyperrealistic sketch, high relief sketch, detailed lines, pencil lines, realism, shading lines, pen on paper, well defined --Watermark\",\n",
        "        \"Photorealistic\": \"photograph, realistic, photorealistic, real life, photography, bokeh, lens attenuation, chromatic aberration, realistic color scheme, by Getty Images --Watermark, Brushwork, Style of Drawing, Style of Painting\",\n",
        "        \"Post-Apocalyptic (Wasteland-like Inspired)\": \"Post-Apocalyptic, Overgrown World, Wasteland, Ruins and Debris, Naturalist Color Scheme, Volumetric Lighting, Cinematic Lighting, Atmospheric, Style of James Chadderton, and Diego Matiz --Watermark\",\n",
        "        \"Pop Art (High Contrast Color Mixing)\": \"Pop Art, Vivid Color Scheme, Stylized Color, Abstract Brushwork, Digital Painting, Pop culture, Surreal, Highly Detailed, Punky, Splat, Pow, Bam --Watermark\",\n",
        "        \"Prismatic Universe (Vivid Rainbow Colors)\": \"Photorealistic, Digital Painting, Quantum Universe, Quantum Energy, Made out of Prismatic Crystals, Chromatic Aberration, Prism Colors, Emitting Energy, Celestial, Etherreal, by Ilya Kuvshinov and Ellen Jewett, Prism Color Scheme, Volumetric Lighting, Cinematic, High Quality, High Resolution, Light Shafts, God Rays --Watermark, Image Compression, Noise, Frame, Border\",\n",
        "        \"Regal Imperial (Decorative Royal Accents)\": \"Imperial Regal Style, Gilded by Golden Wheat and Barley, Adorned in Gemstones, Gold and Silver Accent, 3D Matte, by Ilya Kuvshinov, Eve Ventrue, and Aaron Jasinski, ArtStation, CGSociety, elaborate detailed adornment, royal accent, regal features, atmospheric, cinematic, volumetric lighting, supple complexion --Watermark\",\n",
        "        \"Trippy (Psychedelic Art Inspired)\": \"Photorealistic Mandelbrot Set, Beautiful 3D Fractals, Fractalizations, 3d smooth Kaleidoscopes, Realistic Psychedelic Patterns --Watermark, Image Compression, Film Grain, Noise\",\n",
        "        \"Vivid Disco (Glamour in Dynamic Colors)\": \"Photorealistic, Vivid Disco Color Scheme, Color Mixing, High Contrast, Highly Detailed, Poppy, Lens Flares, Shine, Sparkle, Glitter, Style of Ilya Kuvshinov --Watermark, Compression, Noise\"\n",
        "    }\n",
        "\n",
        "    # Setup param scraper+ \n",
        "    scraper = paramScraper(settings_template, globals())   \n",
        "    scraper.scrape('setup')\n",
        "\n",
        "    last_pipe_type = None\n",
        "    last_model_type = None\n",
        "\n",
        "except OSError as e:\n",
        "    raise e\n",
        "except BaseException as e:\n",
        "    raise e\n",
        "finally:\n",
        "    if CLEAR_SETUP_LOG: clear()\n",
        "    from IPython.display import display, HTML\n",
        "    display(HTML('''\n",
        "        <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.6.1/jquery.min.js\"></script>\n",
        "        <script>\n",
        "            $(function() {\n",
        "            var p = $(\".icons-anim\");\n",
        "            for(var i=0; i<3; i++) {\n",
        "                p.animate({opacity: 0.5}, 500, 'linear')\n",
        "                .animate({opacity: 1}, 500, 'linear');\n",
        "            }\n",
        "            });\n",
        "        </script>\n",
        "        <p align=\"center\" style=\"font-size:24px;text-shadow: 2px 2px 3px rgba(0,0,0,0.5);\"><span class=\"icons-anim\">ðŸŽŠðŸŽ‰</span> <strong>Easy Diffusion</strong> Setup <span style=\"color:green;\">Complete</font> <span class=\"icons-anim\">ðŸŽ‰ðŸŽŠ</span></p>\n",
        "    '''))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Easy Diffusion"
      ],
      "metadata": {
        "id": "CQHBZ8TMcmEE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ucr5_i21xSjv",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import hexlify\n",
        "#@title <a name=\"settingsdiffuse\"><font size=\"5\" color=\"#e8cf53\">**Settings & Diffuse**</font></a>\n",
        "\n",
        "#@markdown ### <a name=\"exportimport\"><font color=\"#e8cf53\">**Export / Import Settings**</a>\n",
        "SAVE_SETTINGS_FILE = True #@param {type:\"boolean\"}\n",
        "#@markdown <font size=\"3\">Save a settings file based on the supplied settings.</font>\n",
        "LOAD_SETTINGS_FILE = '' #@param{type:'string'}\n",
        "#@markdown <font size=\"3\">Load a Easy Diffusion Settings file to ***bypass*** the settings below. Will use the file settings to perform a run.\n",
        "\n",
        "#@markdown ---\n",
        "#markdown <br>\n",
        "\n",
        "#@markdown ### <a name=\"promptsetup\"><font color=\"#e8cf53\">**Prompt Setup**</a>\n",
        "#@markdown <font size=\"3\">Prompts Support: [Noodle Soup Prompts](https://github.com/WASasquatch/noodle-soup-prompts/wiki/Terminology-Reference) \\([NSP Prompt Generator](https://rebrand.ly/noodle-soup-prompts)\\) | Prompt Weights, Ex: `A ((cat)) on the moon` or ` A (cat:1.1) on the moon`</font><br>\n",
        "#@markdown <font size=\"3\">You can split your prompt into positive and negative by using `--` Example: `positive prompt here --negative prompt here`\n",
        "PROMPT = \"A stylish beautiful 3d render portrait of a _noun-emote_ cat in a _color_ space helmet on the moon --dog\" #@param {type:'string'}\n",
        "PROMPT_FILE = '' #@param {type: 'string'}\n",
        "#@markdown <font size=\"3\">`PROMPT_FILE` is a optional text file that contains a prompt ***per*** line. If you use a regular `PROMPT` as well, it will be added as the first prompt in series.</font>\n",
        "PROMPT_STYLE = 'None' #@param['None', 'Anime (Japanese Animation Inspired)', 'Cartoon (Matt Groening)', 'Cartoon (Seth McFarlane)', 'Cosmic (Space Art Style)', 'Cyberpunk', 'Dragan (Andrzej Dragan Inspired)', 'Dystopian (Bleak)', 'Exopunk (Extreterrestrial Cyberpunk Inspired)', 'Futuristic Scifi (Science Fiction Inspired)', 'Gallic (Inspired by Celtic, Gallic, and Gaulish Cultures)', 'Gigercraft (H.R. Giger and H.P. Lovecraft Inspired)', 'Ink & Watercolor (Zhang QuanZong Inspired)', 'Macabre (Midjourney Inspired)', 'Medieval (Medieval Theme Inspired)', 'Modern Religious (Christian Art Inspired)', 'Oil (Impressionist)', 'Oil (Naturalist)', 'Organic Ornate (Elaborate Decorative Style)', 'Pen & Pencil', 'Photorealistic', 'Post-Apocalyptic (Wasteland-like Inspired)', 'Pop Art (High Contrast Color Mixing)', 'Prismatic Universe (Vivid Rainbow Colors)', 'Regal Imperial (Decorative Royal Accents)', 'Trippy (Psychedelic Art Inspired)', 'Vivid Disco (Glamour in Dynamic Colors)']\n",
        "#@markdown <font size=\"3\">Apply a style to your prompt. Focus on the prompt, not the style!\n",
        "NEW_NSP_ON_ITERATION = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Whether to generate NSP once, or on each iteration. Check this if you want each iteration to have a freshly cooked noodle prompt.</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown ### <a name=\"initsetup\"><font color=\"#e8cf53\">**Init Image Setup**</font></a>\n",
        "INIT_IMAGE = \"\" #@param {type: 'string'}\n",
        "INIT_MASK = \"\" #@param {type: 'string'}\n",
        "#@markdown <font size=\"3\">`INIT_IMAGE` and `INIT_MASK` accepts the following formats</font>\n",
        "#@markdown - <font size=\"3\">A single local, or remote image</font>\n",
        "#@markdown - <font size=\"3\">A `.txt` file containing a single local, or remote image ***per*** line.</font>\n",
        "#@markdown - <font size=\"3\">A path to a local folder containing images.</font>\n",
        "\n",
        "#@markdown <font size=\"3\">**Note:** You can use a `PROMPT_FILE` with `INIT_IMAGE`. If you have more images than prompts, it will use the last prompt for all remaining `INIT_IMAGE`. Additionally, if there are more `INIT_IMAGE`'s then `INIT_MASK` the last `INIT_MASK` will be used for the remaining `INIT_IAMGE`</font> \n",
        "INIT_FILTERS = '' #@param{type: 'string'}\n",
        "#@markdown <font size=\"3\">Filter init images in a file or folder by these filters seperated by a comma. Ex: `nature,outdoors,travel`\n",
        "INIT_SCALE = 0.55 #@param{type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown <font size=\"3\">**Note:** Scale of init image from 0 to 1. Lower values adhear more to the image.</font>\n",
        "USE_INIT_IMAGE_SIZE = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Use image dimensions as `WIDTH` and `HEIGHT`</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown #### <a name=\"recursiveevo\"><font color=\"#e8cf53\">Recursive Evolution Settings</font></a>\n",
        "RECURSIVE_EVOLUTION = False #@param{type:'boolean'}\n",
        "#@markdown <font size=\"3\">Automatically start a img2img diffusion run with the diffusion result from the previous result. Will run for `NUM_ITERS` iterations. Is compatible with `INIT_IMAGE` starting point, and `PROMPT_FILE` batches.</font>\n",
        "RECURSIVE_HYBRID_EVO = False #@param{type:'boolean'}\n",
        "#@markdown <font size=\"3\">Apply the previous frame as an overlay for the `INIT_IMAGE` for the current run. Can help stabilize, but may introduce blurring if overlay strength not adjusted properly.</font>\n",
        "RECURSIVE_HYBRID_OVERLAY = 0.5 #@param{type:'slider', min:0.01, max:1.0, step:0.01}\n",
        "#@markdown <font size=\"3\">Recursive hybrid overlay amount</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown ### <a name=\"diffusionsettings\"><font color=\"#e8cf53\">**Diffusion Settings**</font></a><br><br>\n",
        "#@markdown #### <a name=\"diffusionmodel\"><font size=\"4\" color=\"#e8cf53\">**Diffusion Model**:</font></a>\n",
        "MODEL_ID = 'runwayml/stable-diffusion-v1-5' #@param [\"runwayml/stable-diffusion-v1-5\", \"runwayml/stable-diffusion-inpainting\", \"CompVis/stable-diffusion-v1-4-original\", \"CompVis/stable-diffusion-v1-3\",\"CompVis/stable-diffusion-v1-2\",\"CompVis/stable-diffusion-v1-1\",\"hakurei/waifu-diffusion\",\"nitrosocke/redshift-diffusion\",\"lambdalabs/sd-pokemon-diffusers\",\"doohickey/trinart-waifu-diffusion-50-50\",\"spav/nilou-waifu-diffusion\",\"valhalla/sd-wikiart-v2\",\"rrustom/stable-architecture-diffusers\",\"AstraliteHeart/pony-diffusion\",\"nitrosocke/redshift-diffusion\",\"nitrosocke/Arcane-Diffusion\",\"prompthero/openjourney\", \"nitrosocke/Nitro-Diffusion\", \"\"]{allow-input: true}\n",
        "#@markdown <font size=\"3\">Use the drop-down arrow to select a model, or enter in a custom model from Hugging Face.</font><br>\n",
        "#@markdown <font size=\"3\">The model `nitrosocke/redshift-diffusion` does not support `LOW_VRAM_PATCH` mode.</font><br>\n",
        "#@markdown <font size=\"3\">If using `CACHE_PIPELINES` you will need to run `RECACHE_PIPES` once when switching `MODEL_ID`. Allows custom input (for HF models not listed)</font>\n",
        "REDOWNLOAD_MODEL = False #@param{type: 'boolean'}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### <a name=\"conceptembed\"><font size=\"4\" color=\"#e8cf53\">**Concept Embedding**</font></a>\n",
        "CONCEPT_ID = '' #@param{type: 'string'}\n",
        "#@markdown <font size=\"3\">The [Hugging Face Concept](https://huggingface.co/sd-concepts-library) ID or a URL to a remote concept `.bin` file.<br>\n",
        "#@markdown - Example ID: `sd-concepts-library/cat-toy`\n",
        "#@markdown - Exmaplme URL: `https://huggingface.co/sd-concepts-library/cat-toy/resolve/main/learned_embeds.bin` (<font color=\"orange\">WARNING</font> Currently broken)</font>\n",
        "CONCEPT_TOKEN = '' #@param{type: 'string'}\n",
        "#@markdown <font size=\"3\">Define a custom token, or set the token for a custom concept downloaded from a URL.</font>\n",
        "\n",
        "#@markdown ---\n",
        "SAMPLER = 'Euler' #@param [\"DEFAULT\", \"PNDM\", \"LMS\", \"DDIM\", \"Euler_A\", \"Euler\"]\n",
        "DDIM_ETA = 0.65 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown <font size=\"3\">`DDIM_ETA` only applies to the DDIM sampler.</font>\n",
        "STEPS = 10 #@param {type:\"slider\", min:5, max:500, step:5} \n",
        "#@markdown <font size=\"3\">Diffusion steps determines the quality of the final image</font>\n",
        "SEED = 0 #@param {type:'raw'}\n",
        "#@markdown <font size=\"3\">The seed used for the generation. System max value is `9999999999999999`. Leave at `0` for random. You can also enter text encapulated by semi-quotes such as: `'RockyCanyon'`</font>\n",
        "MAX_SEED = 'system_max' #@param{type:'raw'}\n",
        "#@markdown <font size=\"3\">Use `'system_max'` (with semi-quotes) for the maximum seed `int`, or define the max random int size (maximum integer length of 16 digits).\n",
        "INCREMENT_ITERATION_SEED = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Increment seed on each iteration.</font>\n",
        "NUM_ITERS = 5 #@param {type:\"slider\", min:1, max:1000, step:1} \n",
        "#@markdown <font size=\"3\">Number of iterations for a given prompt or init image.</font>\n",
        "WIDTH = 512 #@param {type:\"slider\", min:256, max:4096, step:64}\n",
        "HEIGHT = 768 #@param {type:\"slider\", min:256, max:4096, step:64}\n",
        "SCALE = 13.5 #@param {type:\"slider\", min:0, max:25, step:0.1}\n",
        "#@markdown <font size=\"3\">The CFG `SCALE` determines how closely a generation follows the prompt, or improvisation. Lower values will try to adhear to your prompt.</font>\n",
        "PRECISION = \"full\" #@param [\"full\",\"autocast\"]\n",
        "#@markdown <font size=\"3\">If you're using the `LOW_VRAM_PATCH` you <b>must</b> use `autocast`</font><br>\n",
        "IMAGES_FOLDER = \"time_to_stabilize\" #@param {type: 'string'}\n",
        "#@markdown <font size=\"3\">Define a custom folder to saves images within your `images_out` folder. Example: `CAR_CONCEPTS`</font>\n",
        "#@markdown <font size=\"3\">**Note:** Path: `/content/Stable_Diffusion/images_out` or with Google Drive `/content/drive/MyDrive/AI/Stable_Diffusion/images_out`</font>\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown ### <a name=\"upscalers\"><font color=\"#e8cf53\">**General Upscaling Settings**</font></a>\n",
        "#@markdown <font size=\"3\">`IMAGE_UPSCALER`: may not work at resolutions above 512x768/768x512 on GPUs with ~16GB VRAM. Try using ESRGAN in CPU Mode if you're having issues.<br>**Note:** GFPGAN/CodeFormer is good for faces only.</font>\n",
        "IMAGE_UPSCALER = \"None\" #@param [\"None\",\"GOBIG\",\"IMG2IMG\",\"GFPGAN\",\"Enhanced Real-ESRGAN\", \"GFPGAN + Enhanced ESRGAN\", \"CodeFormer\", \"CodeFormer + Enhanced ESRGAN\"]\n",
        "UPSCALE_AMOUNT = 2 #@param {type:\"slider\", min:2, max:8, step:2}\n",
        "ESRGAN_MODE = 'CUDA' #@param ['CUDA', 'CPU']\n",
        "#@markdown <font size=\"3\">Real-ESRGAN Device Mode. CUDA is GPU.\n",
        "SCALING_SAMPLER = 'LANCZOS' #@param['LANCZOS', 'BILINEAR', 'BICUBIC', 'BOX', 'HAMMING']\n",
        "#@markdown <font size=\"3\">Sampler applies to GOBIG scaling as well.</font>\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #### <a name=\"upscalers-codeformer\"><font color=\"#e8cf53\">CodeFormer Upscale Settings</font></a>\n",
        "CODEFORMER_UPSCALE_AMOUNT = 1 #@param {type: 'number'}\n",
        "#@markdown <font size=\"3\">`CF_UPSCALE_AMOUNT` only applies to CodeFormer. Defined the upscale factor for CodeFormer.\n",
        "CODEFORMER_FIDELITY = 0.6 #@param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "#@markdown <font size=\"3\">`CODEFORMER_FIDELITY`: only applies to CodeFormer. Balance the quality (lower number) and fidelity (higher number)</font><br>\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #### <a name=\"upscalers-gobig\"><font color=\"#e8cf53\">GOBIG Upscale Settings</font></a>\n",
        "GOBIG_DETAIL_PROMPT = '' #@param{type: 'string'}\n",
        "#@markdown <font size=\"3\">An *optional* prompt that is used for slices during the GOBIG process.</font><br>\n",
        "#@markdown <font size=\"3\">Example: `High Quality Digital Art --watermark, logo, text, faces, people, person, low quality`</font>\n",
        "GOBIG_INIT_SCALE = 0.3 #@param{type:\"slider\", min:0, max:1, step:0.01}\n",
        "GOBIG_SLICE_OVERLAP = 85 #@param{type:'slider', min:0, max:256, step:1}\n",
        "GOBIG_MAXIMIZE = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Maximize extends the GOBIG canvas to use areas that would otherwise be wasted, resulting in a larger final image at no additional render time cost.</font>\n",
        "GOBIG_PRESCALED = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Use this if you are using `SKIP_DIFFUSION_RUN` and processing a image with GOBIG and would like to use it's dimennsions as the base resolution to upscale from.</font>\n",
        "#@markdown <font size=\"3\">**Note:** To process a image (non-diffusion result), use `SKIP_DIFFUSION_RUN` with a `INIT_IMAGE`.</font>\n",
        "GOBIG_REAL_ESRGAN = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Apply Real-ESRGAN Upscaling to GOBIG image.</font>\n",
        "GOBIG_SKIP_DIFFUSION_RUN = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Skip diffusion on `INIT_IMAGE` and slice it up as a GOBIG init image.</font>\n",
        "GOBIG_HYBRID_SLICE = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Do a hybrid run on each slice. See hybrid settings below.</font>\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #### <a name=\"upscalers-hybrid\"><font color=\"#e8cf53\">Hybrid Upscale Settings<font color=\"#e8cf53\"></font></a>\n",
        "HYBRID_UPSCALE = 'NONE' #@param['NONE', 'LANCZOS', 'BILINEAR', 'BICUBIC', 'BOX', 'HAMMING']\n",
        "#@markdown <font size=\"3\">Hybrid Upscaling upscales and overlays the original render over the upscaled version, helpful for mitigating the over-smoothness of Real-ESRGAN. This only works if you're using a `IMAGE_UPSCALER`</font>\n",
        "HYBRID_OVERLAY = 0.25 #@param{type:'slider', min:0.01, max:1.0, step:0.01}\n",
        "HYBRID_SUPER_RESOLUTION = False #@param{type:'boolean'}\n",
        "#@markdown <font size=\"3\">Apply Super Resolution to the hybrid upscaling.</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown ### <a name=\"imageprocessors\"><font color=\"#e8cf53\">**Image Processor Setup**</font></a>\n",
        "KEEP_ONLY_FINAL_IMAGE = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Delete original images after final upscaling.</font>\n",
        "SCALE_DOWN_ENHANCEMENTS_FOR_ESRGAN = True #@param{type:'boolean'}\n",
        "#@markdown <font size=\"3\">Scale down enhanced images. Useful if you are also using Real-ESRGAN. This will preserve your upscale factor for Real-ESRGAN after GFPGAN or CodeFormer.</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown #### <a name=\"sharpen\"><font color=\"#e8cf53\">Sharpen Image</font></a>\n",
        "#@markdown <font size=\"3\">Sharpen the base diffusion image before upscsaling.</font>\n",
        "SHARPEN_AMOUNT = 0 #@param{type:'slider', min:0, max:3, step:1}\n",
        "#@markdown <font size=\"3\">Sharpen iteration amount. `0` for no sharpen.</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown #### <a name=\"kromo\"><font color=\"#e8cf53\">Kromo Chromatic Aberration</font></a>\n",
        "CA_DIFFUSE_IMAGE = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Apply Chromatic Aberration to the base diffusion image (pre sharpen if enabled)</font>\n",
        "CA_STRENGTH = 0.1 #@param {type:\"slider\", min:0, max:5, step:0.1}\n",
        "#@markdown <font size=\"3\">Chromatic Aberration strength</font>\n",
        "CA_JITTER = 1 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "#@markdown <font size=\"3\">Chromatic Aberration set channel offset pixels</font>\n",
        "CA_OVERLAY = 0.1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "#@markdown <font size=\"3\">Alpha of original image overlay.</font>\n",
        "CA_NO_RADIAL_BLUR = True #@param{type: 'boolean'}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown #### <a name=\"median\"><font color=\"#e8cf53\">Median Filter Image</font></a>\n",
        "MEDIAN_FILTER_IMAGE = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Apply a Median Filter effect to the diffusion image. This can be tuned similar to a surface blur for reducing detail.</font>\n",
        "MEDIAN_DIAMETER = 2.5 #@param{type:'slider', min:0.1, max:100, step:0.1}\n",
        "#@markdown <font size=\"3\">Radius of the median filtering effect</font>\n",
        "MEDIAN_SIGMA_COLOR = 75 #@param{type: 'number'}\n",
        "#@markdown <font size=\"3\">Sigma Color filters sigma in the color space. A larger value means that farther colors within the pixel neighborhood will be mixed together, resulting in larger areas of semi-equal color.</font>\n",
        "MEDIAN_SIGMA_SPACE = 75 #@param{type: 'number'}\n",
        "#@markdown <font size=\"3\">Sigma Space silters the sigma in the coordinate space. A larger value means that farther pixels will influence each other as long as their colors are close enough.</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown #### <a name=\"midas\"><font color=\"#e8cf53\">MiDaS Depth Map</font></a>\n",
        "GENERATE_MIDAS_DEPTH = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Generate a MiDaS Depth Approximation from the diffusion result.</font>\n",
        "SAVE_MIDAS_DEPTH = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Save MiDaS depth approximation.</font>\n",
        "MIDAS_TYPE = \"DPT_Large\" #@param [\"DPT_Large\",\"DPT_Hybrid\",\"MiDaS_small\"]\n",
        "#@markdown <font size=\"3\">`MIDAS_TYPE` determines the model to use for depth approximation.</font>\n",
        "MIDAS_MODE = \"CPU\" #@param [\"CPU\",\"CUDA\"]\n",
        "#@markdown <font size=\"3\">**CPU Mode:** If you get: \"`RuntimeError: \"linspace_cpu\" not implemented for 'Half'`\" something has changed with CPU and you need to disconnect/reconnect (Google Colab)</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown #### <a name=\"fdof\"><font color=\"#e8cf53\">Fake Depth of Field Filter</font></a>\n",
        "FDOF_IMAGE = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Apply Fake Depth of Field to the image based on the MiDaS Depth Map</font>\n",
        "FDOF_REPLACE_IMAGE = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Replace original diffusion image saved to disk with FDOF image. This will also pipe the image through for upscalers.</font>\n",
        "FDOF_RADIUS = 9 #@param {type:'slider', min: 0.1, max:100, step:0.1}\n",
        "#@markdown <font size=\"3\">Depth of Field Blur Radius\n",
        "FDOF_SAMPLES = 1 #@param{type:'slider', min:1, max:5, step:1}\n",
        "#@markdown <font size=\"3\">Resample the image with DOF (can create a stronger DOF effect based on the MiDas Depth Map)</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown #### <a name=\"tileable\"><font color=\"#e8cf53\">Tilable Seamless Image</font></a>\n",
        "TILEABLE_IMAGE = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Process the image as a seamless tileable texture.</font>\n",
        "TILED = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Tile the image 2x2 (Ex a 512x512 image would be tiled to 1024x1024)</font>\n",
        "TILE_OVERLAP = 0.1 #@param{type:'slider', min:0.01, max:1.0, step:0.01}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown #### <a name=\"clipinterrogator\"><font color=\"#e8cf53\">CLIP Interrogator</font></a>\n",
        "#@markdown <font size=\"3\">CLIP Interrogator requires a substantial chunk of GPU VRAM, and may not be suited for low VRAM cards while running diffusions. Use at your own risk.<br />Using this for init_images with `SKIP_DIFFUSION_RUN` enabled allows you to batch interrogate images to scrape prompt ideas.</font>\n",
        "INTERROGATE_INIT_IMAGE = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Interrogate `INIT_IMAGE`</font>\n",
        "INTERROGATE_DIFFUSION_IMAGE = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Interrogate diffusion result after generation.</font>\n",
        "\n",
        "#@markdown <font color=\"#e8cf53\">Interrogator CLIP Models</font>\n",
        "ViTB32 = False #@param{type:\"boolean\"}\n",
        "ViTB16 = False #@param{type:\"boolean\"}\n",
        "ViTL14 = True #@param{type:\"boolean\"}\n",
        "ViTL14_336px = False #@param{type:\"boolean\"}\n",
        "RN101 = False #@param{type:\"boolean\"}\n",
        "RN50 = False #@param{type:\"boolean\"}\n",
        "RN50x4 = False #@param{type:\"boolean\"}\n",
        "RN50x16 = False #@param{type:\"boolean\"}\n",
        "RN50x64 = False #@param{type:\"boolean\"}\n",
        "#@markdown <font size=\"3\">These models are only relevant if you're interrogating init images or diffusion results with CLIP Interrogator.</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown #### <a name=\"cachepipes\"><font color=\"#e8cf53\">**Pipeline Flags**</font></a>\n",
        "#@markdown <font size=\"3\">Cached pipes store the loaded model in memory, as well as the configured Stable Diffusion pipeline to a object stored on disk. These files can be large. The benefit is once pipes are cached, switching between them takes less than 4 seconds. By contrast, switching pipes from memory in the vanillas Stable Diffusion can take up to 38 seconds. That means just doing 4 images spread across txt2img, and img2img, cached pipes could save you over a minute in time!</font>\n",
        "CACHE_PIPELINES = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Whether to cache pipes to disk and load on demand.</font>\n",
        "RECACHE_PIPES = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Recache pipelines. Required if switching diffusion models, or upgrading the pipe to a new version of diffusers.</font>\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown <font size=\"3\">**NOTE:** If you're having trouble loading pipes to start diffusions, check this and run this cell again.</font><br>\n",
        "SKIP_DIFFUSION_RUN = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Skip diffusion run ***If*** `INIT_IMAGE` ***is defined*** to process images.</font>\n",
        "ENABLE_NSFW_FILTER = False #@param {type:\"boolean\"}\n",
        "#@markdown <font size=\"3\">`ENABLE_NSFW_FILTER`: Will return a blurred image for content flagged as NSFW</font>\n",
        "ENABLE_ATTENTION_SLICES = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Enable attention slices to better utilization of available memory at the cost of diffusion speed.</font>\n",
        "LOW_VRAM_PATCH = True #@param {type:\"boolean\"}\n",
        "#@markdown <font size=\"3\">You may need this if you're using a GPU with ~16GB VRAM. **Note:** This appplies to non-cached pipes.</font>\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown <br>\n",
        "\n",
        "#@markdown ### <a name=\"othersettings\"><font color=\"#e8cf53\">**Console Output Settings**</font></a>\n",
        "IMAGES_DISPLAY_ABOVE_LOG = False #@param{type: 'boolean'}\n",
        "#@markdown Display organized JS Images above log output, not below.</font>\n",
        "USE_BASIC_IMAGE_DISPLAY = False #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Use basic image output instead of organized JS Image Output</font>\n",
        "CLEAR_LOG_BETWEEN_ITERATIONS = True #@param{type: 'boolean'}\n",
        "#@markdown <font size=\"3\">Clear the output log between each iteration. (Organized JS Mode Only)</font>\n",
        "MAX_JS_IMAGE_WIDTH = 515 #@param{type:'slider', min:128, max:768, step:32}\n",
        "#@markdown <font size=\"3\">The maximum width to display images with organized JS mode</font>\n",
        "\n",
        "\n",
        "if LOAD_SETTINGS_FILE.lower() not in [None, 'none', '']:\n",
        "    try:\n",
        "        settings = json.load(open(LOAD_SETTINGS_FILE))\n",
        "    except OSError as e:\n",
        "        raise e\n",
        "    print(f'\\n:information: Loading \\'{LOAD_SETTINGS_FILE}\\' settings file...')\n",
        "    for ss in settings.keys():\n",
        "        for sk in settings[ss].keys():\n",
        "            if globals().__contains__(sk):\n",
        "                globals()[sk] = settings[ss][sk]\n",
        "    print(':information: Settings loaded.\\n')\n",
        "\n",
        "clean_env()\n",
        "\n",
        "last_diffusion_filedir = None\n",
        "last_diffusion_hybrid_filedir = None\n",
        "GOBIG_SLICE_RUN = False\n",
        "\n",
        "download_model(MODEL_ID, REDOWNLOAD_MODEL)\n",
        "\n",
        "if not INSTALL_GOBIG_V2_SUPPORT and IMAGE_UPSCALER is 'GOBIG':\n",
        "    print(pil_warning+\"\\n\")\n",
        "    print(\":WARNING: \\33[31mGOBIG can't run if Pillow v9 is not installed. Disabling GOBIG!\\33[0m\")\n",
        "    IMAGE_UPSCALER = 'NONE'\n",
        "\n",
        "if IMAGE_UPSCALER is 'GOBIG' and NUM_ITERS > 1:\n",
        "    print(':warning: GOBIG is active, and ierations is above 1. Settings \\'NUM_ITERS\\' to 1')\n",
        "    NUM_ITERS = 1\n",
        "\n",
        "if CODEFORMER_UPSCALE_AMOUNT <= 0:\n",
        "    CODEFORMER_UPSCALE_AMOUNT = 1\n",
        "else:\n",
        "    CODEFORMER_UPSCALE_AMOUNT = closest_value([1,2,4,8],CODEFORMER_UPSCALE_AMOUNT)\n",
        "\n",
        "ESRGAN_MODE = ESRGAN_MODE.lower()\n",
        "\n",
        "if LOW_VRAM_PATCH and PRECISION is not 'autocast': \n",
        "    print(f\"PRECISION must be 'autocast' when running in low vram compatibility mode! Defaulting to autocast...\")\n",
        "    PRECISION = 'autocast'\n",
        "\n",
        "if CONCEPT_ID.lower().startswith('http') and CONCEPT_TOKEN.lower() in ['none','']:\n",
        "    raise ValueError(\"You have requested downloading a non Hugging Face custom concept bin, but have not provided a CONCEPT_TOKEN to use withn it.\")\n",
        "\n",
        "precision_scope = autocast if PRECISION is 'autocast' else nullcontext\n",
        "\n",
        "# Max Seed and Custom Seed Setup\n",
        "if MAX_SEED is 'system_max':\n",
        "    MAX_SEED = 9999999999999999\n",
        "else:\n",
        "    MAX_SEED = int(MAX_SEED)\n",
        "\n",
        "text_seed = None\n",
        "if type(SEED) is str:\n",
        "    text_seed = SEED\n",
        "    SEED = text2seed(SEED, 16)\n",
        "else:\n",
        "    SEED = int(SEED)\n",
        "\n",
        "ORIG_SEED = SEED\n",
        "\n",
        "os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "\n",
        "GDRIVE_OUT_PATH = f'{GDRIVE_WORKDIR}/images_out/{IMAGES_FOLDER}'\n",
        "if USE_DRIVE_FOR_PICS:\n",
        "    if not os.path.exists(GDRIVE_OUT_PATH):\n",
        "        os.makedirs(GDRIVE_OUT_PATH)\n",
        "    OUTDIR = GDRIVE_OUT_PATH\n",
        "else:\n",
        "    OUTDIR = f'{STABLE_DIFFUSION_WORKDIR}/images_out/{IMAGES_FOLDER}'\n",
        "\n",
        "# JavaScript Compatible Boolean\n",
        "if IMAGES_DISPLAY_ABOVE_LOG:\n",
        "    IMAGES_DISPLAY_ABOVE_LOG = 1\n",
        "else:\n",
        "    IMAGES_DISPLAY_ABOVE_LOG = 0\n",
        "\n",
        "print(f\":open_file_folder: Images Output Directory: {OUTDIR}\\n\")\n",
        "\n",
        "if RECURSIVE_EVOLUTION is True:\n",
        "    print(\":gear: Recursive Evolution is Enabled\")\n",
        "\n",
        "# Nearest value to UPSCALE_AMOUNT\n",
        "nearest_value = closest_value([2,4,8],UPSCALE_AMOUNT)\n",
        "\n",
        "scraper.updateGlobals(globals())\n",
        "scraper.scrape('upscalers')\n",
        "scraper.scrape('image_processing')\n",
        "scraper.scrape('other_settings')\n",
        "\n",
        "# Diffuse Function\n",
        "def diffuse_run():\n",
        "\n",
        "    clean_env()\n",
        "\n",
        "    global RECACHE_PIPES, CACHE_PIPELINES, PROMPT, PROMPT_STYLE, SEED, UPSCALE_AMOUNT, GENERATE_MIDAS_DEPTH, FDOF_IMAGE, RECURSIVE_EVOLUTION, last_diffusion_filedir, init, original_init, mask, original_mask, last_model_type, pipe_type, PIPE_CACHE, opts, CONCEPT_TOKEN, CONCEPT_ID, corrected_size, last_batch, last_diffusion_hybrid_filedir\n",
        "    if not CACHE_PIPELINES: global pipe\n",
        "    else: pipe = None\n",
        "\n",
        "    if ORIG_SEED is 0 and SEED is 0:\n",
        "        SEED = random.randint(0,MAX_SEED)\n",
        "    else:\n",
        "        if ( INCREMENT_ITERATION_SEED and iteration > 0 ) or ( INCREMENT_ITERATION_SEED and last_batch != i ):\n",
        "            SEED += 1\n",
        "\n",
        "    if not os.path.exists(OUTDIR):\n",
        "        os.makedirs(OUTDIR)\n",
        "\n",
        "    scraper.updateGlobals(globals())\n",
        "    scraper.scrape('prompts')\n",
        "    scraper.scrape('inits')\n",
        "    scraper.scrape('diffusion_settings')\n",
        "\n",
        "    epoch_time = int(time.time())\n",
        "    if not SKIP_DIFFUSION_RUN:\n",
        "\n",
        "        prompt_suffix = f' (Prompt Style: {PROMPT_STYLE})' if style.__contains__(PROMPT_STYLE) else ''\n",
        "        encoded_seed = f' (Encoded from: {text_seed})' if text_seed else ''\n",
        "        gen_seed = torch.Generator(\"cuda\").manual_seed(SEED)\n",
        "        eta_prev = f' (ETA: {DDIM_ETA})' if SAMPLER is 'DDIM' else ''\n",
        "        if not GOBIG_SLICE_RUN:\n",
        "            print(f\"\\n\\033[1mBatch {(i+1)}/{len(ITERATE_THIS)} Iteration {(iteration+1)}/{NUM_ITERS}\\033[0m\")\n",
        "        else:\n",
        "            print(f\"\\n\\033[1mBatch {(i+1)}/{len(ITERATE_THIS)} Slice {(iteration)}/{SLICES_NUM}\\033[0m\")\n",
        "        print(f':seedling: Seed: \\033[1m{SEED}{encoded_seed}\\033[0m, :triangular_ruler: Scale: \\033[1m{SCALE}\\033[0m, :footprints: Steps: \\033[1m{STEPS}\\033[0m, :artist_palette: Sampler: {SAMPLER}{eta_prev} :framed_picture: Resolution: \\033[1m{WIDTH}x{HEIGHT}')\n",
        "        midas_prev = f' (Type: \\033[1m{MIDAS_TYPE}\\033[0m, Mode: \\033[1m{MIDAS_MODE}\\033[0m)' if GENERATE_MIDAS_DEPTH else ''\n",
        "        ca_prev = f' (Strength: \\033[1m{CA_STRENGTH}\\033[0m, Jitter: \\033[1m{CA_JITTER}\\033[0m, Overlay: \\033[1m{CA_OVERLAY}\\033[0m, No Radial Blur: \\033[1m{CA_NO_RADIAL_BLUR}\\033[0m)\\n' if CA_DIFFUSE_IMAGE else ''\n",
        "        print(f'Scale Down: \\033[1m{SCALE_DOWN_ENHANCEMENTS_FOR_ESRGAN}\\033[0m, Sharpen Passes: \\033[1m{SHARPEN_AMOUNT}\\033[0m, Chromatic Aberration: \\033[1m{CA_DIFFUSE_IMAGE}\\033[0m{ca_prev} Depth Export: \\033[1m{GENERATE_MIDAS_DEPTH}\\033[0m{midas_prev}\\n')\n",
        "        print(f\"\\033[0m:black_nib: Prompt{prompt_suffix}:\\033[1m\")\n",
        "        printPrompt(PROMPT)\n",
        "        print(\"\\033[0m\\n\")\n",
        "\n",
        "        # Parse Prompt\n",
        "        NEG_PROMPT = ''\n",
        "        if '--' in PROMPT:\n",
        "            pparts = [p.strip() for p in PROMPT.split('--')]; PROMPT = pparts[0]; NEG_PROMPT = pparts[1]\n",
        "\n",
        "        # Apply Style\n",
        "        if PROMPT_STYLE.lower() not in [None, 'none', ''] and style.__contains__(PROMPT_STYLE):\n",
        "            sparts = [s.strip() for s in style[PROMPT_STYLE].split('--')]; PROMPT += f', {sparts[0]}'; NEG_PROMPT += f', {sparts[1]}'\n",
        "\n",
        "        # Load Cached Pipelines\n",
        "        if CACHE_PIPELINES:\n",
        "            if MODEL_ID is not last_model_type:\n",
        "                RECACHE_PIPE = True\n",
        "            clean_env()\n",
        "            stt = int(time.time())\n",
        "            print(':gear: Loading Stable Diffusion Pipeline from cache...')\n",
        "            pipe = cache_pipe(pipe_type, MODEL_ID, model_cache, PIPE_CACHE)\n",
        "            if pipe is None:\n",
        "                raise Exception(\":warning: Unable to load pipe from cache!\")\n",
        "            fnt = time_format(int(time.time()) - stt)\n",
        "            print(f':check_mark_button: Pipeline loaded in {fnt}')\n",
        "\n",
        "        if RECURSIVE_EVOLUTION and last_diffusion_filedir is not None:\n",
        "            print(\":information: Found evolution file:\", last_diffusion_filedir)\n",
        "            init = last_diffusion_filedir\n",
        "            try:\n",
        "                from PIL import ImageOps\n",
        "                init = Image.open(fetch(init)).convert(\"RGB\")\n",
        "                original_init = init.copy()\n",
        "                if RECURSIVE_HYBRID_EVO and last_diffusion_hybrid_filedir is not None:\n",
        "                    if USE_BASIC_IMAGE_DISPLAY:\n",
        "                        print(\"Last Evolution Image:\")\n",
        "                        display(original_init)\n",
        "                    else:\n",
        "                        displayJsImage((i+1), (iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'Last Evo Image B: {(i+1)} I: {(iteration+1)}', original_init)\n",
        "                    print(f\":information: Applying hybrid overlay to init image with: {last_diffusion_hybrid_filedir}\")\n",
        "                    last_init = Image.open(fetch(last_diffusion_hybrid_filedir)).convert('RGB')\n",
        "                    init = overlayImage(init, last_init, RECURSIVE_HYBRID_OVERLAY, 'LANCZOS', True)\n",
        "                    if USE_BASIC_IMAGE_DISPLAY:\n",
        "                        print(\"Hybrid Evolution Image:\")\n",
        "                        display(init)\n",
        "                    else:\n",
        "                        displayJsImage((i+1), (iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'Hybrid Evo Image B: {(i+1)} I: {(iteration+1)}', init)\n",
        "                init = preprocess(init)\n",
        "                pipe_type = 'img2img'\n",
        "                if not CACHE_PIPELINES:\n",
        "                    try:\n",
        "                        if pipe:\n",
        "                            print(\":computer_disk: Pipeline already in memory. Starting diffusion environment...\\n\")\n",
        "                    except NameError:\n",
        "                        pipe = setup_pipe(pipe_type, MODEL_ID, model_cache)\n",
        "                        pass\n",
        "                    #if type(pipe) is not DiffusionPipeline:\n",
        "                    #    print(\":hourglass_not_done: Switching pipeline to Image-to-Image for Evolution...\")\n",
        "                    #    pipe = setup_pipe(pipe_type, MODEL_ID, model_cache)\n",
        "                else:\n",
        "                    if MODEL_ID is not last_model_type:\n",
        "                        RECACHE_PIPE = True\n",
        "                    #print(\":hourglass_not_done: Switching pipeline cache to Image-to-Image for Evolution...\")\n",
        "                    pipe = cache_pipe(pipe_type, MODEL_ID, model_cache, PIPE_CACHE)\n",
        "            except Exception as e:\n",
        "                raise e\n",
        "\n",
        "    if init is not None:\n",
        "        if not GOBIG_SLICE_RUN and not RECURSIVE_HYBRID_EVO:\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                print(\"Resized Init Image:\")\n",
        "                display(original_init)\n",
        "            else:\n",
        "                displayJsImage((i+1), (iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'Resized Init Image B: {(i+1)} I: {(iteration+1)}', original_init)\n",
        "        if SKIP_DIFFUSION_RUN:\n",
        "            image = original_init.copy()\n",
        "        if INSTALL_CLIP_INTERROGATOR and INTERROGATE_INIT_IMAGE:\n",
        "                do_interrogate(image)\n",
        "                scraper.updateGlobals(globals())\n",
        "                scraper.scrape('clip_interrogator')\n",
        "\n",
        "    if mask is not None:\n",
        "        if not GOBIG_SLICE_RUN:\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                print(\"Resized Mask Image:\")\n",
        "                display(original_mask)\n",
        "            else:\n",
        "                displayJsImage((i+1), (iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'Resized Mask Image B: {(i+1)} I: {(iteration+1)}', original_mask)\n",
        "\n",
        "    # Setup Pipes\n",
        "    if not SKIP_DIFFUSION_RUN:\n",
        "\n",
        "        from diffusers import (\n",
        "            PNDMScheduler, \n",
        "            LMSDiscreteScheduler, \n",
        "            DDIMScheduler, \n",
        "            DDPMScheduler, \n",
        "            EulerAncestralDiscreteScheduler, \n",
        "            EulerDiscreteScheduler, \n",
        "            #DPMSolverMultistepScheduler\n",
        "        )\n",
        "\n",
        "        # Setup sampler\n",
        "        if SAMPLER == 'PNDM':\n",
        "            pipe.scheduler = PNDMScheduler(trained_betas= None, beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "        elif SAMPLER == 'LMS':\n",
        "            pipe.scheduler = LMSDiscreteScheduler(trained_betas= None, beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "        elif SAMPLER == 'DDIM':\n",
        "            pipe.scheduler = DDIMScheduler(trained_betas= None, beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\",  clip_sample=False, set_alpha_to_one=False)\n",
        "        elif SAMPLER == 'Euler_A': \n",
        "            pipe.scheduler = EulerAncestralDiscreteScheduler(trained_betas= None, beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\")\n",
        "        elif SAMPLER == 'Euler':\n",
        "            pipe.scheduler = EulerDiscreteScheduler(trained_betas= None, beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\")\n",
        "        elif SAMPLER == 'DPMSolver':\n",
        "            pipe.scheduler = DPMSolverMultistepScheduler(beta_start=0.00085, beta_end=0.012, solver_order=2, predict_epsilon=True, thresholding=False, algorithm_type=\"dpmsolver++\", solver_type=\"midpoint\")\n",
        "        else:\n",
        "            pipe.scheduler = PNDMScheduler(trained_betas= None, beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "\n",
        "        # Enable Concept\n",
        "        if CONCEPT_ID.lower() not in ['none', '']:\n",
        "            print(f':hourglass_not_done: Downloading concept \\'{CONCEPT_ID}\\' ...')\n",
        "            concept = download_concept(CONCEPT_ID)\n",
        "            #tokenizer, text_encoder = CTTE(MODEL_ID)\n",
        "            if CONCEPT_TOKEN in ['none','']: CONCEPT_TOKEN = None\n",
        "            print(':information: Loading concept into CLIP')\n",
        "            load_concept(concept, pipe.text_encoder, pipe.tokenizer, CONCEPT_TOKEN)\n",
        "            #pipe.tokenizer = tokenizer\n",
        "            #pipe.text_encoder = text_encoder\n",
        "            pipe.to('cuda') # Fix for mixed devices error\n",
        "            print(':check_mark_button: Concept loaded into pipe')\n",
        "\n",
        "        if ENABLE_ATTENTION_SLICES:\n",
        "            print(':gear: Attention Slices Enabled')\n",
        "            pipe.enable_attention_slicing()\n",
        "        else:\n",
        "            pipe.disable_attention_slicing()\n",
        "        print(':check_mark_button: Pipeline setup complete.')\n",
        "\n",
        "        # Do diffusion\n",
        "        pipeout = None\n",
        "        last_model_type = MODEL_ID\n",
        "        try:\n",
        "            stt = int(time.time())\n",
        "            print(f\":alembic: Starting Diffusion run with {MODEL_ID}\")\n",
        "            if init is not None:\n",
        "                if mask is not None:\n",
        "                    pipeout = pipe.inpaint(prompt=PROMPT, negative_prompt=NEG_PROMPT, num_inference_steps=STEPS, init_image=init, mask_image=mask, strength=INIT_SCALE, guidance_scale=SCALE, generator=gen_seed)\n",
        "                    image = pipeout.images[0]\n",
        "                else:\n",
        "                    pipeout = pipe.img2img(prompt=PROMPT, negative_prompt=NEG_PROMPT, num_inference_steps=STEPS, init_image=init, strength=INIT_SCALE, guidance_scale=SCALE, generator=gen_seed)\n",
        "                    image = pipeout.images[0]\n",
        "            else:\n",
        "                pipeout = pipe.text2img(prompt=PROMPT, negative_prompt=NEG_PROMPT, num_inference_steps=STEPS, width=int(WIDTH), height=int(HEIGHT), guidance_scale=SCALE, eta=DDIM_ETA, generator=gen_seed)\n",
        "                image = pipeout.images[0]\n",
        "        except BaseException as e:\n",
        "            raise e\n",
        "        finally:\n",
        "            if pipeout and pipeout.nsfw_content_detected[0] and ENABLE_NSFW_FILTER:\n",
        "                print(\":passport_control: Censoring NSFW content...\")\n",
        "                image = image.filter(ImageFilter.GaussianBlur(radius = 18))\n",
        "            if CACHE_PIPELINES and not SKIP_DIFFUSION_RUN:\n",
        "                print(\":gear: Deleting pipeline...\")\n",
        "                del pipeout, pipe\n",
        "            fnt = time_format(int(time.time()) - stt)\n",
        "            print(f':check_mark_button: Diffusion completed in {fnt}')\n",
        "            clean_env()\n",
        "\n",
        "    filename = f'{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}.png'\n",
        "    filedir = f'{OUTDIR}/{filename}'\n",
        "    image.save(filedir)\n",
        "\n",
        "    if RECURSIVE_EVOLUTION:\n",
        "        if RECURSIVE_HYBRID_EVO and last_diffusion_filedir is not None:\n",
        "            print(f':information: Setting evolution hybrid file to:', last_diffusion_filedir)\n",
        "            last_diffusion_hybrid_filedir = last_diffusion_filedir\n",
        "        last_diffusion_filedir = filedir\n",
        "        print(\":information: Setting evolution file to:\", last_diffusion_filedir)\n",
        "        time.sleep(1)\n",
        "\n",
        "    if INTERROGATE_DIFFUSION_IMAGE:\n",
        "        clean_env()\n",
        "        interrogate_image = image.copy()\n",
        "\n",
        "    # Doing FDoF? We need depth output\n",
        "    if FDOF_IMAGE and not GENERATE_MIDAS_DEPTH:\n",
        "        if not inst['MIDAS_INSTALLED']:\n",
        "            print(':WARNING: Enabling \\'GENERATE_MIDAS_DEPTH\\' (True) for Depth Approximation necessary for Fake Depth of Field.')\n",
        "            install_midas()\n",
        "        else:\n",
        "            print(':WARNING: Unable to generate Fake Depth of Field! MiDaS Compatibility is not installed! Please enable \\'INSTALL_MIDAS\\' and re-run the setup cell.')\n",
        "            GENERATE_MIDAS_DEPTH = False\n",
        "            FDOF_IMAGE = False\n",
        "\n",
        "    # Do Depth Export\n",
        "    depth_image = None\n",
        "    if GENERATE_MIDAS_DEPTH:\n",
        "        if not inst['MIDAS_INSTALLED']:\n",
        "            install_midas()\n",
        "        stt = int(time.time())\n",
        "        print(\"Approximating diffusion depth...\")\n",
        "        midas = torch.hub.load(\"intel-isl/MiDaS\", MIDAS_TYPE)\n",
        "        device = torch.device(\"cuda\") if torch.cuda.is_available() and MIDAS_MODE is 'CUDA' else torch.device(\"cpu\")\n",
        "\n",
        "        midas.to(device).eval()\n",
        "        midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "\n",
        "        if MIDAS_TYPE == \"DPT_Large\" or MIDAS_TYPE == \"DPT_Hybrid\":\n",
        "            transform = midas_transforms.dpt_transform\n",
        "        else:\n",
        "            transform = midas_transforms.small_transform\n",
        "\n",
        "        import cv2\n",
        "        img = cv2.imread(filedir)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        input_batch = transform(img).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prediction = midas(input_batch)\n",
        "\n",
        "            prediction = torch.nn.functional.interpolate(\n",
        "                prediction.unsqueeze(1),\n",
        "                size=img.shape[:2],\n",
        "                mode=\"bicubic\",\n",
        "                align_corners=False,\n",
        "            ).squeeze()\n",
        "\n",
        "        depth = prediction.cpu().numpy()\n",
        "        depth = (depth * 255 / (np.max(depth)+1)).astype('uint8')\n",
        "        depth_image = Image.fromarray(depth)\n",
        "        if SAVE_MIDAS_DEPTH:\n",
        "            depth_image.save(filedir.replace('.png', '_depth.png'))\n",
        "        del midas, device, midas_transforms\n",
        "        del transform, img, input_batch, prediction, depth\n",
        "        fnt = time_format(int(time.time()) - stt)\n",
        "        print(f'Depth approximation completed in {fnt}')\n",
        "        clean_env()\n",
        "\n",
        "    original_displayed = False\n",
        "    # Do Median Filter\n",
        "    if MEDIAN_FILTER_IMAGE:\n",
        "        original_displayed = True\n",
        "        stt = int(time.time())\n",
        "        print('Applying Median Filter...')\n",
        "        if USE_BASIC_IMAGE_DISPLAY:\n",
        "            print(\"Original Diffusion Image:\")\n",
        "            display(image)\n",
        "        else:\n",
        "            displayJsImage((i+1), (iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'Original Diffusion B: {(i+1)} I: {(iteration+1)}', image)\n",
        "        image = medianFilter(image, MEDIAN_DIAMETER, MEDIAN_SIGMA_COLOR, MEDIAN_SIGMA_SPACE)\n",
        "        fnt = time_format(int(time.time()) - stt)\n",
        "        print(f'Applied Median Filter in {fnt}')\n",
        "\n",
        "    # Do Chromatic Aberration\n",
        "    if CA_DIFFUSE_IMAGE:\n",
        "        if not inst['KROMO_INSTALLED']:\n",
        "            install_kromo()\n",
        "        stt = int(time.time())\n",
        "        original_size = image.size\n",
        "        res = ''\n",
        "        print(f\"Applying chromatic aberration to result image.\\n\")\n",
        "        os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/kromo')\n",
        "        ca_no_blur = '-n ' if CA_NO_RADIAL_BLUR else ''\n",
        "        res += subprocess.run(f'python kromo.py -s {CA_STRENGTH} -j {CA_JITTER} -y {CA_OVERLAY} {ca_no_blur}-o {filedir} {filedir}'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "        if res.strip() != '':\n",
        "            print(res)\n",
        "        image = Image.open(filedir)\n",
        "        if image.size != original_size:\n",
        "            image.resize(original_size, PILSampler(SCALING_SAMPLER))\n",
        "        os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "        fnt = time_format(int(time.time() - stt))\n",
        "        print(f'Chromatic aberration applied in {fnt}')\n",
        "        clean_env()\n",
        "\n",
        "    # Do Sharpen\n",
        "    if SHARPEN_AMOUNT > 0:\n",
        "        stt = int(time.time())\n",
        "        print(f\"Sharpening diffusion result with {SHARPEN_AMOUNT} passes.\")\n",
        "        image = sharpenImage(image, SHARPEN_AMOUNT)\n",
        "        fnt = time_format(int(time.time()) - stt)\n",
        "        print(f'Sharpening completed in {fnt}')\n",
        "        clean_env()\n",
        "\n",
        "    # Do FDOF\n",
        "    fdof_title = None\n",
        "    if FDOF_IMAGE:\n",
        "        if depth_image:\n",
        "            stt = int(time.time())\n",
        "            print('Applying Fake Depth of Field...')\n",
        "            fdof_image = portraitBlur(image, depth_image, FDOF_RADIUS, FDOF_SAMPLES).convert('RGB')\n",
        "            fnt = time_format(int(time.time()) - stt)\n",
        "            print(f'Applied FDOF in {fnt}')\n",
        "            if FDOF_REPLACE_IMAGE or KEEP_ONLY_FINAL_IMAGE:\n",
        "                fdof_title = 'FDOF Image'\n",
        "                if not original_displayed:\n",
        "                    if USE_BASIC_IMAGE_DISPLAY:\n",
        "                        print('Original Diffusion Image:')\n",
        "                        display(image)\n",
        "                    else:\n",
        "                        displayJsImage((i+1), (iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'Original Diffusion B: {(i+1)} I: {(iteration+1)}', image)\n",
        "                fdof_image.save(filedir)\n",
        "                image = Image.open(filedir)\n",
        "                fdof_image.close()\n",
        "                del fdof_image \n",
        "        else:\n",
        "            print(f':WARNING: Warning: depth_image is not generated! Is MiDaS compatibility installed?')\n",
        "\n",
        "    # Do Seamless Image\n",
        "    if TILEABLE_IMAGE:\n",
        "        if not inst['IMG2TEXTURE_INSTALLED']:\n",
        "            install_img2texture()\n",
        "        stt = int(time.time())\n",
        "        print(f'Processing tiled seamless image...')\n",
        "        #image = image.resize((WIDTH, HEIGHT))\n",
        "        image.save(f'{OUTDIR}/tile_temp.png')\n",
        "        tileFilename = filename.replace('.png','_seamless.png')\n",
        "        seamlessPath = os.path.join(OUTDIR, tileFilename)\n",
        "        tileFlag = f' --tile' if TILED else ''\n",
        "        if GENERATE_MIDAS_DEPTH and depth_image and SAVE_MIDAS_DEPTH:\n",
        "            depth_seamlessPath = os.path.join(OUTDIR, filename.replace('.png','_depth_seamless.png'))\n",
        "            depthFileDir = filedir.replace('.png', '_depth.png')\n",
        "            res = subprocess.run(f'img2texture {depthFileDir} {depth_seamlessPath} --overlap {TILE_OVERLAP}{tileFlag}'.split(' '), capture_output=True, text=True, input=\"y\")\n",
        "        print(f'Saving tiled image to: {seamlessPath}')\n",
        "        res = subprocess.run(f'img2texture {OUTDIR}/tile_temp.png {seamlessPath} --overlap {TILE_OVERLAP}{tileFlag}'.split(' '), capture_output=True, text=True, input=\"y\")\n",
        "        seamless_image = Image.open(seamlessPath).convert('RGB')\n",
        "        tiled_image = Image.open(seamlessPath.replace('.png','_2x2.jpg')).convert('RGB')\n",
        "        os.remove(f'{OUTDIR}/tile_temp.png')\n",
        "        fnt = time_format(int(time.time()) - stt)\n",
        "        print(f'Processed tiled seamless image in {fnt}')\n",
        "\n",
        "    main_img_title = 'SD Image'\n",
        "    if SKIP_DIFFUSION_RUN or original_displayed:\n",
        "        main_img_title = 'Processed Image'\n",
        "    elif fdof_title:\n",
        "        main_img_title = fdof_title\n",
        "\n",
        "    # Display Diffusion Image\n",
        "    if not GOBIG_SLICE_RUN:\n",
        "        if USE_BASIC_IMAGE_DISPLAY:\n",
        "            print(\"Diffusion Image:\")\n",
        "            display(image)\n",
        "        else:\n",
        "            displayJsImage((i+1), (iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'{main_img_title} B: {(i+1)} I: {(iteration+1)}', image)\n",
        "\n",
        "    # Display Depth Image\n",
        "    try:\n",
        "        if depth_image:\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                print('Depth Map:')\n",
        "                display(depth_image)\n",
        "            else:\n",
        "                displayJsImage((i+1), (iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'Depth Map B: {(i+1)} I: {(iteration+1)}', depth_image)\n",
        "            depth_image.close()\n",
        "            del depth_image\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    # Display FDOF Image\n",
        "    try:\n",
        "        if fdof_image:\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                print('FDOF Image:')\n",
        "                display(fdof_image)\n",
        "            else:\n",
        "                displayJsImage((i+1), (iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'FDOF B: {(i+1)} I: {(iteration+1)}', fdof_image)\n",
        "            fdof_image.close()\n",
        "            del fdof_image\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    # Display Tiled Seamless Image\n",
        "    try:\n",
        "        if seamless_image:\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                print('Seamless Image:')\n",
        "                display(seamless_image)\n",
        "            else:\n",
        "                displayJsImage((i+1), (iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'Seamless Image B: {(i+1)} I: {(iteration+1)}', seamless_image)\n",
        "            seamless_image.close()\n",
        "            del seamless_image\n",
        "    except NameError:\n",
        "        pass\n",
        "    try:\n",
        "        if tiled_image:\n",
        "            if USE_BASIC_IMAGE_DISPLAY:\n",
        "                print('Tiled Image:')\n",
        "                display(tiled_image)\n",
        "            else:\n",
        "                displayJsImage((i+1), (iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'Tiled Image B: {(i+1)} I: {(iteration+1)}', tiled_image)\n",
        "            tiled_image.close()\n",
        "            del tiled_image\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    if IMAGE_UPSCALER == \"IMG2IMG\":\n",
        "\n",
        "        orig_image = image.copy()\n",
        "\n",
        "        image = image.resize((int(WIDTH) * int(UPSCALE_AMOUNT), int(HEIGHT) * int(UPSCALE_AMOUNT)), PILSampler(SCALING_SAMPLER))\n",
        "\n",
        "        last_pipe = pipe_type\n",
        "        print('Last Pipe:', last_pipe)\n",
        "\n",
        "        stt = int(time.time())\n",
        "        if CACHE_PIPELINES and pipe_type is not 'img2img':\n",
        "            if MODEL_ID is not last_model_type:\n",
        "                RECACHE_PIPE = True\n",
        "            clean_env()\n",
        "            print(':gear: Loading IMG2IMG Stable Diffusion Pipeline from cache...')\n",
        "            pipe = cache_pipe('img2img', MODEL_ID, model_cache, PIPE_CACHE)\n",
        "            if type(pipe) is DiffusionPipeline:\n",
        "                fnt = time_format(int(time.time()) - stt)\n",
        "                print(f':check_mark_button: Pipeline loaded in {fnt}')\n",
        "            else:\n",
        "                raise Exception(\":warning: Unable to load pipeline from cache!\")\n",
        "\n",
        "        if not CACHE_PIPELINES and pipe_type is not 'img2img':\n",
        "            print(\":hourglass_not_done: Switching pipeline to Image-to-Image for Upscaling...\")\n",
        "            pipe = setup_pipe('img2img', MODEL_ID, model_cache)\n",
        "            if type(pipe) is DiffusionPipeline:\n",
        "                fnt = time_format(int(time.time()) - stt)\n",
        "                print(f':check_mark_button: Pipeline loaded in {fnt}')\n",
        "            else: \n",
        "                raise Exception(\":warning: Unable to load proper pipeline!\")\n",
        "        else:\n",
        "            try:\n",
        "                if pipeout:\n",
        "                    del pipeout\n",
        "            except NameError:\n",
        "                pass\n",
        "\n",
        "        from diffusers import (\n",
        "            PNDMScheduler, \n",
        "            LMSDiscreteScheduler, \n",
        "            DDIMScheduler, \n",
        "            DDPMScheduler, \n",
        "            EulerAncestralDiscreteScheduler, \n",
        "            EulerDiscreteScheduler, \n",
        "            DPMSolverMultistepScheduler\n",
        "        )\n",
        "\n",
        "        # Setup sampler\n",
        "        if SAMPLER == 'PNDM':\n",
        "            pipe.scheduler = PNDMScheduler(trained_betas= None, beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "        elif SAMPLER == 'LMS':\n",
        "            pipe.scheduler = LMSDiscreteScheduler(trained_betas= None, beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "        elif SAMPLER == 'DDIM':\n",
        "            pipe.scheduler = DDIMScheduler(trained_betas= None, beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\",  clip_sample=False, set_alpha_to_one=False)\n",
        "        elif SAMPLER == 'Euler_A': \n",
        "            pipe.scheduler = EulerAncestralDiscreteScheduler(trained_betas= None, beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\")\n",
        "        elif SAMPLER == 'Euler':\n",
        "            pipe.scheduler = EulerDiscreteScheduler(trained_betas= None, beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\")\n",
        "        elif SAMPLER == 'DPMSolver':\n",
        "            pipe.scheduler = DPMSolverMultistepScheduler(beta_start=0.00085, beta_end=0.012, solver_order=2, predict_epsilon=True, thresholding=False, algorithm_type=\"dpmsolver++\", solver_type=\"midpoint\")\n",
        "        else:\n",
        "            pipe.scheduler = PNDMScheduler(trained_betas= None, beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "\n",
        "        # Enable Concept\n",
        "        if CONCEPT_ID.lower() not in ['none', '']:\n",
        "            print(f':hourglass_not_done: Downloading concept \\'{CONCEPT_ID}\\' ...')\n",
        "            concept = download_concept(CONCEPT_ID, NON_HF_CONCEPT)\n",
        "            tokenizer, text_encoder = CTTE(MODEL_ID)\n",
        "            if CONCEPT_TOKEN in ['none','']: CONCEPT_TOKEN = None\n",
        "            print(':information: Loading concept into CLIP')\n",
        "            load_concept(concept, text_encoder, tokenizer, CONCEPT_TOKEN)\n",
        "            pipe.tokenizer = tokenizer\n",
        "            pipe.text_encoder = text_encoder\n",
        "            pipe.to('cuda') # Fix for mixed devices error\n",
        "            print(':check_mark_button: Concept loaded into pipe')\n",
        "\n",
        "        if ENABLE_ATTENTION_SLICES:\n",
        "            print(':gear: Attention Slices Enabled')\n",
        "            pipe.enable_attention_slicing()\n",
        "        else:\n",
        "            pipe.disable_attention_slicing()\n",
        "        print(':check_mark_button: Pipeline setup complete.')\n",
        "\n",
        "        print(\":alembic: Starting IMG2IMG Upscale Diffusion...\")\n",
        "        try:\n",
        "            with autocast(\"cuda\"):\n",
        "                pipeout = pipe.img2img(prompt=PROMPT, negative_prompt=NEG_PROMPT, num_inference_steps=STEPS, init_image=image, strength=INIT_SCALE, guidance_scale=SCALE, generator=gen_seed)\n",
        "                image = pipeout.images[0]\n",
        "                if pipeout and pipeout.nsfw_content_detected[0] and ENABLE_NSFW_FILTER:\n",
        "                    print(\":passport_control: Censoring NSFW content...\")\n",
        "                    image = image.filter(ImageFilter.GaussianBlur(radius = 18))\n",
        "        except RuntimeError as e:\n",
        "            if 'out of memory' in str(e):\n",
        "                print(f\"\\u001b[31m\\u001b[1m\\u001b[4mCRITICAL ERROR\\u001b[0m: {gpu_name} ran out of memory! If this error persists, the GPU may have crashed, and requires a disconnect/re-run.\")\n",
        "                pass\n",
        "            else:\n",
        "                raise e\n",
        "        finally:\n",
        "            if CACHE_PIPELINES:\n",
        "                del pipeout, pipe\n",
        "\n",
        "        fnt = time_format(int(time.time()) - stt)\n",
        "        print(f':check_mark_button: IMG2IMG Upscale completed in {fnt}')\n",
        "        clean_env()\n",
        "        \n",
        "        if HYBRID_UPSCALE.lower() not in [None, 'none', '']:\n",
        "            print(':information: Applying hybrid upscale...')\n",
        "            image = overlayImage(image.copy(), orig_image.copy(), HYBRID_OVERLAY, HYBRID_UPSCALE, HYBRID_SUPER_RESOLUTION)\n",
        "\n",
        "        if KEEP_ONLY_FINAL_IMAGE:\n",
        "            image.save(filedir)\n",
        "        else:\n",
        "            image.save(filedir.replace('.png','_upscaled.png'))\n",
        "\n",
        "        if USE_BASIC_IMAGE_DISPLAY:\n",
        "            print('IMG2IMG Upscale Image:')\n",
        "            display(image)\n",
        "        else:\n",
        "            displayJsImage((i+1), (iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'IMG2IMG B: {(i+1)} I: {(iteration+1)}', image)\n",
        "\n",
        "        if ( not CACHE_PIPELINES \n",
        "                and INIT_IMAGE.lower() in [None, '', 'none'] ):\n",
        "            print(\"Switching back to original pipeline...\")\n",
        "            pipe = setup_pipe(last_pipe, MODEL_ID, model_cache)\n",
        "            pipe_type = last_pipe\n",
        "\n",
        "    if IMAGE_UPSCALER == \"GFPGAN\":\n",
        "        if not inst['GFPGAN_INSTALLED']:\n",
        "            install_gfpgan()\n",
        "        stt = int(time.time())\n",
        "        clean_env()\n",
        "        print(':sparkle: GFPGAN Face Restoration... ')\n",
        "        os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/GFPGAN')\n",
        "        print(subprocess.run(f'python inference_gfpgan.py -i {filedir} -o {OUTDIR} -v 1.3 -s {UPSCALE_AMOUNT} --bg_upsampler realesrgan'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        gfpgan_image = Image.open(f'{OUTDIR}/restored_imgs/{filename}')\n",
        "        if HYBRID_UPSCALE.lower() not in [None, 'none', '']:\n",
        "            gfpgan_image = overlayImage(gfpgan_image, image, HYBRID_OVERLAY, HYBRID_UPSCALE, HYBRID_SUPER_RESOLUTION)\n",
        "            gfpgan_image.save(f'{OUTDIR}/restored_imgs/{filename}')\n",
        "        if USE_BASIC_IMAGE_DISPLAY:\n",
        "            print('GFPGAN Image:')\n",
        "            display(gfpgan_image)\n",
        "        else:\n",
        "            displayJsImage((i+1), (iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'GFPGAN B: {(i+1)} I: {(iteration+1)}', gfpgan_image)\n",
        "        os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "        print(f'Moving enhanced image to {OUTDIR}')\n",
        "        if not KEEP_ONLY_FINAL_IMAGE:\n",
        "            shutil.move(f'{OUTDIR}/restored_imgs/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "        else:\n",
        "            shutil.move(f'{OUTDIR}/restored_imgs/{filename}', f'{filedir}')\n",
        "        fnt = time_format(int(time.time()) - stt)\n",
        "        print(f'GFPGAN Face Restoration completed in {fnt}')\n",
        "        clean_env()\n",
        "\n",
        "\n",
        "    if IMAGE_UPSCALER == \"Enhanced Real-ESRGAN\":\n",
        "        if not inst['ESRGAN_INSTALLED']:\n",
        "            install_realesrgan()\n",
        "        stt = int(time.time())\n",
        "        clean_env()\n",
        "        print(':multiply: Real-ESRGAN Upscaling... ')\n",
        "        print(f'For Real-ESRGAN upscaling only 2, 4, and 8 are supported. Choosing the nearest Value: {nearest_value}')\n",
        "        UPSCALE_AMOUNT = nearest_value\n",
        "        os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "        sr_image = upscale(image, UPSCALE_AMOUNT, ESRGAN_MODE)\n",
        "        if HYBRID_UPSCALE.lower() not in [None, 'none', '']:\n",
        "            sr_iamge = overlayImage(sr_image, image, HYBRID_OVERLAY, HYBRID_UPSCALE, HYBRID_SUPER_RESOLUTION)\n",
        "        if USE_BASIC_IMAGE_DISPLAY:\n",
        "            print('Real-ESRGAN Image:')\n",
        "            display(sr_image)\n",
        "        else:\n",
        "            displayJsImage((i+1), (iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'Real-ESRGAN B: {(i+1)} I: {(iteration+1)}', sr_image)\n",
        "        if not KEEP_ONLY_FINAL_IMAGE:\n",
        "            sr_image.save(f'{OUTDIR}/{str(epoch_time)}_scale_{SCALE}_steps_{STEPS}_seed_{SEED}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "        else:\n",
        "            sr_image.save(filedir)\n",
        "        sr_image.close()\n",
        "        del sr_image\n",
        "        fnt = time_format(int(time.time()) - stt)\n",
        "        print(f'Enhanced Real-ESRGAN completed in {fnt}')\n",
        "        clean_env()\n",
        "\n",
        "    if IMAGE_UPSCALER == \"GFPGAN + Enhanced ESRGAN\":\n",
        "        if not inst['GFPGAN_INSTALLED']:\n",
        "            install_gfpgan()\n",
        "        if not inst['ESRGAN_INSTALLED']:\n",
        "            install_realesrgan()\n",
        "        stt = int(time.time())\n",
        "        clean_env()\n",
        "        # GFPGAN\n",
        "        print(':sparkle: GFPGAN Face Restoration... ')\n",
        "        os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/GFPGAN')\n",
        "        print(subprocess.run(f'python inference_gfpgan.py -i {filedir} -o {OUTDIR} -v 1.3 -s 1 --bg_upsampler realesrgan'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        os.chdir(STABLE_DIFFUSION_WORKDIR)\n",
        "        shutil.move(f'{OUTDIR}/restored_imgs/{filename}', f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            \n",
        "        # Real-ESRGAN\n",
        "        os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/Real-ESRGAN')\n",
        "        enhanced_image = Image.open(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "        if SCALE_DOWN_ENHANCEMENTS_FOR_ESRGAN:\n",
        "            enhanced_image = enhanced_image.resize(image.size, PILSampler(SCALING_SAMPLER))\n",
        "        if USE_BASIC_IMAGE_DISPLAY:\n",
        "            print('GFPGAN Image:')\n",
        "            display(enhanced_image)\n",
        "        else:\n",
        "            displayJsImage((i+1), (iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'GFPGAN B: {(i+1)} I: {(iteration+1)}', enhanced_image)\n",
        "        print(\":multiply: Real-ESRGAN Upscaling... \")\n",
        "        if UPSCALE_AMOUNT not in [2,4,8]:\n",
        "            UPSCALE_AMOUNT = nearest_value\n",
        "            print(f'For Real-ESRGAN upscaling only 2, 4, and 8 are supported. Choosing the nearest Value: {nearest_value}')\n",
        "        sr_image = upscale(enhanced_image, UPSCALE_AMOUNT, ESRGAN_MODE)\n",
        "        if HYBRID_UPSCALE.lower() not in [None, 'none', '']:\n",
        "            sr_iamge = overlayImage(sr_image, image, HYBRID_OVERLAY, HYBRID_UPSCALE, HYBRID_SUPER_RESOLUTION)\n",
        "        if USE_BASIC_IMAGE_DISPLAY:\n",
        "            print('Real-ESRGAN Image:')\n",
        "            display(sr_image)\n",
        "        else:\n",
        "            displayJsImage((i+1), (iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'Real-ESRGAN B: {(i+1)} I: {(iteration+1)}', sr_image)\n",
        "        if not KEEP_ONLY_FINAL_IMAGE:\n",
        "            sr_image.save(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "        else:\n",
        "            os.remove(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "            sr_image.save(filedir)\n",
        "        sr_image.close()\n",
        "        del sr_image\n",
        "        enhanced_image.close()\n",
        "        del enhanced_image\n",
        "        fnt = time_format(int(time.time()) - stt)\n",
        "        print(f'GFPGAN + Real-ESRGAN completed in {fnt}')\n",
        "        clean_env()\n",
        "\n",
        "    if IMAGE_UPSCALER == \"CodeFormer\":\n",
        "        if not inst['CODEFORMER_INSTALLED']:\n",
        "            install_codeformer()\n",
        "        stt = int(time.time())\n",
        "        fidelity = float(CODEFORMER_FIDELITY)\n",
        "        if fidelity is 0:\n",
        "            fidelity = '0.0'\n",
        "        clean_env()\n",
        "        print(\":sparkle: CodeFormer Face Restoration... \")\n",
        "        print(subprocess.run(f'cp {filedir} {STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp/'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer')\n",
        "        print(subprocess.run(f'python inference_codeformer.py -w {fidelity} -i {STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp -o {STABLE_DIFFUSION_WORKDIR}/CodeFormer/results -s {CODEFORMER_UPSCALE_AMOUNT} --bg_upsampler realesrgan --face_upsample'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        os.remove(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp/{filename}')\n",
        "        cftarget = f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{CODEFORMER_UPSCALE_AMOUNT}.png' if not KEEP_ONLY_FINAL_IMAGE else filedir\n",
        "        shutil.copyfile(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer/results/final_results/{filename}', cftarget)\n",
        "        os.chdir(f'{STABLE_DIFFUSION_WORKDIR}')\n",
        "        sr_image = Image.open(cftarget)\n",
        "        if HYBRID_UPSCALE.lower() not in [None, 'none', '']:\n",
        "            sr_iamge = overlayImage(sr_image, image, HYBRID_OVERLAY, HYBRID_UPSCALE, HYBRID_SUPER_RESOLUTION)\n",
        "        if USE_BASIC_IMAGE_DISPLAY:\n",
        "            print('CodeFormer Image:')\n",
        "            display(sr_image)\n",
        "        else:\n",
        "            displayJsImage((i+1), (iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'CodeFormer B: {(i+1)} I: {(iteration+1)}', sr_image)\n",
        "        fnt = time_format(int(time.time()) - stt)\n",
        "        print(f'CodeFormer Face Restoration completed in {fnt}')\n",
        "        clean_env()\n",
        "\n",
        "    if IMAGE_UPSCALER == \"CodeFormer + Enhanced ESRGAN\":\n",
        "        if not inst['CODEFORMER_INSTALLED']:\n",
        "            install_codeformer()\n",
        "        if not inst['ESRGAN_INSTALLED']:\n",
        "            install_realesrgan()\n",
        "        stt = int(time.time())\n",
        "        fidelity = float(CODEFORMER_FIDELITY)\n",
        "        if fidelity is 0:\n",
        "            fidelity = '0.0'\n",
        "        clean_env()\n",
        "        # CodeFormer\n",
        "        print(\":sparkle: CodeFormer Face Restoration... \")\n",
        "        print(subprocess.run(f'cp {filedir} {STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp/'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer')\n",
        "        print(subprocess.run(f'python inference_codeformer.py --w {fidelity} --test_path {STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp'.split(\" \"), stdout=subprocess.PIPE).stdout.decode('utf-8'))\n",
        "        os.remove(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer/temp/{filename}')\n",
        "        cftarget = f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{CODEFORMER_UPSCALE_AMOUNT}.png' if not KEEP_ONLY_FINAL_IMAGE else filedir\n",
        "        shutil.copyfile(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer/results/temp_{fidelity}/final_results/{filename}', cftarget)\n",
        "            \n",
        "        # Real-ESRGAN\n",
        "        os.chdir(f'{STABLE_DIFFUSION_WORKDIR}/Real-ESRGAN')\n",
        "        enhanced_image = Image.open(cftarget)\n",
        "        if SCALE_DOWN_ENHANCEMENTS_FOR_ESRGAN:\n",
        "            enhanced_image = enhanced_image.resize(image.size, PILSampler(SCALING_SAMPLER))\n",
        "        if USE_BASIC_IMAGE_DISPLAY:\n",
        "            print('CodeFormer Image:')\n",
        "            display(enhanced_image)\n",
        "        else:\n",
        "            displayJsImage((i+1), (iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'CodeFormer B: {(i+1)} I: {(iteration+1)}', enhanced_image)\n",
        "        print(\":multiply: Real-ESRGAN Upscaling... \")\n",
        "        if UPSCALE_AMOUNT not in [2,4,8]:\n",
        "            UPSCALE_AMOUNT = nearest_value\n",
        "            print(f'For Real-ESRGAN upscaling only 2, 4, and 8 are supported. Choosing the nearest Value: {nearest_value}')\n",
        "        sr_image = upscale(enhanced_image, UPSCALE_AMOUNT, ESRGAN_MODE)\n",
        "        if HYBRID_UPSCALE.lower() not in [None, 'none', '']:\n",
        "            sr_iamge = overlayImage(sr_image, image, HYBRID_OVERLAY, HYBRID_UPSCALE, HYBRID_SUPER_RESOLUTION)\n",
        "        if USE_BASIC_IMAGE_DISPLAY:\n",
        "            print('Real-ESRGAN Image:')\n",
        "            display(sr_image)\n",
        "        else:\n",
        "            displayJsImage((i+1), (iteration+1), MAX_JS_IMAGE_WIDTH, IMAGES_DISPLAY_ABOVE_LOG, f'Real-ESRGAN B: {(i+1)} I: {(iteration+1)}', sr_image)\n",
        "        if not KEEP_ONLY_FINAL_IMAGE:\n",
        "            sr_image.save(f'{OUTDIR}/{filename.replace(\".png\",\"\")}_upscaled_{UPSCALE_AMOUNT}.png')\n",
        "        else:\n",
        "            sr_image.save(filedir)\n",
        "        sr_image.close()\n",
        "        del sr_image\n",
        "        enhanced_image.close()\n",
        "        del enhanced_image\n",
        "        fnt = time_format(int(time.time()) - stt)\n",
        "        print(f'CodeFormer + Real-ESRGAN completed in {fnt}')\n",
        "        clean_env()\n",
        "   \n",
        "    if INSTALL_CLIP_INTERROGATOR and INTERROGATE_DIFFUSION_IMAGE:\n",
        "        if not SKIP_DIFFUSION_RUN:\n",
        "            clean_env()\n",
        "            do_interrogate(interrogate_image.copy())\n",
        "            interrogate_image.close()\n",
        "            del interrogate_image\n",
        "            scraper.updateGlobals(globals())\n",
        "            scraper.scrape('clip_interrogator')\n",
        "\n",
        "    if SAVE_SETTINGS_FILE:\n",
        "        epoch_time = int(time.time())\n",
        "        with open(f'{OUTDIR}/{epoch_time}_settings.txt', 'w') as file:\n",
        "            file.write(json.dumps(scraper.params, indent=4))\n",
        "\n",
        "    if os.path.exists(f'{OUTDIR}/cmp'):\n",
        "        shutil.rmtree(f'{OUTDIR}/cmp')\n",
        "    if os.path.exists(f'{OUTDIR}/cropped_faces'):\n",
        "        shutil.rmtree(f'{OUTDIR}/cropped_faces')\n",
        "    if os.path.exists(f'{OUTDIR}/restored_faces'):\n",
        "        shutil.rmtree(f'{OUTDIR}/restored_faces')\n",
        "    if os.path.exists(f'{OUTDIR}/restored_imgs'):\n",
        "        shutil.rmtree(f'{OUTDIR}/restored_imgs')\n",
        "    if os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer/cropped_faces'):\n",
        "        shutil.rmtree(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer/cropped_faces')\n",
        "    if os.path.exists(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer/restored_faces'):\n",
        "        shutil.rmtree(f'{STABLE_DIFFUSION_WORKDIR}/CodeFormer/restored_faces')\n",
        "\n",
        "    if CLEAR_LOG_BETWEEN_ITERATIONS and not USE_BASIC_IMAGE_DISPLAY:\n",
        "        clearOutputArea(i, iteration);\n",
        "\n",
        "    return (image, filedir)\n",
        "\n",
        "# End Diffuse Function\n",
        "\n",
        "corrected_size = None\n",
        "last_batch = None\n",
        "\n",
        "if SKIP_DIFFUSION_RUN:\n",
        "    print(f':gear: Skipping Diffusion Run to Process Images...')\n",
        "    PROMPT = 'None'\n",
        "    PROMPT_FILE = ''\n",
        "    NUM_ITERS = 1\n",
        "else:\n",
        "    # Setup Prompts\n",
        "    if PROMPT.lower() in [None, '', 'none'] and PROMPT_FILE in [None, '', 'none']:\n",
        "        raise AttributeError(\"PROMPT and PROMPT_FILE are empty! You need to provide a PROMPT or PROMPT_FILE!\")\n",
        "\n",
        "PROMPTS = []\n",
        "if PROMPT_FILE not in ['','none']:\n",
        "    try:\n",
        "        with open(PROMPT_FILE, \"r\") as f:\n",
        "            PROMPTS = f.read().splitlines()\n",
        "    except OSError as e:\n",
        "        raise e\n",
        "    new_prompts = []\n",
        "    for prompt in PROMPTS:\n",
        "        if '|' in prompt and prompt.startswith('^'):\n",
        "            prompt = prompt.split('|')\n",
        "            pri = int(prompt[0].replace('^',''))\n",
        "            for x in range(pri):\n",
        "                new_prompts.append(prompt[1].strip())\n",
        "        else:\n",
        "            new_prompts.append(prompt.strip())\n",
        "    PROMPTS = new_prompts\n",
        "        \n",
        "# Insert prompt string first\n",
        "if PROMPT not in ['', 'none']:\n",
        "    PROMPTS.insert(0, PROMPT)\n",
        "\n",
        "original_prompts = PROMPTS.copy()\n",
        "\n",
        "#Get corrected sizes\n",
        "wx, hx = map(lambda x: x - x % 64, (int(WIDTH), int(HEIGHT)))\n",
        "if not corrected_size:\n",
        "    if int(wx) != int(WIDTH) or int(hx) != int(HEIGHT):\n",
        "        print(f':warning: Changing output size to {wx}x{hx}. Dimensions must by multiples of 64.')\n",
        "        WIDTH = wx\n",
        "        HEIGHT = hx\n",
        "        corrected_size = (WIDTH,HEIGHT)\n",
        "\n",
        "# Setup init_iamge\n",
        "inits = None\n",
        "masks = None\n",
        "init = None\n",
        "mask = None\n",
        "original_init = None\n",
        "original_mask = None\n",
        "if INIT_IMAGE.lower() not in [None, '', 'none']:\n",
        "    print(\":hourglass_not_done: Searching for init images...\")\n",
        "    if INIT_IMAGE.lower().startswith('http://') or INIT_IMAGE.lower().startswith('https://'):\n",
        "        inits = INIT_IMAGE\n",
        "    else:\n",
        "        inits = getInitImages(INIT_IMAGE, INIT_FILTERS, True)\n",
        "    if inits is not None:\n",
        "        pipe_type = 'img2img'\n",
        "    else:\n",
        "        print(f\":WARNING: No valid image(s) found in {INIT_IMAGE}. Switching to default Text-to-Image run...\")\n",
        "        pipe_type = 'lowvram' if LOW_VRAM_PATCH else 'default'\n",
        "else:\n",
        "    pipe_type = 'lowvram' if LOW_VRAM_PATCH else 'default'\n",
        "    \n",
        "if INIT_MASK.lower() not in [None, '', 'none']:\n",
        "    print(\":hourglass_not_done: Searching for mask images...\")\n",
        "    if INIT_MASK.lower().startswith('http://') or INIT_MASK.lower().startswith('https://'):\n",
        "        masks = [INIT_MASK]\n",
        "    else:\n",
        "        masks = getInitImages(INIT_MASK, INIT_FILTERS, True)\n",
        "    if masks is not None:\n",
        "        pipe_type = 'inpaint'\n",
        "    else:\n",
        "        if inits is None:\n",
        "            print(f\":WARNING: No valid mask image(s) found in {INIT_MASK}. Switching to default Text-to-Image run...\")\n",
        "            pipe_type = 'lowvram' if LOW_VRAM_PATCH else 'default'\n",
        "        else:\n",
        "            print(f\":WARNING: No valid mask image(s) found in {INIT_MASK}. Switching to default Image-to-Image run...\")\n",
        "            pipe_type = 'img2img'\n",
        "\n",
        "# Check if there are images before skipping diffusion\n",
        "if SKIP_DIFFUSION_RUN and inits is None:\n",
        "    SystemExit(\":warning: INIT_IMAGE must be defined with valid image(s) to skip diffusion run!\")\n",
        "\n",
        "# Initiate non-cached pipelines\n",
        "if not CACHE_PIPELINES and not SKIP_DIFFUSION_RUN:\n",
        "    print(\"Setting up diffusion model pipeline...\")\n",
        "    pipe = setup_pipe(pipe_type, MODEL_ID, model_cache)\n",
        "    #if ( last_diffusion_filedir is None \n",
        "    #        and inits is None \n",
        "    #        and type(pipe) is not StableDiffusionPipeline ):\n",
        "    #    print(\"Pipeline in memory is is a img2img-type pipeline, but no INIT_IMAGE or Evolution is defined.\")\n",
        "    #    pipe = setup_pipe('lowvram', MODEL_ID, model_cache) if LOW_VRAM_PATCH else setup_pipe('default', MODEL_ID, model_cache)\n",
        "    #    pipe_type = 'lowvram' if LOW_VRAM_PATCH else 'default'\n",
        "    #if pipe_type is not last_pipe_type:\n",
        "    #    pipe = setup_pipe(pipe_type, MODEL_ID, model_cache)\n",
        "    if ENABLE_ATTENTION_SLICES:\n",
        "        print(':gear: Attention Slices Enabled.')\n",
        "        pipe.enable_attention_slicing()\n",
        "        #optimize_attention(pipe.unet)\n",
        "    else:\n",
        "        print(':gear: Attention Slices Disabled.')\n",
        "        pipe.disable_attention_slicing()\n",
        "    print(':check_mark_button: Pipeline setup complete.')\n",
        "\n",
        "last_pipe_type = pipe_type\n",
        "\n",
        "with torch.no_grad():\n",
        "    with precision_scope(\"cuda\"):\n",
        "\n",
        "        # Hack in Image List Support\n",
        "        DO = None\n",
        "        if type(inits) is list:\n",
        "            ITERATE_THIS = inits\n",
        "            DO = 'inits'\n",
        "        else:\n",
        "            ITERATE_THIS = PROMPTS\n",
        "            DO = 'prompts'\n",
        "\n",
        "        i = 0\n",
        "        print(f\":information: Doing {len(ITERATE_THIS)} batches.\")\n",
        "        for pi in ITERATE_THIS: # Replace PROMPTS with ITERATE_THIS switch\n",
        "\n",
        "            print(f\":information: Starting batch {i+1}\")\n",
        "            last_batch = i+1\n",
        "\n",
        "            if DO is 'inits':\n",
        "                init = pi\n",
        "                if i > len(PROMPTS)-1:\n",
        "                    pi = PROMPTS[-1]\n",
        "                else:\n",
        "                    pi = PROMPTS[i]\n",
        "                if masks:\n",
        "                    if i > len(masks)-1:\n",
        "                        mask = masks[-1]\n",
        "                    else:\n",
        "                        mask = masks[i]\n",
        "                else:\n",
        "                    mask = None\n",
        "            elif DO is 'prompts':\n",
        "                if inits is not None:\n",
        "                    init = inits\n",
        "                if masks:\n",
        "                    if type(masks) is list:\n",
        "                        if i > len(masks)-1:\n",
        "                            mask = masks[-1]\n",
        "                        else:\n",
        "                            mask = masks[i]\n",
        "                    else:\n",
        "                        mask = masks\n",
        "                else:\n",
        "                    mask = None\n",
        "\n",
        "            from PIL import ImageOps\n",
        "            if init != None:    \n",
        "                init_path = init\n",
        "                if type(init) is torch.Tensor:\n",
        "                    init = transforms.ToPILImage()(init.squeeze())\n",
        "                else:\n",
        "                    init = Image.open(fetch(init)).convert(\"RGB\")\n",
        "                if not USE_INIT_IMAGE_SIZE:\n",
        "                    init = init.resize((WIDTH,HEIGHT))\n",
        "                else:\n",
        "                    iwidth, iheight = init.size\n",
        "                    if not corrected_size:\n",
        "                        wx, hx = map(lambda x: x - x % 64, (iwidth, iheight))\n",
        "                        if int(wx) != int(iwidth) or int(hx) != int(iheight):\n",
        "                            init = init.resize((int(wx),int(hx)))\n",
        "                    else:\n",
        "                        init.resize(corrected_size)\n",
        "                original_init = init.copy()\n",
        "            if mask != None:\n",
        "                mask_path = mask\n",
        "                mask = Image.open(fetch(mask)).convert(\"RGB\")\n",
        "                if not USE_INIT_IMAGE_SIZE:\n",
        "                    mask = mask.resize((WIDTH,HEIGHT))\n",
        "                else:\n",
        "                    mwidth, mheight = mask.size\n",
        "                    if not corrected_size:\n",
        "                        wx, hx = map(lambda x: x - x % 64, (mwidth, mheight))\n",
        "                        if int(wx) != int(mwidth) or int(hx) != int(mheight):\n",
        "                            mask = mask.resize((int(wx),int(hx)))\n",
        "                    else:\n",
        "                        mask.resize(corrected_size)\n",
        "                original_mask = mask.copy()\n",
        "\n",
        "            if init and USE_INIT_IMAGE_SIZE:\n",
        "                WIDTH, HEIGHT = init.size\n",
        "                print(f':information: Setting diffusion \\'WIDTH\\' and \\'HEIGHT\\' to \\'{WIDTH}x{HEIGHT}\\'')\n",
        "\n",
        "            #Get corrected sizes\n",
        "            wx, hx = map(lambda x: x - x % 64, (WIDTH, HEIGHT))\n",
        "            if not corrected_size:\n",
        "                if int(wx) != int(WIDTH) or int(hx) != int(HEIGHT):\n",
        "                    print(f':warning: Changing output size to {wx}x{hx}. Dimensions must by multiples of 64.')\n",
        "                    WIDTH = wx\n",
        "                    HEIGHT = hx\n",
        "            else:\n",
        "                WIDTH = corrected_size[0]\n",
        "                HEIGHT = corrected_size[1]\n",
        "\n",
        "            # Save resized init and mask for GOBIG run\n",
        "            if GOBIG_SKIP_DIFFUSION_RUN or SKIP_DIFFUSION_RUN:\n",
        "                if init:\n",
        "                    init.save('./gobig_init_temp.png')\n",
        "                if mask:\n",
        "                    mask.save('./gobig_mask_temp.png')\n",
        "\n",
        "            # Define Run Prompt\n",
        "            if NEW_NSP_ON_ITERATION is not True:\n",
        "                PROMPT = dynamic_value(nsp_parse(pi))\n",
        "\n",
        "            for iteration in range(NUM_ITERS):\n",
        "\n",
        "                # Define Iteration Prompt\n",
        "                if NEW_NSP_ON_ITERATION:\n",
        "                    PROMPT = dynamic_value(nsp_parse(pi))\n",
        "                    original_prompt = PROMPT\n",
        "\n",
        "                try:\n",
        "\n",
        "                    result = None\n",
        "                    path = None\n",
        "                    mask_path = None\n",
        "                    if IMAGE_UPSCALER is not 'GOBIG':\n",
        "                        result, path = diffuse_run()\n",
        "                    else:\n",
        "                        if init is not None:\n",
        "                            if SKIP_DIFFUSION_RUN or GOBIG_SKIP_DIFFUSION_RUN:\n",
        "                                path = './gobig_init_temp.png'\n",
        "                            else:\n",
        "                                result, path = diffuse_run()\n",
        "                        else:\n",
        "                            result, path = diffuse_run()\n",
        "                        if mask is not None:\n",
        "                            mask_path = './gobig_mask_temp.png'\n",
        "\n",
        "                    if IMAGE_UPSCALER is 'GOBIG' and path is not None:\n",
        "\n",
        "\n",
        "                        # Set GOBIG Run Conditions\n",
        "                        SLICES_NUM = None\n",
        "\n",
        "                        if ORIG_SEED is 0:\n",
        "                            SEED = random.randint(0,MAX_SEED)\n",
        "\n",
        "                        save_settings_original = SAVE_SETTINGS_FILE\n",
        "                        SAVE_SETTINGS_FILE = False\n",
        "                        original_batch = i\n",
        "                        original_iteration = iteration\n",
        "                        original_num_iters = NUM_ITERS\n",
        "\n",
        "                        if GOBIG_DETAIL_PROMPT.lower() not in [None, 'none', '']:\n",
        "                            GOBIG_PROMPT = GOBIG_DETAIL_PROMPT\n",
        "                        else:\n",
        "                            GOBIG_PROMPT = PROMPT\n",
        "\n",
        "                        GOBIG_MASK_IMAGE = None\n",
        "                        if mask_path is not None:\n",
        "                            GOBIG_MASK_IMAGE = mask_path\n",
        "\n",
        "                        if GOBIG_HYBRID_SLICE and HYBRID_UPSCALE.lower() in [None, 'none','']:\n",
        "                            HYBRID_UPSCALE = 'LANCZOS'\n",
        "\n",
        "                        class opts:\n",
        "                            W = int(WIDTH)\n",
        "                            H = int(HEIGHT)\n",
        "                            gobig_mask_image = GOBIG_MASK_IMAGE\n",
        "                            gobig_prompt = GOBIG_PROMPT\n",
        "                            gobig_scale = int(GOBIG_INIT_SCALE)\n",
        "                            gobig_overlap = int(GOBIG_SLICE_OVERLAP)\n",
        "                            gobig_maximize = GOBIG_MAXIMIZE\n",
        "                            gobig_prescaled = GOBIG_PRESCALED\n",
        "                            gobig_scale = int(UPSCALE_AMOUNT)\n",
        "                            gobig_realesrgan = GOBIG_REAL_ESRGAN\n",
        "                            gobig_keep_slices = False\n",
        "                            gobig_hybrid_slice = GOBIG_HYBRID_SLICE\n",
        "                            gobig_hybrid_overlay = HYBRID_OVERLAY\n",
        "                            gobig_hybrid_sampler = HYBRID_UPSCALE\n",
        "                            gobig_hybrid_superres = HYBRID_SUPER_RESOLUTION\n",
        "                            filetype = '.png'\n",
        "                            quality = 100\n",
        "                            subsampling = 0\n",
        "                            outdir = OUTDIR\n",
        "                            batch = i\n",
        "                            iteration = iteration\n",
        "\n",
        "                        do_gobig(path, opts())\n",
        "\n",
        "                        NUM_ITERS = original_num_iters\n",
        "                        SAVE_SETTINGS_FILE = save_settings_original\n",
        "\n",
        "                        if path and os.path.exists(path):\n",
        "                            os.remove(path)\n",
        "                        if mask_path and os.path.exists(mask_path):\n",
        "                            os.remove(mask_path)\n",
        "                        if init and os.path.exists('./gobig_init_temp.png'):\n",
        "                            os.remove('./gobig_init_temp.png')\n",
        "                        if mask and os.path.exists('./gobig_mask_temp.png'):\n",
        "                            os.remove('./gobig_mask_temp.png')\n",
        "\n",
        "                    print(f\":information: Finished batch {i+1}\") \n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    if 'out of memory' in str(e):\n",
        "                        print(f\"\\u001b[31m\\u001b[1m\\u001b[4mCRITICAL ERROR\\u001b[0m: {gpu_name} ran out of memory! If this error persists, the GPU may have crashed, and requires a disconnect/re-run.\")\n",
        "                        pass\n",
        "                    else:\n",
        "                        raise e\n",
        "                except KeyboardInterrupt as e:\n",
        "                    raise SystemExit('\\33[33mExecution interrupted by user.\\33[0m')\n",
        "                except Exception as e:\n",
        "                    raise e\n",
        "                finally:\n",
        "                    clean_env()\n",
        "\n",
        "            # displayJsImage true iteration counter        \n",
        "            i+=1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <a name=\"cleanenv\"><font color=\"#e8cf53\">**Clean Environment Up**</font></a>\n",
        "#@markdown <font size=\"3\">**Soft Reset** the environment by deleting pipes, models, and image handlers from memory.<br><br>\n",
        "#@markdown **Note:** Before using this cell, give a minute for the system itself to flush some stuff. This will give a higher chance of this function working.<br>\n",
        "#@markdown **Note 2:** Sometimes you'll get a persistent OOM bug when the GPU has been unallocated from your session. This is common with the new (09/2022) Free Colab Sessions</font>\n",
        "delete = ['pipe','pipeout','midas','transform','prediction','input_batch','depth','depth_image','image','sr_image','enhanced_img','img','init','original_init','orig_init','model']\n",
        "for d in delete:\n",
        "    try:\n",
        "        if globals().__contains__(d):\n",
        "            del globals()[d]\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "for clnc in range(1,6):\n",
        "    print(f\"Sweeping cycle {clnc}\")\n",
        "    clean_env(True)\n",
        "    time.sleep(5)\n",
        "    if clnc != 5:\n",
        "        clear()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RjZhn2vwEydT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"#e8cf53\">**Menu**</font>\n",
        "- <a href=\"#changelog\">**Change Log**</a>\n",
        "- <a href=\"#gpustatus\">**Check GPU Status**</a>\n",
        "- #### <a href=\"#setupenv\">**Setup Environment**</a>\n",
        " - <a href=\"#googledrive\">**Google Drive Options**</a>\n",
        " - <a href=\"#optionalfeats\">**Install Optional Features**</a>\n",
        " - <a href=\"#otherinstall\">**Other Install Options**</a>\n",
        "- #### <a href=\"#settingsdiffuse\">**Settings & Diffuse**</a>\n",
        " - <a href=\"exportimport\">**Export / Import Settings**</a>\n",
        " - <a href=\"#promptsetup\">**Prompt Setup**</a>\n",
        " - <a href=\"#initsetup\">**Init Image Setup**</a>\n",
        "   - <a href=\"#recursiveevo\">**Recursive Evolution**</a>\n",
        " - <a href=\"#diffusionsettings\">**Diffusion Settings**</a>\n",
        "   - <a href=\"#diffusionmodel\">**Diffusion Model**</a>\n",
        "   - <a href=\"#conceptembed\">**Concept Embedding**</a>\n",
        " - <a href=\"#upscalers\">**Upscaling Setup**</a>\n",
        "   - <a href=\"#upscalers-codeformer\">**CodeFormer Upscale Settings**</a>\n",
        "   - <a href=\"#upscalers-gobig\">**GOBIG Upscale Settings**</a>\n",
        "   - <a href=\"#upscalers-hybrid\">**Hybrid Upscale Settings**</a>\n",
        " - <a href=\"#imageprocessors\">**Image Processing Setup**</a>\n",
        "   - <a href=\"#sharpen\">**Sharpen Image**</a>\n",
        "   - <a href=\"#kromo\">**Kromo Chromatic Aberration**</a>\n",
        "   - <a href=\"#median\">**Median Filter Image**</a>\n",
        "   - <a href=\"#midas\">**MiDaS Depth Export**</a>\n",
        "   - <a href=\"#fdof\">**Fake Depth of Field Filter**</a>\n",
        "   - <a href=\"#tileable\">**Tileable Seamless Image**</a>\n",
        " - <a href=\"#clipinterrogator\">**CLIP Interrogator**</a>\n",
        " - <a href=\"#cachepipes\">**Pipeline Flags**</a>\n",
        " - <a href=\"#othersettings\">**Console Output settings**</a>\n",
        "- <a href=\"#cleanenv\">**Clean Environment Up**</a>"
      ],
      "metadata": {
        "id": "mOY91EzZ1nQH"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}